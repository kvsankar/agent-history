#!/usr/bin/env python3
"""
agent-history - Manage and export AI coding assistant conversation sessions

A unified tool to list, filter, and export conversation sessions from Claude Code
and Codex CLI, organized by workspace. Supports both agents with auto-detection.

Author: Built with Claude Code
Version: 1.0.0
License: MIT
"""

import argparse
import base64
import json
import math
import os
import platform
import re
import shlex
import shutil
import sqlite3
import stat
import subprocess
import sys
import tempfile
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path, PureWindowsPath
from typing import Optional

__version__ = "1.5.1"

DEFAULT_CLI_NAME = "agent-history"
DEFAULT_SKILL_NAME = "agent-history"
DEFAULT_BIN_DIR = Path.home() / ".local/bin"
DEFAULT_SKILL_DIR = Path.home() / ".claude/skills" / DEFAULT_SKILL_NAME
DEFAULT_CLEANUP_DAYS = 99999

# Agent backend identifiers
AGENT_CLAUDE = "claude"
AGENT_CODEX = "codex"
AGENT_GEMINI = "gemini"

# Codex home directory
CODEX_HOME_DIR = Path.home() / ".codex" / "sessions"

# Gemini home directory (sessions stored in ~/.gemini/tmp/<hash>/chats/)
GEMINI_HOME_DIR = Path.home() / ".gemini" / "tmp"

# ============================================================================
# Named Constants (for readability and maintainability)
# ============================================================================

# Path parsing constants
MIN_WINDOWS_PATH_LEN = 2  # Minimum length for "C:" style paths
MIN_WSL_MNT_PATH_LEN = 6  # Length of "/mnt/c" prefix
WSL_UNC_MIN_PARTS = 3  # Minimum parts in //wsl.localhost/Distro/path
WSL_LOCALHOST_SKIP_PARTS = 4  # Parts to skip in /wsl.localhost/Distro/path
MIN_ENCODED_PATH_LEN = 3  # Minimum for "C--" style encoded paths

# Splitting constants
MIN_SPLIT_POINTS = 2  # Minimum split points (start + end) for no-split case
MIN_MESSAGES_FOR_SPLIT = 3  # Minimum messages before considering split

# Remote/WSL parsing constants
REMOTE_PARTS_WITH_PATH = 3  # Parts in "remote_hostname_path" after split
SSH_WORKSPACE_PARTS = 3  # Parts in workspace line "encoded|decoded|count"
SSH_SESSION_PARTS = 4  # Parts in session line "file|size|mtime|lines"

# Alias parsing constants
WSL_PREFIX_LEN = 6  # Length of "wsl://"
MNT_ENCODED_PREFIX_LEN = 7  # Length of "-mnt-X-" prefix
WINDOWS_PREFIX_LEN = 8  # Length of "windows:"

# Display constants
MAX_WORKSPACE_NAME_DISPLAY = 28  # Max chars for workspace name in stats display
MAX_SHORT_PART_LEN = 10  # Max length for "short" path component heuristic
HASH_DISPLAY_LEN = 8  # Characters to show for truncated hash display
MAX_THOUGHT_LEN = 200  # Max length for thought descriptions before truncating
MAX_TOOL_OUTPUT_LEN = 2000  # Max length for tool output before truncating

# Codex date folder parsing
CODEX_DATE_FOLDER_DEPTH = 4  # Depth of YYYY/MM/DD/file structure

# Time constants
SECONDS_PER_MINUTE = 60

# Database migration versions
DB_VERSION_TIME_TRACKING = 3  # Version that added time tracking
DB_VERSION_AGENT_COLUMN = 4  # Version that added agent column


def codex_get_home_dir() -> Path:
    """Get Codex sessions directory (~/.codex/sessions/).

    Supports CODEX_SESSIONS_DIR environment variable override for testing
    and custom configurations.
    """
    env_override = os.environ.get("CODEX_SESSIONS_DIR")
    if env_override:
        return Path(env_override).expanduser()
    return CODEX_HOME_DIR


def gemini_get_home_dir() -> Path:
    """Get Gemini sessions directory (~/.gemini/tmp/).

    Supports GEMINI_SESSIONS_DIR environment variable override for testing
    and custom configurations.
    """
    env_override = os.environ.get("GEMINI_SESSIONS_DIR")
    if env_override:
        return Path(env_override).expanduser()
    return GEMINI_HOME_DIR


def detect_agent_from_path(path: Path) -> str:
    """Detect agent type from file path.

    Args:
        path: Path to a session file

    Returns:
        AGENT_GEMINI if path contains .gemini,
        AGENT_CODEX if path contains .codex,
        otherwise AGENT_CLAUDE
    """
    path_str = str(path)
    if "/.gemini/" in path_str or "\\.gemini\\" in path_str:
        return AGENT_GEMINI
    if "/.codex/" in path_str or "\\.codex\\" in path_str:
        return AGENT_CODEX
    return AGENT_CLAUDE


# ============================================================================
# Data Classes
# ============================================================================


@dataclass
class ExportConfig:
    """Configuration for batch export operations.

    This dataclass replaces ad-hoc argument classes used throughout the codebase,
    providing type safety and a single source of truth for export parameters.
    """

    output_dir: str
    patterns: list = field(default_factory=list)
    since: Optional[str] = None
    until: Optional[str] = None
    force: bool = False
    minimal: bool = False
    split: Optional[int] = None
    flat: bool = False
    remote: Optional[str] = None
    lenient: bool = False
    agent: str = "auto"

    @property
    def workspace(self) -> str:
        """First pattern as workspace (backward compatibility)."""
        return self.patterns[0] if self.patterns else ""

    @classmethod
    def from_args(cls, args, **overrides) -> "ExportConfig":
        """Create ExportConfig from argparse args with optional overrides.

        Args:
            args: Parsed argparse arguments
            **overrides: Values to override from args

        Returns:
            ExportConfig instance
        """
        return cls(
            output_dir=overrides.get("output_dir", getattr(args, "output_dir", ".")),
            patterns=overrides.get("patterns", getattr(args, "patterns", [])),
            since=overrides.get("since", getattr(args, "since", None)),
            until=overrides.get("until", getattr(args, "until", None)),
            force=overrides.get("force", getattr(args, "force", False)),
            minimal=overrides.get("minimal", getattr(args, "minimal", False)),
            split=overrides.get("split", getattr(args, "split", None)),
            flat=overrides.get("flat", getattr(args, "flat", False)),
            remote=overrides.get("remote", getattr(args, "remote", None)),
            lenient=overrides.get("lenient", getattr(args, "lenient", False)),
            agent=overrides.get("agent", getattr(args, "agent", None) or "auto"),
        )


# ============================================================================
# Date Parsing
# ============================================================================


def parse_date_string(date_str):
    """
    Parse ISO date string (YYYY-MM-DD) into datetime object.
    Returns datetime object or None if parsing fails.
    """
    if not date_str:
        return None

    try:
        return datetime.strptime(date_str.strip(), "%Y-%m-%d")
    except ValueError:
        return None


def validate_split_lines(value):
    """
    Validate split lines argument for argparse.
    Ensures value is a positive integer.
    """
    try:
        ivalue = int(value)
        if ivalue <= 0:
            raise argparse.ArgumentTypeError(
                f"split value must be a positive integer, got '{value}'"
            )
        return ivalue
    except ValueError as e:
        raise argparse.ArgumentTypeError(f"split value must be an integer, got '{value}'") from e


# ============================================================================
# Input Validation (Security)
# ============================================================================

# Pattern for valid workspace directory names (Claude Code encoded paths)
# Allow: alphanumeric, dashes, underscores, dots
# Workspace names always start with dash (Unix) or letter (Windows drive)
WORKSPACE_NAME_PATTERN = re.compile(r"^[-a-zA-Z0-9_.]+$")

# Pattern for valid SSH remote host specifications
# Allow: user@hostname, hostname, user@hostname:port, IPv4/IPv6
REMOTE_HOST_PATTERN = re.compile(
    r"^(?:[\w.-]+@)?"  # Optional user@
    r"(?:[\w.-]+|\[[0-9a-fA-F:]+\])"  # Hostname or IPv6 in brackets
    r"(?::\d+)?$"  # Optional :port
)


def validate_workspace_name(workspace_name: str) -> bool:
    """Validate workspace name to prevent command injection and path traversal.

    Workspace names from Claude Code are encoded paths like:
    - '-home-user-project' (Unix)
    - 'C--Users-name-project' (Windows)
    - 'remote_host_home-user-project' (cached remote)
    - 'wsl_Ubuntu_home-user-project' (cached WSL)

    Returns True if valid, False otherwise.
    """
    if not workspace_name:
        return False
    if len(workspace_name) > MAX_WORKSPACE_NAME_LENGTH:
        return False
    # Check for path traversal sequences
    if ".." in workspace_name:
        return False
    return bool(WORKSPACE_NAME_PATTERN.match(workspace_name))


def validate_remote_host(remote_host: str) -> bool:
    """Validate remote host specification to prevent command injection.

    Valid formats:
    - hostname
    - user@hostname
    - user@hostname:port
    - IPv4 address
    - user@IPv6 (with brackets)

    Returns True if valid, False otherwise.
    """
    if not remote_host:
        return False
    if len(remote_host) > MAX_REMOTE_HOST_LENGTH:
        return False
    return bool(REMOTE_HOST_PATTERN.match(remote_host))


def sanitize_for_shell(value: str) -> str:
    """Sanitize a value for safe use in shell commands.

    Uses shlex.quote for proper escaping.
    """
    return shlex.quote(value)


def is_safe_path(base_dir: Path, target_path: Path) -> bool:
    """Check if target_path is safely within base_dir (no path traversal).

    This prevents directory traversal attacks where a malicious path
    could escape the intended base directory.

    Args:
        base_dir: The base directory that should contain the path
        target_path: The path to validate

    Returns:
        True if target_path is within base_dir, False otherwise.
    """
    try:
        # Resolve both paths to absolute paths (resolves symlinks and ..)
        base_resolved = base_dir.resolve()
        target_resolved = target_path.resolve()

        # Check if target is within base (or is base itself)
        return target_resolved == base_resolved or str(target_resolved).startswith(
            str(base_resolved) + os.sep
        )
    except (OSError, ValueError):
        return False


# ============================================================================
# Constants
# ============================================================================

# Validation limits
MAX_WORKSPACE_NAME_LENGTH = 1000  # Reasonable limit for workspace names
MAX_REMOTE_HOST_LENGTH = 255  # DNS hostname limit

# Conversation splitting thresholds
SPLIT_MIN_FACTOR = 0.8  # Minimum lines before considering split (80% of target)
SPLIT_MAX_FACTOR = 1.3  # Maximum lines before forcing split (130% of target)
HEADER_LINES_ESTIMATE = 30  # Approximate header size in lines
METADATA_LINES_ESTIMATE = 20  # Approximate metadata section size in lines

# Split point scoring weights
SCORE_USER_MESSAGE_NEXT = 100  # Next message is User (best - starting new topic)
SCORE_TOOL_RESULT = 50  # Current message is tool result (action complete)
SCORE_TIME_GAP_LARGE = 30  # Time gap > 5 minutes
SCORE_TIME_GAP_MEDIUM = 10  # Time gap > 1 minute
SCORE_DISTANCE_PENALTY = 0.05  # Penalty per line away from target

# Time gap thresholds (in seconds)
TIME_GAP_LARGE = 300  # 5 minutes
TIME_GAP_MEDIUM = 60  # 1 minute

# Display limits
MAX_WORKSPACE_CANDIDATES = 20  # Maximum workspaces to show in suggestions
MAX_TOP_WORKSPACES = 20  # Top workspaces to show in stats

# Timeouts (in seconds)
SSH_TIMEOUT = 300  # 5 minutes for SSH operations
SCP_TIMEOUT = 60  # 1 minute for single file copy

# Work period detection (in seconds)
WORK_PERIOD_GAP_THRESHOLD = 30 * 60  # 30 minutes gap starts new work period

# Display formatting
DISPLAY_SEPARATOR_WIDTH = 60  # Width of separator lines

# Cached workspace prefixes
CACHED_REMOTE_PREFIX = "remote_"
CACHED_WSL_PREFIX = "wsl_"
CACHED_WINDOWS_PREFIX = "windows_"

# ============================================================================
# Error Handling Utilities
# ============================================================================


def exit_with_error(message: str, suggestions: list = None, exit_code: int = 1):
    """Exit with formatted error message and optional actionable suggestions.

    This centralizes error formatting to provide consistent, helpful error
    messages throughout the CLI. Following Rhodes' principle that errors
    should guide users to solutions.

    Args:
        message: The main error message (without "Error:" prefix)
        suggestions: Optional list of actionable suggestions for the user
        exit_code: Exit code (default: 1)

    Example:
        exit_with_error(
            "Not in a Claude Code workspace",
            suggestions=[
                "Run from within a workspace directory",
                "Specify a workspace pattern: lss <pattern>",
                "Use --aw to list all workspaces"
            ]
        )
    """
    sys.stderr.write(f"Error: {message}\n")
    if suggestions:
        sys.stderr.write("\nTo resolve, try:\n")
        for suggestion in suggestions:
            sys.stderr.write(f"  • {suggestion}\n")
    sys.exit(exit_code)


def exit_with_date_error(flag_name: str, invalid_value: str):
    """Exit with date format error message.

    Args:
        flag_name: The flag that received invalid date (e.g., "--since")
        invalid_value: The invalid date string provided
    """
    exit_with_error(
        f"Invalid date format for {flag_name}: '{invalid_value}'",
        suggestions=["Use YYYY-MM-DD format (e.g., 2025-11-01)"],
    )


# ============================================================================
# Utility Functions
# ============================================================================


def is_cached_workspace(name: str) -> bool:
    """Check if a workspace/directory name is a cached remote, WSL, or Windows workspace.

    Cached workspaces are created when fetching from remote SSH hosts, WSL
    distributions, or Windows (from WSL). They have prefixes like:
    - 'remote_hostname_...'
    - 'wsl_distro_...'
    - 'windows_username_...'

    Args:
        name: Workspace or directory name to check

    Returns:
        True if this is a cached workspace, False otherwise.
    """
    return (
        name.startswith(CACHED_REMOTE_PREFIX)
        or name.startswith(CACHED_WSL_PREFIX)
        or name.startswith(CACHED_WINDOWS_PREFIX)
    )


def is_native_workspace(name: str) -> bool:
    """Check if a workspace/directory name is a native (non-cached) workspace.

    This is the inverse of is_cached_workspace().

    Args:
        name: Workspace or directory name to check

    Returns:
        True if this is a native workspace, False if cached.
    """
    return not is_cached_workspace(name)


def matches_any_pattern(workspace_name: str, patterns: list) -> bool:
    """Check if workspace name matches any pattern in the list.

    A pattern matches if it is a substring of the workspace name.
    Empty patterns ('', '*', 'all') match all workspaces.

    Args:
        workspace_name: Workspace name to check
        patterns: List of patterns to match against

    Returns:
        True if workspace matches any pattern, False otherwise.
    """
    if not patterns or patterns in ([""], ["*"], ["all"]):
        return True
    return any(pattern and pattern in workspace_name for pattern in patterns)


def parse_and_validate_dates(since_str: str, until_str: str) -> tuple:
    """Parse and validate date filter arguments.

    Validates that date strings are in YYYY-MM-DD format and that
    the since date comes before the until date (if both provided).

    Args:
        since_str: Start date string (YYYY-MM-DD) or None
        until_str: End date string (YYYY-MM-DD) or None

    Returns:
        Tuple of (since_date, until_date) as datetime objects or None.
        Exits with error if validation fails.
    """
    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    if since_str and since_date is None:
        exit_with_date_error("--since", since_str)

    if until_str and until_date is None:
        exit_with_date_error("--until", until_str)

    if since_date and until_date and since_date > until_date:
        exit_with_error("--since date must be before --until date")

    return since_date, until_date


def get_patterns_from_args(args) -> list:
    """Extract workspace patterns from command arguments.

    Handles both new-style 'patterns' (lsw) and legacy 'workspace' (lss) attributes.
    Ensures we always return a flat list of strings and default to [""] (match all)
    when no explicit patterns are provided.

    Args:
        args: Parsed argument object

    Returns:
        List[str] of patterns ("" matches all workspaces)
    """
    # New-style 'patterns' takes precedence if present
    patterns = getattr(args, "patterns", None)
    if patterns is not None:
        return patterns if patterns else [""]

    # lss uses 'workspace' (nargs="*") which is a list
    work = getattr(args, "workspace", None)
    if work is None:
        return [""]
    if isinstance(work, list):
        return work if work else [""]
    # Fallback: single string value
    return [work]


def detect_alias_from_args(args, workspace_list: list = None) -> str:
    """Detect alias name from args or @ prefix in workspace list.

    Args:
        args: Parsed arguments with optional 'alias' attribute
        workspace_list: List of workspace patterns (may have @prefix)

    Returns:
        Alias name if found, None otherwise
    """
    alias_flag = getattr(args, "alias", None)
    if alias_flag:
        return alias_flag
    if workspace_list and workspace_list[0].startswith("@"):
        return workspace_list[0][1:]
    return None


def resolve_patterns_for_command(
    workspace_list: list,
    this_only: bool = False,
    require_workspace: bool = False,
    error_message: str = None,
) -> tuple:
    """Resolve workspace patterns, optionally checking for alias membership.

    Args:
        workspace_list: Explicit workspace patterns from args
        this_only: If True, skip alias checking
        require_workspace: If True, exit with error if not in workspace
        error_message: Custom error message for require_workspace

    Returns:
        Tuple of (patterns, alias_name) where alias_name is set if
        current workspace belongs to an alias and this_only is False
    """
    if workspace_list:
        return workspace_list, None

    pattern, exists = check_current_workspace_exists()
    if not exists:
        if require_workspace:
            msg = error_message or "Not in a Claude Code workspace."
            exit_with_error(msg)
        return [""], None

    if not this_only:
        alias_for_ws = get_alias_for_workspace(pattern, "local")
        if alias_for_ws:
            return [pattern], alias_for_ws

    return [pattern], None


def collect_sessions_with_dedup(
    patterns: list,
    since_date=None,
    until_date=None,
    projects_dir: Path = None,
    quiet: bool = True,
    include_cached: bool = False,
    skip_message_count: bool = False,
    agent: str = "auto",
) -> list:
    """Collect sessions from multiple patterns with automatic deduplication.

    This is a common operation used by cmd_list, cmd_batch, and cmd_export_all.
    Sessions are deduplicated by file path.

    Args:
        patterns: List of workspace patterns to search
        since_date: Optional start date filter
        until_date: Optional end date filter
        projects_dir: Optional projects directory (for WSL/Windows access)
        quiet: If True, suppress "not found" warnings
        include_cached: If True, include cached remote/WSL workspaces
        skip_message_count: If True, skip counting messages (faster for slow filesystems)
        agent: Agent backend to use - 'auto', 'claude', or 'codex'

    Returns:
        List of session dictionaries, deduplicated by file path.
    """
    all_sessions = []
    seen_files = set()

    for pattern in patterns if patterns else [""]:
        # Use unified sessions when agent is specified (for multi-backend support)
        pattern_sessions = get_unified_sessions(
            agent=agent,
            pattern=pattern,
            since_date=since_date,
            until_date=until_date,
            skip_message_count=skip_message_count,
            projects_dir=projects_dir,
            quiet=quiet,
        )
        for session in pattern_sessions:
            file_key = str(session["file"])
            if file_key not in seen_files:
                seen_files.add(file_key)
                all_sessions.append(session)

    # Filter out cached workspaces if requested (only for Claude backend)
    if not include_cached:
        all_sessions = [
            s
            for s in all_sessions
            if s.get("agent") == AGENT_CODEX or is_native_workspace(s["workspace"])
        ]

    return all_sessions


def format_session_line(session: dict, source_label: str) -> str:
    """Format a session as a tab-separated output line.

    Args:
        session: Session dictionary with workspace_readable, filename, etc.
        source_label: Source label (e.g., 'Local', 'WSL (Ubuntu)', 'Remote (host)')

    Returns:
        Tab-separated string for output.
    """
    agent = session.get("agent", AGENT_CLAUDE)
    return (
        f"{agent}\t{source_label}\t{session['workspace_readable']}\t{session['filename']}\t"
        f"{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
    )


def print_sessions_output(sessions: list, source_label: str, workspaces_only: bool):
    """Print sessions in appropriate format (workspaces-only or full details).

    Args:
        sessions: List of session dictionaries
        source_label: Source label for output
        workspaces_only: If True, print only unique workspace names
    """
    if workspaces_only:
        seen = set()
        ordered = []
        for s in sessions:
            # Use workspace_readable for dedup - it's consistent across agents
            key = s.get("workspace_readable") or s.get("workspace")
            if key in seen:
                continue
            seen.add(key)
            ordered.append(s["workspace_readable"])
        for ws in sorted(ordered):
            # On Windows, only filter non-existent paths for Windows sources
            if sys.platform == "win32" and source_label.startswith("Windows"):
                path_str = ws.replace(" [missing]", "")
                try:
                    if not Path(path_str).exists():
                        continue
                except Exception:
                    pass
            print(ws)
    else:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for session in sessions:
            print(format_session_line(session, source_label))


# ============================================================================
# External Command Paths (Security)
# ============================================================================

# Cache for resolved command paths
_COMMAND_PATHS = {}


def get_command_path(cmd: str) -> str:
    """Get absolute path for an external command.

    Uses shutil.which() to find the command in PATH, then caches the result.
    Falls back to the command name if not found (lets subprocess handle the error).

    Args:
        cmd: Command name (e.g., 'ssh', 'rsync', 'wsl')

    Returns:
        Absolute path to the command, or the original name if not found.
    """
    if cmd not in _COMMAND_PATHS:
        path = shutil.which(cmd)
        _COMMAND_PATHS[cmd] = path if path else cmd
    return _COMMAND_PATHS[cmd]


# ============================================================================
# JSONL Parsing and Markdown Conversion
# ============================================================================


def decode_content(encoded_str: str) -> str:
    """Decode base64-encoded content from Claude API responses.

    Some content in .jsonl files (particularly large tool outputs) is stored
    base64-encoded to preserve binary data and special characters.

    Args:
        encoded_str: Base64-encoded string from API response

    Returns:
        Decoded UTF-8 string, or error message if decoding fails.
        Returns error message rather than raising to allow partial
        conversation recovery when some content is corrupted.
    """
    try:
        return base64.b64decode(encoded_str).decode("utf-8")
    except (ValueError, UnicodeDecodeError) as e:
        # ValueError: invalid base64, UnicodeDecodeError: not valid UTF-8
        return f"[Error decoding content: {e}]"


def _format_tool_use_block(block: dict) -> list:
    """Format a tool_use block as markdown lines."""
    tool_name = block.get("name", "unknown")
    tool_id = block.get("id", "")
    tool_input = block.get("input", {})

    lines = [f"\n**[Tool Use: {tool_name}]**"]
    if tool_id:
        lines.append(f"Tool ID: `{tool_id}`")
    lines.extend(["\nInput:", "```json", json.dumps(tool_input, indent=2), "```\n"])
    return lines


def _format_tool_result_block(block: dict) -> list:
    """Format a tool_result block as markdown lines."""
    tool_use_id = block.get("tool_use_id", "")
    is_error = block.get("is_error", False)
    result_content = block.get("content", "")

    # Handle both string and list content in tool results
    if isinstance(result_content, list):
        result_text = "\n".join(
            item.get("text", "") if isinstance(item, dict) else str(item) for item in result_content
        )
    else:
        result_text = result_content

    status = "ERROR" if is_error else "Success"
    lines = [f"\n**[Tool Result: {status}]**"]
    if tool_use_id:
        lines.append(f"Tool Use ID: `{tool_use_id}`")
    lines.extend(["\n```", result_text, "```\n"])
    return lines


def extract_content(message_obj: dict) -> str:
    """Extract text content from message object, preserving all information.

    Claude API messages contain different content structures:
    - User messages: Simple string content
    - Assistant messages: Array of content blocks (text, tool_use, tool_result)

    Args:
        message_obj: Message dictionary from JSONL entry

    Returns:
        Markdown-formatted string of all message content.
    """
    if "content" not in message_obj:
        return "[No content]"

    content = message_obj["content"]

    # User messages have simple string content
    if isinstance(content, str):
        return content

    # Assistant messages have array of content blocks
    if not isinstance(content, list):
        return "[No content]"

    content_parts = []
    for block in content:
        block_type = block.get("type")
        if block_type == "text":
            content_parts.append(block.get("text", ""))
        elif block_type == "tool_use":
            content_parts.extend(_format_tool_use_block(block))
        elif block_type == "tool_result":
            content_parts.extend(_format_tool_result_block(block))

    return "\n".join(content_parts) if content_parts else "[No content]"


def get_first_timestamp(jsonl_file: Path) -> str:
    """Extract the first message timestamp from a .jsonl file.

    Scans the file line by line looking for the first user or assistant
    message with a timestamp. Returns None if file cannot be read or
    contains no valid messages.
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    entry_type = entry.get("type")
                    if entry_type in ("user", "assistant"):
                        timestamp = entry.get("timestamp", "")
                        if timestamp:
                            return timestamp
                except json.JSONDecodeError:
                    continue
    except OSError:
        # File doesn't exist, permission denied, or other I/O error
        pass
    return None


def estimate_message_lines(msg_content: str, has_metadata: bool) -> int:
    """Estimate number of lines a message will take in markdown."""
    lines = 0
    lines += 1  # Message header (## Message N)
    lines += 1  # Empty line
    lines += 1  # Timestamp
    lines += 1  # Empty line
    lines += len(msg_content.split("\n"))  # Content
    lines += 1  # Empty line
    if has_metadata:
        lines += METADATA_LINES_ESTIMATE
        lines += 1  # Empty line
    lines += 1  # Separator (---)
    lines += 1  # Empty line
    return lines


def is_tool_result_message(msg_content: str) -> bool:
    """Check if message content contains a tool result."""
    return "**[Tool Result:" in msg_content


def calculate_time_gap(msg1: dict, msg2: dict) -> float:
    """Calculate time gap in seconds between two messages.

    Used by the conversation splitting algorithm to identify natural
    break points - longer gaps suggest topic changes or session breaks.

    Args:
        msg1: First message dictionary (must have 'timestamp' key)
        msg2: Second message dictionary (must have 'timestamp' key)

    Returns:
        Absolute time difference in seconds between messages.
        Returns 0 if timestamps are missing or malformed, which
        causes the split algorithm to rely on other signals.
    """
    try:
        ts1 = datetime.fromisoformat(msg1["timestamp"].replace("Z", "+00:00"))
        ts2 = datetime.fromisoformat(msg2["timestamp"].replace("Z", "+00:00"))
        return abs((ts2 - ts1).total_seconds())
    except (ValueError, KeyError, TypeError):
        # Timestamps missing or malformed - expected for some messages
        return 0


def _calculate_split_score(
    messages: list, index: int, msg: dict, current_lines: int, target_lines: int
) -> float:
    """Calculate score for splitting after message at index."""
    score = 0

    # Priority 1: Next message is User (best - starting new topic)
    if index + 1 < len(messages) and messages[index + 1]["role"] == "user":
        score += SCORE_USER_MESSAGE_NEXT

    # Priority 2: Current message is tool result (action complete)
    if is_tool_result_message(msg["content"]):
        score += SCORE_TOOL_RESULT

    # Priority 3: Time gap before next message
    if index + 1 < len(messages):
        time_gap = calculate_time_gap(msg, messages[index + 1])
        if time_gap > TIME_GAP_LARGE:
            score += SCORE_TIME_GAP_LARGE
        elif time_gap > TIME_GAP_MEDIUM:
            score += SCORE_TIME_GAP_MEDIUM

    # Slight preference for being closer to target
    score -= abs(current_lines - target_lines) * SCORE_DISTANCE_PENALTY
    return score


def find_best_split_point(messages: list, target_lines: int, minimal: bool) -> int:
    """Find the optimal message index to split a conversation."""
    min_lines = int(target_lines * SPLIT_MIN_FACTOR)
    max_lines = int(target_lines * SPLIT_MAX_FACTOR)

    current_lines = HEADER_LINES_ESTIMATE
    best_split = None
    best_score = -1

    for i, msg in enumerate(messages):
        current_lines += estimate_message_lines(msg["content"], not minimal)

        if current_lines > max_lines:
            return i if i > 0 else 1

        if current_lines >= min_lines:
            score = _calculate_split_score(messages, i, msg, current_lines, target_lines)
            if score > best_score:
                best_score = score
                best_split = i + 1

    return best_split


def read_jsonl_messages(jsonl_file: Path):
    """Read and parse all messages from a Claude Code JSONL session file.

    Parses each line of the JSONL file, extracting user and assistant
    messages along with their metadata. Invalid JSON lines are skipped
    with a warning.

    Args:
        jsonl_file: Path to the .jsonl session file

    Returns:
        List of message dicts containing:
        - role: "user" or "assistant"
        - content: Extracted and formatted message content
        - timestamp: ISO 8601 timestamp string
        - uuid, parentUuid, sessionId, agentId: Message identifiers
        - cwd, version, gitBranch: Context information
        - model, usage, stop_reason: Assistant message metadata
    """
    messages = []

    with open(jsonl_file, encoding="utf-8") as f:
        for line in f:
            try:
                entry = json.loads(line)

                # Look for user or assistant message types
                entry_type = entry.get("type")
                if entry_type in ("user", "assistant"):
                    message_obj = entry.get("message", {})
                    role = message_obj.get("role", entry_type)
                    timestamp = entry.get("timestamp", "")
                    content = extract_content(message_obj)

                    # Preserve all metadata from the entry
                    messages.append(
                        {
                            "role": role,
                            "content": content,
                            "timestamp": timestamp,
                            "uuid": entry.get("uuid", ""),
                            "parentUuid": entry.get("parentUuid"),
                            "sessionId": entry.get("sessionId", ""),
                            "agentId": entry.get("agentId"),
                            "requestId": entry.get("requestId"),
                            "cwd": entry.get("cwd", ""),
                            "version": entry.get("version", ""),
                            "gitBranch": entry.get("gitBranch"),
                            "isSidechain": entry.get("isSidechain"),
                            "userType": entry.get("userType"),
                            "model": message_obj.get("model"),
                            "usage": message_obj.get("usage"),
                            "stop_reason": message_obj.get("stop_reason"),
                            "stop_sequence": message_obj.get("stop_sequence"),
                        }
                    )

            except json.JSONDecodeError as e:
                print(f"Warning: Couldn't parse line: {e}", file=sys.stderr)
                continue

    return messages


def _generate_identifiers_metadata(msg: dict, uuid_to_index: dict) -> list:
    """Generate core identifiers metadata with navigation links."""
    lines = []
    if msg.get("uuid"):
        lines.append(f"- **UUID:** `{msg['uuid']}`")

    if msg.get("parentUuid"):
        parent_uuid = msg["parentUuid"]
        if parent_uuid in uuid_to_index:
            parent_msg_num = uuid_to_index[parent_uuid]
            lines.append(
                f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(→ Message {parent_msg_num})*"
            )
        else:
            lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")

    if msg.get("sessionId"):
        lines.append(f"- **Session ID:** `{msg['sessionId']}`")
    if msg.get("agentId"):
        lines.append(f"- **Agent ID:** `{msg['agentId']}`")
    if msg.get("requestId"):
        lines.append(f"- **Request ID:** `{msg['requestId']}`")
    return lines


def _generate_environment_metadata(msg: dict) -> list:
    """Generate environment info metadata."""
    lines = []
    if msg.get("cwd"):
        lines.append(f"- **Working Directory:** `{msg['cwd']}`")
    if msg.get("gitBranch"):
        lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
    if msg.get("version"):
        lines.append(f"- **Version:** `{msg['version']}`")
    if msg.get("userType"):
        lines.append(f"- **User Type:** `{msg['userType']}`")
    if msg.get("isSidechain") is not None:
        lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")
    return lines


def _generate_model_metadata(msg: dict) -> list:
    """Generate model info and usage stats metadata."""
    lines = []
    if msg.get("model"):
        lines.append(f"- **Model:** `{msg['model']}`")
    if msg.get("stop_reason"):
        lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
    if msg.get("stop_sequence"):
        lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

    if msg.get("usage"):
        usage = msg["usage"]
        lines.append("- **Usage:**")
        lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
        lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
        if usage.get("cache_creation_input_tokens"):
            lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
        if usage.get("cache_read_input_tokens"):
            lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")
    return lines


def _generate_full_metadata(msg: dict, uuid_to_index: dict) -> list:
    """Generate complete metadata section for a message."""
    lines = ["### Metadata", ""]
    lines.extend(_generate_identifiers_metadata(msg, uuid_to_index))
    lines.extend(_generate_environment_metadata(msg))
    lines.extend(_generate_model_metadata(msg))
    lines.append("")
    return lines


def _generate_message_section(
    msg: dict, msg_num: int, is_agent: bool, minimal: bool, uuid_to_index: dict
) -> list:
    """Generate markdown section for a single message."""
    lines = []

    # Determine message label
    if is_agent and msg_num == 1 and msg["role"] == "user":
        role_label = "Task Prompt (from Parent Claude)"
    else:
        role_label = msg["role"].title()

    # Add HTML anchor (skip in minimal mode)
    if not minimal and msg.get("uuid"):
        lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
        lines.append("")

    lines.append(f"## Message {msg_num} - {role_label}")
    lines.append("")
    lines.append(f"*{msg['timestamp']}*")
    lines.append("")
    lines.append(msg["content"])
    lines.append("")

    # Add metadata section (skip in minimal mode)
    if not minimal:
        lines.extend(_generate_full_metadata(msg, uuid_to_index))

    lines.append("---")
    lines.append("")
    return lines


def _generate_markdown_file_header(jsonl_file: Path, messages: list, is_agent: bool) -> list:
    """Generate the file header section of markdown."""
    lines = ["# Claude Conversation (Agent)" if is_agent else "# Claude Conversation", ""]
    lines.append(f"**File:** {jsonl_file.name}")
    lines.append(f"**Messages:** {len(messages)}")
    if messages:
        lines.append(f"**First message:** {messages[0]['timestamp']}")
        lines.append(f"**Last message:** {messages[-1]['timestamp']}")
    return lines


def _generate_agent_conversation_notice(parent_session_id: str, agent_id: str) -> list:
    """Generate the agent conversation notice section."""
    lines = [
        "",
        "> **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.",
        ">",
        "> - Messages labeled 'User' represent task instructions from the parent Claude session",
        "> - Messages labeled 'Assistant' are responses from this agent",
    ]
    if parent_session_id:
        lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
    if agent_id:
        lines.append(f"> - **Agent ID:** `{agent_id}`")
    return lines


def _get_agent_info(messages: list) -> tuple:
    """Extract agent info from messages. Returns (is_agent, parent_session_id, agent_id)."""
    is_agent = any(msg.get("isSidechain") for msg in messages)
    if is_agent and messages:
        return is_agent, messages[0].get("sessionId"), messages[0].get("agentId")
    return False, None, None


def parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False, messages: list = None) -> str:
    """Convert a Claude Code JSONL session file to readable Markdown."""
    if messages is None:
        messages = read_jsonl_messages(jsonl_file)

    is_agent, parent_session_id, agent_id = _get_agent_info(messages)

    md_lines = _generate_markdown_file_header(jsonl_file, messages, is_agent)
    if is_agent:
        md_lines.extend(_generate_agent_conversation_notice(parent_session_id, agent_id))
    md_lines.extend(["", "---", ""])

    uuid_to_index = {msg["uuid"]: i for i, msg in enumerate(messages, 1) if msg.get("uuid")}
    for i, msg in enumerate(messages, 1):
        md_lines.extend(_generate_message_section(msg, i, is_agent, minimal, uuid_to_index))

    return "\n".join(md_lines)


def generate_markdown_parts(messages, jsonl_file: Path, minimal: bool, split_lines: int):
    """Generate multiple markdown parts from messages, split at smart break points.

    Returns list of (part_num, total_parts, markdown_content, start_msg, end_msg) tuples.
    """
    if not split_lines or len(messages) == 0:
        return None

    # Find all split points
    split_points = [0]  # Start with message 0
    remaining_messages = messages

    while True:
        # Find next split point in remaining messages
        split_idx = find_best_split_point(remaining_messages, split_lines, minimal)

        if split_idx is None or split_idx >= len(remaining_messages):
            # No more splits needed
            break

        # Add this split point (adjusted for global message index)
        global_idx = split_points[-1] + split_idx
        split_points.append(global_idx)
        remaining_messages = messages[global_idx:]

    # Add end point
    split_points.append(len(messages))

    # If only one part, no splitting needed
    if len(split_points) <= MIN_SPLIT_POINTS:
        return None

    total_parts = len(split_points) - 1
    parts = []

    for part_num in range(total_parts):
        start_idx = split_points[part_num]
        end_idx = split_points[part_num + 1]
        part_messages = messages[start_idx:end_idx]

        # Generate markdown for this part
        part_md = generate_markdown_for_messages(
            part_messages,
            jsonl_file,
            minimal,
            part_num=part_num + 1,
            total_parts=total_parts,
            start_msg_num=start_idx + 1,
            end_msg_num=end_idx,
        )

        parts.append((part_num + 1, total_parts, part_md, start_idx + 1, end_idx))

    return parts


def _generate_markdown_header(
    jsonl_file, part_messages, is_agent, part_num, total_parts, start_msg_num, end_msg_num
):
    """Generate markdown header section."""
    md_lines = []

    # Title
    title_base = "Claude Conversation (Agent)" if is_agent else "Claude Conversation"
    if part_num and total_parts:
        md_lines.append(f"# {title_base} - Part {part_num} of {total_parts}")
    else:
        md_lines.append(f"# {title_base}")

    md_lines.append("")
    md_lines.append(f"**File:** {jsonl_file.name}")

    if part_num and total_parts:
        md_lines.append(f"**Part:** {part_num} of {total_parts}")
        md_lines.append(
            f"**Messages in this part:** {len(part_messages)} (#{start_msg_num}-#{end_msg_num})"
        )
    else:
        md_lines.append(f"**Messages:** {len(part_messages)}")

    if part_messages:
        md_lines.append(f"**First message:** {part_messages[0]['timestamp']}")
        md_lines.append(f"**Last message:** {part_messages[-1]['timestamp']}")

    return md_lines


def _generate_agent_notice(parent_session_id, agent_id):
    """Generate agent conversation notice."""
    md_lines = [
        "",
        "> **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.",
        ">",
        "> - Messages labeled 'User' represent task instructions from the parent Claude session",
        "> - Messages labeled 'Assistant' are responses from this agent",
    ]
    if parent_session_id:
        md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
    if agent_id:
        md_lines.append(f"> - **Agent ID:** `{agent_id}`")
    return md_lines


def _generate_message_metadata(msg, uuid_to_index):
    """Generate metadata section for a message."""
    md_lines = ["### Metadata", ""]

    # Helper to add optional field
    def add_field(key, label, format_str="- **{label}:** `{value}`"):
        value = msg.get(key)
        if value is not None:
            md_lines.append(format_str.format(label=label, value=value))

    add_field("uuid", "UUID")

    # Parent UUID with link
    parent_uuid = msg.get("parentUuid")
    if parent_uuid:
        if parent_uuid in uuid_to_index:
            parent_msg_num = uuid_to_index[parent_uuid]
            md_lines.append(
                f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(→ Message {parent_msg_num})*"
            )
        else:
            md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")

    add_field("sessionId", "Session ID")
    add_field("agentId", "Agent ID")
    add_field("requestId", "Request ID")
    add_field("cwd", "Working Directory")
    add_field("gitBranch", "Git Branch")
    add_field("version", "Version")
    add_field("userType", "User Type")

    if msg.get("isSidechain") is not None:
        md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

    add_field("model", "Model")
    add_field("stop_reason", "Stop Reason")
    add_field("stop_sequence", "Stop Sequence")

    # Usage stats
    usage = msg.get("usage")
    if usage:
        md_lines.append("- **Usage:**")
        md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
        md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
        if usage.get("cache_creation_input_tokens"):
            md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
        if usage.get("cache_read_input_tokens"):
            md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

    md_lines.append("")
    return md_lines


def _get_role_label(msg: dict, is_agent: bool, index: int, start_msg_num: int) -> str:
    """Get display label for message role."""
    if is_agent and index == start_msg_num and msg["role"] == "user":
        return "Task Prompt (from Parent Claude)"
    return msg["role"].title()


def _generate_message_block(
    msg: dict, index: int, is_agent: bool, start_msg_num: int, minimal: bool, uuid_to_index: dict
) -> list:
    """Generate markdown lines for a single message."""
    lines = []
    role_label = _get_role_label(msg, is_agent, index, start_msg_num)

    if not minimal and msg.get("uuid"):
        lines.extend([f'<a name="msg-{msg["uuid"]}"></a>', ""])

    lines.extend(
        [f"## Message {index} - {role_label}", "", f"*{msg['timestamp']}*", "", msg["content"], ""]
    )

    if not minimal:
        lines.extend(_generate_message_metadata(msg, uuid_to_index))

    lines.extend(["---", ""])
    return lines


def generate_markdown_for_messages(
    part_messages,
    jsonl_file,
    minimal,
    part_num=None,
    total_parts=None,
    start_msg_num=1,
    end_msg_num=None,
):
    """Generate markdown for a subset of messages (used for splitting)."""
    is_agent = any(msg.get("isSidechain") for msg in part_messages)
    parent_session_id = part_messages[0].get("sessionId") if is_agent and part_messages else None
    agent_id = part_messages[0].get("agentId") if is_agent and part_messages else None

    md_lines = _generate_markdown_header(
        jsonl_file, part_messages, is_agent, part_num, total_parts, start_msg_num, end_msg_num
    )

    if is_agent:
        md_lines.extend(_generate_agent_notice(parent_session_id, agent_id))

    md_lines.extend(["", "---", ""])

    # Build UUID to message index map
    uuid_to_index = {
        msg["uuid"]: i for i, msg in enumerate(part_messages, start_msg_num) if msg.get("uuid")
    }

    # Generate messages
    for i, msg in enumerate(part_messages, start_msg_num):
        md_lines.extend(
            _generate_message_block(msg, i, is_agent, start_msg_num, minimal, uuid_to_index)
        )

    return "\n".join(md_lines)


# ============================================================================
# Codex Backend - JSONL Parsing
# ============================================================================


def codex_extract_content(payload: dict) -> str:
    """Extract text content from Codex message payload.

    Codex messages have content as either a string or an array of objects
    with type "input_text" (user) or "output_text" (assistant).

    Args:
        payload: The message payload dict containing content field

    Returns:
        Extracted text content as a string
    """
    content = payload.get("content", [])
    if isinstance(content, str):
        return content
    parts = []
    for item in content:
        if isinstance(item, dict) and item.get("type") in ("input_text", "output_text"):
            parts.append(item.get("text", ""))
    return "\n".join(parts)


def codex_format_function_call(payload: dict) -> str:
    """Format a Codex function_call payload as markdown.

    Args:
        payload: The function_call payload dict

    Returns:
        Formatted markdown string for the tool call
    """
    name = payload.get("name", "unknown")
    args = payload.get("arguments", "{}")
    call_id = payload.get("call_id", "")
    return f"**[Tool: {name}]**\nCall ID: `{call_id}`\n```json\n{args}\n```"


def codex_format_function_result(payload: dict) -> str:
    """Format a Codex function_call_output payload as markdown.

    Args:
        payload: The function_call_output payload dict

    Returns:
        Formatted markdown string for the tool result
    """
    call_id = payload.get("call_id", "")
    output = payload.get("output", "")
    return f"**[Tool Result]**\nCall ID: `{call_id}`\n```\n{output}\n```"


def codex_read_jsonl_messages(jsonl_file: Path) -> tuple:
    """Read messages from Codex rollout JSONL file.

    Parses the Codex JSONL format which uses a {timestamp, type, payload}
    envelope structure. Extracts session metadata and all messages including
    function calls and results.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Tuple of (messages_list, session_meta_dict or None)
        Messages contain: role, content, timestamp, and optionally
        is_tool_call or is_tool_result flags
    """
    messages = []
    session_meta = None

    with open(jsonl_file, encoding="utf-8") as f:
        for line in f:
            try:
                entry = json.loads(line)
                entry_type = entry.get("type")
                timestamp = entry.get("timestamp", "")
                payload = entry.get("payload", {})

                if entry_type == "session_meta":
                    session_meta = payload
                elif entry_type == "response_item":
                    payload_type = payload.get("type")
                    if payload_type == "message":
                        messages.append(
                            {
                                "role": payload.get("role"),
                                "content": codex_extract_content(payload),
                                "timestamp": timestamp,
                            }
                        )
                    elif payload_type in ("function_call", "custom_tool_call"):
                        messages.append(
                            {
                                "role": "assistant",
                                "content": codex_format_function_call(payload),
                                "timestamp": timestamp,
                                "is_tool_call": True,
                            }
                        )
                    elif payload_type in ("function_call_output", "custom_tool_call_output"):
                        messages.append(
                            {
                                "role": "tool",
                                "content": codex_format_function_result(payload),
                                "timestamp": timestamp,
                                "is_tool_result": True,
                            }
                        )
            except json.JSONDecodeError:
                continue

    return messages, session_meta


def codex_get_first_timestamp(jsonl_file: Path) -> str:
    """Get timestamp from Codex session's session_meta line.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        ISO 8601 timestamp string or None if not found
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            first_line = f.readline()
            entry = json.loads(first_line)
            if entry.get("type") == "session_meta":
                return entry.get("timestamp", "")
    except (OSError, json.JSONDecodeError):
        pass
    return None


def codex_parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False) -> str:
    """Convert Codex rollout JSONL to markdown format.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file
        minimal: If True, omit metadata sections

    Returns:
        Markdown formatted string of the conversation
    """
    messages, session_meta = codex_read_jsonl_messages(jsonl_file)

    md_lines = ["# Codex Conversation", ""]

    if session_meta and not minimal:
        md_lines.extend(
            [
                "## Session Metadata",
                "",
                f"- **Session ID:** `{session_meta.get('id', 'unknown')}`",
                f"- **Working Directory:** `{session_meta.get('cwd', 'unknown')}`",
                f"- **CLI Version:** `{session_meta.get('cli_version', 'unknown')}`",
                f"- **Source:** `{session_meta.get('source', 'unknown')}`",
                "",
            ]
        )

    md_lines.extend(["---", ""])

    for i, msg in enumerate(messages, 1):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        timestamp = msg.get("timestamp", "")

        if role == "user":
            md_lines.append(f"## 👤 User (Message {i})")
        elif role == "assistant":
            if msg.get("is_tool_call"):
                md_lines.append(f"## 🔧 Tool Call (Message {i})")
            else:
                md_lines.append(f"## 🤖 Assistant (Message {i})")
        elif role == "tool":
            md_lines.append(f"## 📤 Tool Result (Message {i})")
        else:
            md_lines.append(f"## {role.title()} (Message {i})")

        if timestamp and not minimal:
            md_lines.append(f"*{timestamp}*")

        md_lines.extend(["", content, "", "---", ""])

    return "\n".join(md_lines)


def codex_extract_metrics_from_jsonl(jsonl_file: Path) -> dict:
    """Extract metrics from Codex JSONL file for stats database.

    Mirror of extract_metrics_from_jsonl() for Codex format.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Dict with session, messages, and tool_uses data
    """
    messages, session_meta = codex_read_jsonl_messages(jsonl_file)

    metrics = {
        "session": {
            "id": session_meta.get("id") if session_meta else None,
            "cwd": session_meta.get("cwd") if session_meta else None,
            "cli_version": session_meta.get("cli_version") if session_meta else None,
            "model": None,
        },
        "messages": [],
        "tool_uses": [],
    }

    # Extract model from turn_context
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    if entry.get("type") == "turn_context":
                        metrics["session"]["model"] = entry.get("payload", {}).get("model")
                        break
                except json.JSONDecodeError:
                    continue
    except OSError:
        pass

    for msg in messages:
        if msg.get("is_tool_call"):
            # Extract tool name from content (format: "**[Tool: name]**...")
            content = msg.get("content", "")
            tool_name = "unknown"
            if "**[Tool:" in content:
                try:
                    tool_name = content.split("**[Tool:")[1].split("]**")[0].strip()
                except IndexError:
                    pass
            metrics["tool_uses"].append(
                {
                    "name": tool_name,
                    "timestamp": msg.get("timestamp"),
                }
            )
        elif not msg.get("is_tool_result"):
            metrics["messages"].append(
                {
                    "role": msg.get("role"),
                    "timestamp": msg.get("timestamp"),
                }
            )

    return metrics


# ============================================================================
# Codex Backend - Session Scanning
# ============================================================================


def codex_get_workspace_from_session(jsonl_file: Path) -> str:
    """Extract workspace (cwd) from Codex session's session_meta.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Encoded workspace name (e.g., '-home-user-project') or 'unknown'
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            first_line = f.readline()
            entry = json.loads(first_line)
            if entry.get("type") == "session_meta":
                cwd = entry.get("payload", {}).get("cwd", "")
                if cwd:
                    return path_to_encoded_workspace(cwd)
    except (OSError, json.JSONDecodeError):
        pass
    return "unknown"


def codex_count_messages(jsonl_file: Path) -> int:
    """Count user/assistant messages in a Codex session.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Number of user and assistant messages (excluding tool calls/results)
    """
    count = 0
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    if entry.get("type") == "response_item":
                        payload = entry.get("payload", {})
                        if payload.get("type") == "message":
                            count += 1
                except json.JSONDecodeError:
                    continue
    except OSError:
        pass
    return count


# ============================================================================
# Codex Session Index - Incremental workspace mapping
# ============================================================================

CODEX_INDEX_VERSION = 1


def codex_get_index_file() -> Path:
    """Get path to Codex session index file (~/.claude-history/codex_index.json)."""
    return get_config_dir() / "codex_index.json"


def codex_load_index() -> dict:
    """Load Codex session index from file.

    Returns:
        Index dict with keys: version, last_scan_date, sessions
        sessions maps session file path (str) to encoded workspace name
    """
    index_file = codex_get_index_file()
    if index_file.exists():
        try:
            with open(index_file, encoding="utf-8") as f:
                data = json.load(f)
                if data.get("version") == CODEX_INDEX_VERSION:
                    return data
        except (OSError, json.JSONDecodeError):
            pass
    return {"version": CODEX_INDEX_VERSION, "last_scan_date": None, "sessions": {}}


def codex_save_index(index: dict) -> None:
    """Save Codex session index to file."""
    index_file = codex_get_index_file()
    index_file.parent.mkdir(parents=True, exist_ok=True)
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(index, f, indent=2)


def _codex_parse_date_folder(folder_path: Path) -> str:
    """Parse YYYY/MM/DD folder structure to date string.

    Args:
        folder_path: Path like .../2025/12/15/rollout-xxx.jsonl

    Returns:
        Date string like "2025-12-15" or empty string if invalid
    """
    try:
        parts = folder_path.parts
        # Find YYYY/MM/DD pattern in path (last 4 parts before filename)
        if len(parts) >= CODEX_DATE_FOLDER_DEPTH:
            year, month, day = parts[-4], parts[-3], parts[-2]
            if year.isdigit() and month.isdigit() and day.isdigit():
                return f"{year}-{month}-{day}"
    except (ValueError, IndexError):
        pass
    return ""


def _codex_date_folders_since(sessions_dir: Path, since_date: str) -> list:  # noqa: C901
    """Get list of date folders on or after since_date.

    Args:
        sessions_dir: Base sessions directory (~/.codex/sessions/)
        since_date: Date string "YYYY-MM-DD" to start from (inclusive)

    Returns:
        List of Path objects for YYYY/MM/DD folders to scan
    """
    folders_to_scan = []

    if not sessions_dir.exists():
        return folders_to_scan

    since_dt = datetime.strptime(since_date, "%Y-%m-%d") if since_date else None

    # Walk through year folders
    for year_dir in sorted(sessions_dir.iterdir()):
        if not year_dir.is_dir() or not year_dir.name.isdigit():
            continue
        year = int(year_dir.name)

        # Quick skip: if entire year is before since_date
        if since_dt and year < since_dt.year:
            continue

        for month_dir in sorted(year_dir.iterdir()):
            if not month_dir.is_dir() or not month_dir.name.isdigit():
                continue
            month = int(month_dir.name)

            # Quick skip: if entire month is before since_date
            if since_dt and year == since_dt.year and month < since_dt.month:
                continue

            for day_dir in sorted(month_dir.iterdir()):
                if not day_dir.is_dir() or not day_dir.name.isdigit():
                    continue
                day = int(day_dir.name)

                # Check if this date is >= since_date
                if since_dt:
                    folder_date = datetime(year, month, day)
                    if folder_date.date() < since_dt.date():
                        continue

                folders_to_scan.append(day_dir)

    return folders_to_scan


def codex_ensure_index_updated(sessions_dir: Path = None) -> dict:
    """Ensure Codex session index is up-to-date.

    This is the common function used by all Codex operations. It:
    1. Loads existing index
    2. Scans only new date folders since last_scan_date
    3. Updates and saves the index
    4. Returns the session->workspace mapping

    Args:
        sessions_dir: Override sessions directory (for testing)

    Returns:
        Dict mapping session file paths (str) to encoded workspace names
    """
    if sessions_dir is None:
        sessions_dir = codex_get_home_dir()

    if not sessions_dir.exists():
        return {}

    index = codex_load_index()
    last_scan = index.get("last_scan_date")
    sessions_map = index.get("sessions", {})
    today = datetime.now().strftime("%Y-%m-%d")

    # Clean up stale entries (files that no longer exist)
    stale_keys = [k for k in sessions_map if not Path(k).exists()]
    for k in stale_keys:
        del sessions_map[k]

    # Determine what to scan
    if last_scan is None:
        # First run: scan everything
        folders_to_scan = _codex_date_folders_since(sessions_dir, None)
    else:
        # Incremental: scan from last_scan_date onwards
        # We re-scan last_scan_date in case new sessions were added that day
        folders_to_scan = _codex_date_folders_since(sessions_dir, last_scan)

    # Scan new folders
    for day_dir in folders_to_scan:
        for jsonl_file in day_dir.glob("rollout-*.jsonl"):
            file_key = str(jsonl_file)
            if file_key not in sessions_map:
                workspace = codex_get_workspace_from_session(jsonl_file)
                sessions_map[file_key] = workspace

    # Update and save index
    index["sessions"] = sessions_map
    index["last_scan_date"] = today
    codex_save_index(index)

    return sessions_map


def _codex_build_session_dict(
    jsonl_file: Path, workspace: str, modified: datetime, skip_message_count: bool
) -> dict:
    """Build a session dictionary for a Codex session file."""
    return {
        "agent": AGENT_CODEX,
        "workspace": workspace,
        "workspace_readable": normalize_workspace_name(workspace, verify_local=False),
        "file": jsonl_file,
        "filename": jsonl_file.name,
        "message_count": 0 if skip_message_count else codex_count_messages(jsonl_file),
        "modified": modified,
        "source": "local",
    }


def _codex_session_matches_filters(
    workspace: str, modified: datetime, pattern: str, since_date, until_date
) -> bool:
    """Check if a Codex session matches the given filters."""
    if pattern and pattern not in workspace:
        return False
    if since_date and modified.date() < since_date.date():
        return False
    if until_date and modified.date() > until_date.date():
        return False
    return True


def codex_scan_sessions(
    pattern: str = "",
    since_date=None,
    until_date=None,
    sessions_dir: Path = None,
    skip_message_count: bool = False,
) -> list:
    """Scan ~/.codex/sessions/YYYY/MM/DD/ for rollout-*.jsonl files.

    Uses incremental indexing for efficient workspace lookups. The index maps
    session files to workspaces and is updated incrementally based on date folders.

    Args:
        pattern: Substring pattern to filter workspaces (empty matches all)
        since_date: Only include sessions modified on or after this date
        until_date: Only include sessions modified on or before this date
        sessions_dir: Override sessions directory (for testing)
        skip_message_count: If True, skip counting messages (set to 0)

    Returns:
        List of session dicts sorted by modified time (newest first)
    """
    if sessions_dir is None:
        sessions_dir = codex_get_home_dir()

    if not sessions_dir.exists():
        return []

    # Get workspace mapping from incremental index
    sessions_map = codex_ensure_index_updated(sessions_dir)

    sessions = []
    # Walk through YYYY/MM/DD structure using glob
    for jsonl_file in sessions_dir.glob("*/*/*/rollout-*.jsonl"):
        file_key = str(jsonl_file)
        # Look up workspace from index (fallback to file read if not in index or empty)
        workspace = sessions_map.get(file_key)
        if not workspace:  # None or empty string
            workspace = codex_get_workspace_from_session(jsonl_file)
            # Update index with recomputed workspace if we had to fall back
            if workspace and file_key in sessions_map:
                sessions_map[file_key] = workspace

        modified = datetime.fromtimestamp(jsonl_file.stat().st_mtime)

        if _codex_session_matches_filters(workspace, modified, pattern, since_date, until_date):
            sessions.append(
                _codex_build_session_dict(jsonl_file, workspace, modified, skip_message_count)
            )

    return sorted(sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Gemini Backend - JSON Parsing
# ============================================================================


def gemini_read_json_messages(json_file: Path) -> tuple:  # noqa: C901
    """Read messages from Gemini CLI JSON session file.

    Gemini stores sessions as single JSON files (not JSONL) with structure:
    {sessionId, projectHash, startTime, lastUpdated, messages: [...]}

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Tuple of (messages_list, session_meta_dict or None)
        Messages contain: role, content, timestamp, and optionally
        is_tool_call, thoughts, tokens, model fields
    """
    messages = []
    session_meta = None

    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)

        session_meta = {
            "sessionId": data.get("sessionId"),
            "projectHash": data.get("projectHash"),
            "startTime": data.get("startTime"),
            "lastUpdated": data.get("lastUpdated"),
            "summary": data.get("summary"),
        }

        for msg in data.get("messages", []):
            msg_type = msg.get("type", "")
            content = msg.get("content", "")

            # Handle content that may be a string or PartListUnion
            if isinstance(content, list):
                # Extract content from parts array (handles various part types)
                text_parts = []
                for part in content:
                    if isinstance(part, str):
                        text_parts.append(part)
                    elif isinstance(part, dict):
                        if "text" in part:
                            text_parts.append(part["text"])
                        elif "inlineData" in part:
                            # Binary data (images, etc.) - show placeholder
                            mime = part["inlineData"].get("mimeType", "unknown")
                            text_parts.append(f"[Inline data: {mime}]")
                        elif "executableCode" in part:
                            # Code block
                            code = part["executableCode"]
                            lang = code.get("language", "")
                            text_parts.append(f"```{lang}\n{code.get('code', '')}\n```")
                        elif "codeExecutionResult" in part:
                            # Code execution result
                            result = part["codeExecutionResult"]
                            text_parts.append(f"**Output:**\n```\n{result.get('output', '')}\n```")
                content = "\n".join(text_parts)

            if msg_type == "user":
                messages.append(
                    {
                        "role": "user",
                        "content": content,
                        "timestamp": msg.get("timestamp", ""),
                    }
                )
            elif msg_type == "gemini":
                messages.append(
                    {
                        "role": "assistant",
                        "content": content,
                        "timestamp": msg.get("timestamp", ""),
                        "thoughts": msg.get("thoughts", []),
                        "tokens": msg.get("tokens"),
                        "model": msg.get("model"),
                        "tool_calls": msg.get("toolCalls", []),
                    }
                )
            elif msg_type in ("info", "error", "warning"):
                messages.append(
                    {
                        "role": msg_type,
                        "content": content,
                        "timestamp": msg.get("timestamp", ""),
                    }
                )

    except (OSError, json.JSONDecodeError):
        pass

    return messages, session_meta


def gemini_format_tool_call(tool_call: dict) -> str:
    """Format a Gemini tool call as markdown.

    Args:
        tool_call: Tool call dict with id, name, args, result, status, etc.

    Returns:
        Formatted markdown string for the tool call
    """
    name = tool_call.get("displayName") or tool_call.get("name", "unknown")
    args = tool_call.get("args", {})
    status = tool_call.get("status", "")
    result = tool_call.get("result", [])

    lines = [f"**[Tool: {name}]** ({status})"]

    if args:
        try:
            args_str = json.dumps(args, indent=2)
        except (TypeError, ValueError):
            args_str = str(args)
        lines.append(f"```json\n{args_str}\n```")

    # Extract result output if present
    if result:
        for r in result:
            if isinstance(r, dict):
                func_resp = r.get("functionResponse", {})
                output = func_resp.get("response", {}).get("output", "")
                if output:
                    # Truncate very long outputs
                    if len(output) > MAX_TOOL_OUTPUT_LEN:
                        output = output[:MAX_TOOL_OUTPUT_LEN] + "\n... [truncated]"
                    lines.append(f"**Result:**\n```\n{output}\n```")

    return "\n".join(lines)


def gemini_format_thoughts(thoughts: list) -> str:
    """Format Gemini reasoning thoughts as markdown.

    Args:
        thoughts: List of thought dicts with subject, description, timestamp
                  (or strings for legacy/alternative formats)

    Returns:
        Formatted markdown string for the thoughts
    """
    if not thoughts:
        return ""

    lines = ["", "**Reasoning:**"]
    for thought in thoughts:
        # Handle both dict format and potential string format
        if isinstance(thought, str):
            if thought.strip():
                truncated = len(thought) > MAX_THOUGHT_LEN
                text = thought[:MAX_THOUGHT_LEN] + "..." if truncated else thought
                lines.append(f"- {text}")
        elif isinstance(thought, dict):
            subject = thought.get("subject", "")
            description = thought.get("description", "")
            truncated = len(description) > MAX_THOUGHT_LEN
            desc_text = description[:MAX_THOUGHT_LEN] + ("..." if truncated else "")
            if subject:
                lines.append(f"- **{subject}**: {desc_text}")
            elif description:
                lines.append(f"- {desc_text}")
        # Skip other types silently

    return "\n".join(lines)


def gemini_get_first_timestamp(json_file: Path) -> str:
    """Get startTime from Gemini session.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        ISO 8601 timestamp string or None if not found
    """
    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
            return data.get("startTime", "")
    except (OSError, json.JSONDecodeError):
        pass
    return None


def gemini_parse_json_to_markdown(json_file: Path, minimal: bool = False) -> str:  # noqa: C901
    """Convert Gemini session JSON to markdown format.

    Args:
        json_file: Path to the Gemini session .json file
        minimal: If True, omit metadata sections

    Returns:
        Markdown formatted string of the conversation
    """
    messages, session_meta = gemini_read_json_messages(json_file)

    md_lines = ["# Gemini Conversation", ""]

    if session_meta and not minimal:
        project_hash = session_meta.get("projectHash", "unknown")
        # Show first 8 chars of hash for readability
        short_hash = (
            project_hash[:HASH_DISPLAY_LEN]
            if len(project_hash) > HASH_DISPLAY_LEN
            else project_hash
        )
        md_lines.extend(
            [
                "## Session Metadata",
                "",
                f"- **Session ID:** `{session_meta.get('sessionId', 'unknown')}`",
                f"- **Project Hash:** `{short_hash}...`",
                f"- **Start Time:** `{session_meta.get('startTime', 'unknown')}`",
                f"- **Last Updated:** `{session_meta.get('lastUpdated', 'unknown')}`",
                "",
            ]
        )
        if session_meta.get("summary"):
            md_lines.extend(
                [
                    "### Summary",
                    "",
                    session_meta["summary"],
                    "",
                ]
            )

    md_lines.extend(["---", ""])

    for i, msg in enumerate(messages, 1):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        timestamp = msg.get("timestamp", "")
        thoughts = msg.get("thoughts", [])
        tool_calls = msg.get("tool_calls", [])
        tokens = msg.get("tokens")
        model = msg.get("model")

        if role == "user":
            md_lines.append(f"## 👤 User (Message {i})")
        elif role == "assistant":
            md_lines.append(f"## 🤖 Gemini (Message {i})")
        elif role == "info":
            md_lines.append(f"## Info (Message {i})")
        elif role == "error":
            md_lines.append(f"## ❌ Error (Message {i})")
        elif role == "warning":
            md_lines.append(f"## ⚠️ Warning (Message {i})")
        else:
            md_lines.append(f"## {role.title()} (Message {i})")

        if timestamp and not minimal:
            md_lines.append(f"*{timestamp}*")

        if model and not minimal:
            md_lines.append(f"*Model: {model}*")

        md_lines.append("")

        # Add thoughts/reasoning if present
        if thoughts and not minimal:
            md_lines.append(gemini_format_thoughts(thoughts))
            md_lines.append("")

        # Add content
        if content:
            md_lines.append(content)
            md_lines.append("")

        # Add tool calls if present
        for tc in tool_calls:
            md_lines.append(gemini_format_tool_call(tc))
            md_lines.append("")

        # Add token usage if present and not minimal
        if tokens and not minimal:
            md_lines.append(
                f"*Tokens: {tokens.get('total', 0)} total "
                f"(in: {tokens.get('input', 0)}, out: {tokens.get('output', 0)}, "
                f"cached: {tokens.get('cached', 0)})*"
            )
            md_lines.append("")

        md_lines.extend(["---", ""])

    return "\n".join(md_lines)


def gemini_extract_metrics_from_json(json_file: Path) -> dict:
    """Extract metrics from Gemini JSON file for stats database.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Dict with session, messages, and tool_uses data
    """
    messages, session_meta = gemini_read_json_messages(json_file)

    metrics = {
        "session": {
            "id": session_meta.get("sessionId") if session_meta else None,
            "cwd": session_meta.get("projectHash")
            if session_meta
            else None,  # Use hash as workspace
            "cli_version": None,  # Gemini doesn't store CLI version in session
            "model": None,
        },
        "messages": [],
        "tool_uses": [],
        "tokens": {
            "input": 0,
            "output": 0,
            "total": 0,
        },
    }

    for msg in messages:
        role = msg.get("role")
        timestamp = msg.get("timestamp")

        # Extract model from first assistant message
        if role == "assistant" and not metrics["session"]["model"]:
            metrics["session"]["model"] = msg.get("model")

        # Accumulate token usage
        tokens = msg.get("tokens")
        if tokens:
            metrics["tokens"]["input"] += tokens.get("input", 0)
            metrics["tokens"]["output"] += tokens.get("output", 0)
            metrics["tokens"]["total"] += tokens.get("total", 0)

        # Track tool calls
        for tc in msg.get("tool_calls", []):
            status = tc.get("status", "")
            metrics["tool_uses"].append(
                {
                    "name": tc.get("name", "unknown"),
                    "timestamp": tc.get("timestamp") or timestamp,
                    "is_error": status.lower() in ("error", "failed", "failure")
                    if status
                    else False,
                }
            )

        # Track messages (exclude system messages)
        if role in ("user", "assistant"):
            msg_data = {
                "role": role,
                "timestamp": timestamp,
                "model": msg.get("model"),
            }
            # Include per-message token usage for stats
            if tokens:
                msg_data["input_tokens"] = tokens.get("input", 0)
                msg_data["output_tokens"] = tokens.get("output", 0)
            metrics["messages"].append(msg_data)

    return metrics


# ============================================================================
# Gemini Hash Index - Progressive hash-to-path mapping
# ============================================================================

GEMINI_HASH_INDEX_VERSION = 1


def gemini_get_hash_index_file() -> Path:
    """Get path to Gemini hash index file (~/.claude-history/gemini_hash_index.json)."""
    return get_config_dir() / "gemini_hash_index.json"


def gemini_load_hash_index() -> dict:
    """Load Gemini hash-to-path index from file.

    Returns:
        Index dict with keys: version, hashes
        hashes maps project hash (str) to absolute path (str)
    """
    index_file = gemini_get_hash_index_file()
    if index_file.exists():
        try:
            with open(index_file, encoding="utf-8") as f:
                data = json.load(f)
                if data.get("version") == GEMINI_HASH_INDEX_VERSION:
                    return data
        except (OSError, json.JSONDecodeError):
            pass
    return {"version": GEMINI_HASH_INDEX_VERSION, "hashes": {}}


def gemini_save_hash_index(index: dict) -> None:
    """Save Gemini hash-to-path index to file."""
    index_file = gemini_get_hash_index_file()
    index_file.parent.mkdir(parents=True, exist_ok=True)
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(index, f, indent=2)


def gemini_compute_project_hash(path: Path) -> str:
    """Compute SHA-256 hash of a path, matching Gemini CLI's approach.

    Gemini CLI uses SHA-256 of the absolute path string as the project identifier.

    Args:
        path: Path to hash (will be resolved to absolute)

    Returns:
        SHA-256 hex digest of the absolute path string
    """
    import hashlib

    abs_path = str(path.resolve())
    return hashlib.sha256(abs_path.encode("utf-8")).hexdigest()


def gemini_update_hash_index_from_cwd() -> dict:
    """Update Gemini hash index based on current working directory.

    Called on each agent-history run to progressively build hash→path mapping.
    Checks if the current directory's hash exists in Gemini's session storage,
    and if so, records the mapping.

    Returns:
        Updated hash index dict
    """
    index = gemini_load_hash_index()
    cwd = Path.cwd()
    cwd_hash = gemini_compute_project_hash(cwd)

    # Check if this hash exists in Gemini's session storage
    gemini_dir = gemini_get_home_dir()
    hash_dir = gemini_dir / cwd_hash / "chats"

    if hash_dir.exists() and any(hash_dir.glob("session-*.json")):
        # Found Gemini sessions for this directory - record the mapping
        current_path = str(cwd.resolve())
        existing_path = index["hashes"].get(cwd_hash)

        if existing_path != current_path:
            index["hashes"][cwd_hash] = current_path
            gemini_save_hash_index(index)

    return index


def gemini_get_path_for_hash(project_hash: str) -> str | None:
    """Look up the real path for a Gemini project hash.

    Args:
        project_hash: SHA-256 hash of the project path

    Returns:
        Absolute path string if known, None otherwise
    """
    index = gemini_load_hash_index()
    return index["hashes"].get(project_hash)


def gemini_get_workspace_readable(workspace: str) -> str:
    """Get human-readable workspace name for a Gemini workspace identifier.

    Handles three cases:
    - Real path (starts with /): return as-is
    - SHA-256 hash: looks up in hash index, falls back to truncated hash
    - Encoded path (like -home-user-project): normalizes to readable path

    Args:
        workspace: Real path, encoded workspace path, or SHA-256 hash

    Returns:
        Readable workspace name (path or [hash:xxxxxxxx])
    """
    # If it's already a real path, return it directly
    if workspace.startswith("/"):
        return workspace

    # Try to look up as a hash in the index
    real_path = gemini_get_path_for_hash(workspace)
    if real_path:
        # Return the path directly - it's already readable
        return real_path

    # Fall back to truncated hash display
    if len(workspace) > HASH_DISPLAY_LEN:
        return f"[hash:{workspace[:HASH_DISPLAY_LEN]}]"
    return workspace


def gemini_add_paths_to_index(paths: list) -> dict:
    """Add explicit paths to the Gemini hash→path index.

    For each path, computes its SHA-256 hash and checks if Gemini has
    sessions for that hash. If sessions exist, adds the mapping to the index.

    Args:
        paths: List of Path objects to add to the index

    Returns:
        Dict with 'added', 'existing', 'no_sessions' counts and 'mappings' list
    """
    result = {"added": 0, "existing": 0, "no_sessions": 0, "mappings": []}

    if not paths:
        return result

    # Load existing index
    index = gemini_load_hash_index()
    gemini_dir = gemini_get_home_dir()

    for path in paths:
        resolved_path = path.resolve()
        project_hash = gemini_compute_project_hash(resolved_path)
        project_str = str(resolved_path)
        path_missing = not resolved_path.exists()

        # Check if sessions exist for this hash
        hash_dir = gemini_dir / project_hash / "chats"
        has_sessions = hash_dir.exists() and any(hash_dir.glob("session-*.json"))

        if not has_sessions:
            result["no_sessions"] += 1
            result["mappings"].append(
                {
                    "path": project_str,
                    "hash": project_hash[:HASH_DISPLAY_LEN],
                    "status": "no_sessions",
                    "path_missing": path_missing,
                }
            )
            continue

        # Check if already in index
        existing = index["hashes"].get(project_hash)
        if existing == project_str:
            result["existing"] += 1
            result["mappings"].append(
                {
                    "path": project_str,
                    "hash": project_hash[:HASH_DISPLAY_LEN],
                    "status": "existing",
                    "path_missing": path_missing,
                }
            )
            continue

        # Add to index
        index["hashes"][project_hash] = project_str
        result["added"] += 1
        result["mappings"].append(
            {
                "path": project_str,
                "hash": project_hash[:HASH_DISPLAY_LEN],
                "status": "added",
                "path_missing": path_missing,
            }
        )

    # Save updated index
    if result["added"] > 0:
        gemini_save_hash_index(index)

    return result


# ============================================================================
# Gemini Backend - Session Scanning
# ============================================================================


def gemini_get_workspace_from_session(json_file: Path) -> str:
    """Extract workspace identifier from Gemini session.

    Uses the hash index to get the real path if known, otherwise returns
    the hash. This ensures consistent workspace naming across scan/export/stats.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Encoded path (if hash→path known) or project hash, or 'unknown'
    """
    # The file is in ~/.gemini/tmp/<project_hash>/chats/<session>.json
    # So we need to go up two levels to get the project hash
    try:
        chats_dir = json_file.parent  # chats/
        project_dir = chats_dir.parent  # <project_hash>/
        project_hash = project_dir.name

        # Try to get the real path from hash index
        real_path = gemini_get_path_for_hash(project_hash)
        if real_path:
            # Return real path directly - encoding would mangle hyphens in folder names
            return real_path
        return project_hash
    except (AttributeError, IndexError):
        pass
    return "unknown"


def gemini_count_messages(json_file: Path) -> int:
    """Count user/assistant messages in a Gemini session.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Number of user and gemini messages
    """
    count = 0
    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
            for msg in data.get("messages", []):
                if msg.get("type") in ("user", "gemini"):
                    count += 1
    except (OSError, json.JSONDecodeError):
        pass
    return count


def _gemini_build_session_dict(
    json_file: Path, workspace: str, modified: datetime, skip_message_count: bool
) -> dict:
    """Build a session dictionary for a Gemini session file."""
    # Use hash index to get readable workspace name if known
    workspace_readable = gemini_get_workspace_readable(workspace)
    return {
        "agent": AGENT_GEMINI,
        "workspace": workspace,
        "workspace_readable": workspace_readable,
        "file": json_file,
        "filename": json_file.name,
        "message_count": 0 if skip_message_count else gemini_count_messages(json_file),
        "modified": modified,
        "source": "local",
    }


def _gemini_session_matches_filters(
    workspace: str, modified: datetime, pattern: str, since_date, until_date
) -> bool:
    """Check if a Gemini session matches the given filters.

    Pattern matching checks:
    1. The workspace identifier (encoded path or hash)
    2. The readable workspace name (for hash→path lookups)
    """
    if pattern:
        # Check workspace directly (encoded path or hash)
        if pattern in workspace:
            pass  # Match found
        else:
            # Also check readable name (handles hash→path from index)
            readable = gemini_get_workspace_readable(workspace)
            if pattern not in readable:
                return False
    if since_date and modified.date() < since_date.date():
        return False
    if until_date and modified.date() > until_date.date():
        return False
    return True


def gemini_scan_sessions(
    pattern: str = "",
    since_date=None,
    until_date=None,
    sessions_dir: Path = None,
    skip_message_count: bool = False,
) -> list:
    """Scan ~/.gemini/tmp/*/chats/ for session-*.json files.

    Args:
        pattern: Substring pattern to filter workspaces (empty matches all)
        since_date: Only include sessions modified on or after this date
        until_date: Only include sessions modified on or before this date
        sessions_dir: Override sessions directory (for testing)
        skip_message_count: If True, skip counting messages (set to 0)

    Returns:
        List of session dicts sorted by modified time (newest first)
    """
    if sessions_dir is None:
        sessions_dir = gemini_get_home_dir()

    if not sessions_dir.exists():
        return []

    sessions = []
    # Gemini stores sessions in ~/.gemini/tmp/<project_hash>/chats/session-*.json
    for json_file in sessions_dir.glob("*/chats/session-*.json"):
        workspace = gemini_get_workspace_from_session(json_file)
        modified = datetime.fromtimestamp(json_file.stat().st_mtime)

        if _gemini_session_matches_filters(workspace, modified, pattern, since_date, until_date):
            sessions.append(
                _gemini_build_session_dict(json_file, workspace, modified, skip_message_count)
            )

    return sorted(sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Unified Backend Dispatch
# ============================================================================


def get_active_backends(agent: str) -> list:
    """Return list of active backends based on agent flag.

    Args:
        agent: Agent selection - 'auto', 'claude', 'codex', or 'gemini'

    Returns:
        List of backend identifiers (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI) that exist
    """
    if agent == AGENT_CLAUDE:
        projects_dir = Path.home() / ".claude" / "projects"
        return [AGENT_CLAUDE] if projects_dir.exists() else []
    elif agent == AGENT_CODEX:
        return [AGENT_CODEX] if codex_get_home_dir().exists() else []
    elif agent == AGENT_GEMINI:
        return [AGENT_GEMINI] if gemini_get_home_dir().exists() else []
    else:  # auto
        backends = []
        projects_dir = Path.home() / ".claude" / "projects"
        if projects_dir.exists():
            backends.append(AGENT_CLAUDE)
        if codex_get_home_dir().exists():
            backends.append(AGENT_CODEX)
        if gemini_get_home_dir().exists():
            backends.append(AGENT_GEMINI)
        return backends


def get_unified_sessions(
    agent: str = "auto",
    pattern: str = "",
    since_date=None,
    until_date=None,
    skip_message_count: bool = False,
    **kwargs,
) -> list:
    """Get sessions from specified agent backend(s).

    Args:
        agent: Agent selection - 'auto', 'claude', 'codex', or 'gemini'
        pattern: Workspace pattern to filter by
        since_date: Filter to sessions modified on or after this date
        until_date: Filter to sessions modified on or before this date
        skip_message_count: If True, skip counting messages
        **kwargs: Additional arguments passed to backend functions

    Returns:
        List of session dicts from all active backends, sorted by modified (newest first)
    """
    all_sessions = []
    backends = get_active_backends(agent)

    for backend in backends:
        if backend == AGENT_CLAUDE:
            try:
                sessions = get_workspace_sessions(
                    pattern,
                    since_date=since_date,
                    until_date=until_date,
                    skip_message_count=skip_message_count,
                    **kwargs,
                )
                for s in sessions:
                    s["agent"] = AGENT_CLAUDE
                all_sessions.extend(sessions)
            except SystemExit:
                # get_claude_projects_dir() calls sys.exit if not found
                pass
        elif backend == AGENT_CODEX:
            sessions = codex_scan_sessions(
                pattern=pattern,
                since_date=since_date,
                until_date=until_date,
                skip_message_count=skip_message_count,
            )
            all_sessions.extend(sessions)
        elif backend == AGENT_GEMINI:
            sessions = gemini_scan_sessions(
                pattern=pattern,
                since_date=since_date,
                until_date=until_date,
                skip_message_count=skip_message_count,
            )
            all_sessions.extend(sessions)

    return sorted(all_sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Workspace Scanning
# ============================================================================


def get_claude_projects_dir():
    """Get the Claude projects directory, with error handling."""
    env_override = os.environ.get("CLAUDE_PROJECTS_DIR")
    if env_override:
        projects_dir = Path(env_override).expanduser()
    else:
        projects_dir = Path.home() / ".claude" / "projects"

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found at {projects_dir}\n")
        sys.exit(1)

    return projects_dir


def path_to_encoded_workspace(path: str) -> str:
    """Convert an absolute path to Claude's encoded workspace directory name.

    Args:
        path: Absolute path (e.g., '/home/user/my-project')

    Returns:
        Encoded workspace name (e.g., '-home-user-my-project')
    """
    # Remove trailing slash if present
    path = path.rstrip("/")

    # Handle WSL-mounted Windows paths like /mnt/c/Users/me/project
    if path.startswith("/mnt/") and len(path) > MIN_WSL_MNT_PATH_LEN:
        drive_letter = path[5]
        sep = path[MIN_WSL_MNT_PATH_LEN]
        if drive_letter.isalpha() and sep in ("/", "\\"):
            remainder = path[MIN_WSL_MNT_PATH_LEN + 1 :].replace("\\", "/").strip("/")
            normalized = remainder.replace("/", "-") if remainder else ""
            return f"{drive_letter.upper()}--{normalized}"

    # Replace / with - and add leading -
    if path.startswith("/"):
        return "-" + path[1:].replace("/", "-")
    else:
        return "-" + path.replace("/", "-")


def _convert_windows_path_to_encoded(path: str) -> str:
    """Convert Windows absolute path (C:\\... or C:/...) to encoded format."""
    drive = path[0].upper()
    rest = path[2:].lstrip("/\\").replace("\\", "/").replace("/", "-")
    return f"{drive}--{rest}"


def _get_workspaces_from_dir(projects_dir: Path) -> list:
    """Get native workspace names from a projects directory."""
    if not projects_dir:
        return []
    return [d.name for d in projects_dir.iterdir() if d.is_dir() and is_native_workspace(d.name)]


def _get_workspaces_for_source(source_key: str, args=None) -> list:
    """Get list of workspace names for a given source.

    Args:
        source_key: Source identifier ('local', 'remote:host', 'wsl:distro', 'windows[:user]')
        args: Arguments object for remote access

    Returns:
        List of workspace names for the source
    """
    if source_key.startswith("wsl:"):
        return _get_workspaces_from_dir(get_wsl_projects_dir(source_key[4:]))

    if source_key.startswith("windows"):
        user = source_key[8:] if source_key.startswith("windows:") else None
        return _get_workspaces_from_dir(get_windows_projects_dir(user))

    if source_key.startswith("remote:"):
        hostname = source_key[7:]
        remote_host = getattr(args, "remote", hostname) if args else hostname
        batch_ws = get_remote_workspaces_batch(remote_host)
        return [ws["encoded"] for ws in batch_ws] if batch_ws else []

    # Local
    return _get_workspaces_from_dir(get_claude_projects_dir())


def resolve_workspace_input(workspace_input: str, source_key: str = "local", args=None) -> list:
    """Resolve a workspace input (pattern, path, or encoded name) to actual encoded workspace names.

    Args:
        workspace_input: Can be:
            - Encoded name: '-home-user-project'
            - Absolute path: '/home/user/project'
            - Pattern: 'project' or 'my-project'
        source_key: Source to search in ('local', 'remote:host', 'wsl:distro', 'windows')
        args: Arguments object for remote access

    Returns:
        List of matching encoded workspace names
    """
    # Case 1: Already an encoded workspace name (starts with -)
    if workspace_input.startswith("-"):
        return [workspace_input]

    # Case 2: Absolute path - convert to encoded name
    if workspace_input.startswith("/"):
        return [path_to_encoded_workspace(workspace_input)]

    # Case 3: Windows absolute path (C:\... or C:/...)
    if len(workspace_input) > MIN_WINDOWS_PATH_LEN and workspace_input[1] == ":":
        return [_convert_windows_path_to_encoded(workspace_input)]

    # Case 4: Pattern - search for matching workspaces
    try:
        workspaces = _get_workspaces_for_source(source_key, args)
    except (OSError, PermissionError):
        return []

    return [ws for ws in workspaces if workspace_input in ws]


def _looks_like_windows_drive(path: str) -> bool:
    """Return True if the string resembles a Windows absolute path (C:/...)."""
    return len(path) > MIN_WINDOWS_PATH_LEN and path[1] == ":"


def _collapse_slashes(path: str) -> str:
    """Collapse duplicate slashes for consistent comparisons."""
    return re.sub(r"/{2,}", "/", path)


def _strip_wsl_unc_prefix(path: str) -> str:
    """Convert //wsl.localhost/Distro/... into /Distro/... form."""
    lowered = path.lower()
    if lowered.startswith("//wsl.localhost/") or lowered.startswith("//wsl$/"):
        parts = [p for p in path.split("/") if p]
        if len(parts) >= WSL_UNC_MIN_PARTS:
            return "/" + "/".join(parts[2:])
    return path


def _extract_workspace_from_projects_path(parts: list[str]) -> tuple[str | None, list[str]]:
    """Return workspace dir when path includes .claude/projects/<workspace>."""
    if ".claude" not in parts:
        return None, parts
    try:
        idx = parts.index(".claude")
    except ValueError:
        return None, parts
    if idx + 2 < len(parts) and parts[idx + 1] == "projects":
        workspace_dir = parts[idx + 2]
        remainder = parts[idx + 2 :]
        return workspace_dir, remainder
    return None, parts


def _projects_dir_from_wsl_unc(path: str) -> Path:
    """Best-effort guess of ~/.claude/projects for a WSL UNC input."""
    parts = [p for p in path.split("/") if p]
    distro = parts[1] if len(parts) > 1 else None
    try:
        home_idx = parts.index("home")
        user = parts[home_idx + 1] if len(parts) > home_idx + 1 else None
    except ValueError:
        user = None
    if distro and user:
        return Path(f"//wsl.localhost/{distro}/home/{user}/.claude/projects")
    if distro:
        return Path(f"//wsl.localhost/{distro}/home/.claude/projects")
    return Path(path).parent


def _is_already_encoded_or_special(target: str) -> bool:
    """Check if target is already encoded, an alias, or a jsonl file."""
    if not target:
        return True
    if target.startswith("-") or target.startswith("@"):
        return True
    if target.lower().endswith(".jsonl"):
        return True
    return False


def _try_extract_workspace_from_absolute_path(expanded: str, parts: list) -> str | None:
    """Try to extract workspace pattern from an absolute path ending in '-...'."""
    try:
        if Path(expanded).is_absolute() and parts and parts[-1].startswith("-"):
            return parts[-1]
    except OSError:
        pass
    return None


def _coerce_target_to_workspace_pattern(target: str) -> str:
    """Convert absolute filesystem paths into Claude workspace patterns."""
    if _is_already_encoded_or_special(target):
        return target

    expanded = os.path.expanduser(target).strip()
    if not expanded:
        return target

    normalized = _collapse_slashes(_strip_wsl_unc_prefix(expanded.replace("\\", "/")))
    parts = [p for p in normalized.split("/") if p]

    # Direct absolute path that already points to a workspace dir
    workspace_from_path = _try_extract_workspace_from_absolute_path(expanded, parts)
    if workspace_from_path:
        return workspace_from_path

    workspace_dir, remainder = _extract_workspace_from_projects_path(parts)
    if workspace_dir:
        if workspace_dir.startswith("-"):
            return workspace_dir
        normalized = "/" + "/".join(remainder)
    elif normalized.startswith("/wsl.localhost/") or normalized.startswith("/wsl$/"):
        split_parts = normalized.split("/")
        if len(split_parts) >= WSL_LOCALHOST_SKIP_PARTS:
            normalized = "/" + "/".join(split_parts[3:])

    if normalized.startswith("/"):
        return path_to_encoded_workspace(normalized)

    if _looks_like_windows_drive(normalized):
        return _convert_windows_path_to_encoded(expanded)

    return target


def _is_windows_encoded_path(name: str) -> bool:
    """Check if name is a Windows-style encoded path (e.g., 'C--path-to-dir')."""
    return len(name) >= MIN_ENCODED_PATH_LEN and name[1:3] == "--"


def _resolve_path_segments(parts: list, base_path: Path) -> list:
    """Resolve dash-separated parts to actual filesystem path segments.

    Tries progressively longer combinations of parts to handle directory
    names that contain dashes (e.g., 'my-project' encoded as 'my-project').

    Args:
        parts: List of dash-split path components
        base_path: Base path to verify against filesystem

    Returns:
        List of resolved path segments
    """
    path_segments = []
    i = 0

    while i < len(parts):
        # Build current path to check children against
        if path_segments:
            current_path = base_path / "/".join(path_segments)
        else:
            current_path = base_path

        # Try progressively longer combinations (longest first)
        best_match = None
        best_match_len = 0

        unc_mode = _is_wsl_unc_path(base_path)

        for j in range(len(parts), i, -1):
            candidate_segment = "-".join(parts[i:j])
            candidate_path = current_path / candidate_segment

            exists = candidate_path.exists()
            is_directory = candidate_path.is_dir() if not unc_mode else True
            if exists and is_directory:
                best_match = candidate_segment
                best_match_len = j - i
                break

        if best_match:
            path_segments.append(best_match)
            i += best_match_len
        else:
            path_segments.append(parts[i])
            i += 1

    return path_segments


def _generate_merge_segments(parts: list, mask: int) -> list:
    """Generate segment list from parts based on merge mask."""
    segments = []
    current = parts[0]
    for i in range(1, len(parts)):
        if mask & (1 << (i - 1)):
            current += "-" + parts[i]
        else:
            segments.append(current)
            current = parts[i]
    segments.append(current)
    return segments


def _find_deepest_existing_index(segments: list, base_path: Path) -> int:
    """Find the deepest index where the path exists under base_path."""
    deepest = -1
    probe = base_path
    for idx, seg in enumerate(segments):
        probe = probe.joinpath(seg)
        try:
            if probe.exists():
                deepest = idx
            else:
                break
        except OSError:
            break
    return deepest


def _get_fallback_from_base(base_path: Path, parts: list) -> tuple:
    """Return first child dir as fallback, or original parts if nothing found."""
    try:
        existing = [
            child.name
            for child in base_path.iterdir()
            if child.is_dir() and not child.name.startswith(".")
        ]
        if existing:
            return [existing[0]], False
    except OSError:
        pass
    return parts, False


def _resolve_existing_wsl_path(parts: list, base_path: Path) -> tuple:
    """Try all segment merges to find the deepest existing path under base_path.

    This is used for UNC WSL paths on Windows where directory names may contain
    dashes that were split into separate parts. Returns the best-matching
    segment list (trimmed to the deepest existing prefix). If nothing exists,
    returns the original parts and a False flag.
    """
    if not base_path or not parts:
        return parts, False

    best_segments = None
    best_depth = -1
    best_full_segments = None
    n = len(parts)

    for mask in range(1 << (n - 1)):
        segments = _generate_merge_segments(parts, mask)
        deepest = _find_deepest_existing_index(segments, base_path)

        # Decide whether to update best result
        should_update = deepest > best_depth
        if best_segments is not None and deepest == best_depth:
            should_update = len(segments) < len(best_segments)

        if should_update:
            best_depth = deepest
            best_segments = segments[: deepest + 1] if deepest >= 0 else None
            if deepest == len(segments) - 1:
                best_full_segments = segments

        # Track best full match
        if deepest == len(segments) - 1:
            if best_full_segments is None or len(segments) < len(best_full_segments):
                best_full_segments = segments

    if best_full_segments is not None:
        return best_full_segments, True
    if best_segments is not None:
        return best_segments, False

    return _get_fallback_from_base(base_path, parts)


def _normalize_windows_path(workspace_dir_name: str, verify_local: bool) -> str:
    """Normalize a Windows-style encoded path (e.g., 'C--sankar-projects').

    Args:
        workspace_dir_name: Encoded name starting with drive letter and '--'
        verify_local: If True, verify against /mnt/<drive>/ filesystem

    Returns:
        Decoded path (e.g., 'C:\\sankar\\projects' on Windows, '/mnt/c/...' on WSL)
    """
    drive_letter = workspace_dir_name[0].upper()
    rest = workspace_dir_name[3:]  # Skip 'C--'
    parts = rest.split("-")

    # On Windows, return a native drive path (prefer verified segments)
    if verify_local and sys.platform == "win32":
        drive_root = Path(f"{drive_letter}:\\")
        resolved = _resolve_path_segments(parts, drive_root)
        return str(drive_root.joinpath(*resolved))

    # On WSL, prefer /mnt/<drive> if available
    if verify_local:
        mnt_base = Path(f"/mnt/{drive_letter.lower()}")
        if mnt_base.exists():
            path_segments = _resolve_path_segments(parts, mnt_base)
            if path_segments:
                # Return a WSL-usable path: /mnt/<drive>/<segments>
                return "/mnt/" + drive_letter.lower() + "/" + "/".join(path_segments)

    # Fallback: POSIX-style drive path
    return f"/{drive_letter}/" + rest.replace("-", "/")


def _format_unc_path(base_path: Path, readable_path: str) -> str:
    """Format a UNC path string from a base and POSIX-readable path."""
    base_str = str(base_path)
    if base_str.startswith("//"):
        base_str = "\\" + base_str.lstrip("/")
    if not base_str.startswith("\\\\"):
        base_str = "\\\\" + base_str.lstrip("\\")
    base_str = base_str.replace("/", "\\").rstrip("\\")
    suffix = readable_path.lstrip("/")
    if suffix:
        suffix = suffix.replace("/", "\\")
        return f"{base_str}\\{suffix}"
    return base_str


def _is_wsl_unc_path(path: Path | None) -> bool:
    if not path:
        return False
    base_str = str(path)
    # Normalize extended UNC (\\?\UNC\wsl.localhost\...) to standard UNC
    if base_str.startswith("\\\\?\\UNC\\"):
        base_str = "\\\\" + base_str[8:]
    return (
        base_str.startswith("\\\\wsl.localhost\\")
        or base_str.startswith("\\\\wsl$\\")
        or base_str.startswith("//wsl.localhost/")
        or base_str.startswith("//wsl$/")
    )


def _normalize_unix_path(encoded: str, verify_local: bool, base_path: Path = None) -> str:
    """Normalize a Unix-style encoded path (e.g., 'home-user-my-project').

    Args:
        encoded: Encoded path without leading dash
        verify_local: If True, verify against filesystem
        base_path: Base path for verification (e.g., //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., '/home/user/my-project')
    """
    parts = encoded.split("-")

    if base_path and (_is_wsl_unc_path(base_path) or sys.platform == "win32"):
        resolved_segments, found_full = _resolve_existing_wsl_path(parts, Path(base_path))
        segments = list(resolved_segments) if resolved_segments else list(parts)
        if not found_full and len(segments) < len(parts):
            segments.append("-".join(parts[len(segments) :]))
        readable_path = "/" + "/".join(segments)
        marker = "" if found_full else " [missing]"
        base_str = str(base_path)
        if base_str.startswith("/") and not base_str.startswith("//"):
            return readable_path + marker
        return _format_unc_path(base_path, readable_path) + marker

    if not verify_local:
        return "/" + encoded.replace("-", "/")

    effective_base = base_path if base_path else Path("/")
    path_segments = _resolve_path_segments(parts, effective_base)

    # If resolution found no real merges (pure fallback), keep hyphens in the last segment
    if path_segments == parts:
        path_segments = (
            [*parts[:2], "-".join(parts[2:])] if len(parts) > MIN_WINDOWS_PATH_LEN else parts
        )

    path_segments, partial_marker = _apply_windows_base_resolution(parts, base_path, path_segments)

    readable_path = "/" + "/".join(path_segments)
    return readable_path + partial_marker


def _apply_windows_base_resolution(parts: list[str], base_path: Path, current_segments: list[str]):
    """Handle Windows base paths when normalizing Unix-style encodings."""
    if not base_path or sys.platform != "win32":
        return current_segments, ""

    resolved_segments, found_full = _resolve_existing_wsl_path(parts, Path(base_path))
    if not resolved_segments:
        return current_segments, ""

    combined_segments = list(resolved_segments)
    if not found_full and len(resolved_segments) < len(parts):
        combined_segments.append("-".join(parts[len(resolved_segments) :]))

    final_path = Path(base_path).joinpath(*combined_segments)
    try:
        full_exists = len(resolved_segments) == len(parts) and final_path.exists()
    except OSError:
        full_exists = False

    if full_exists:
        return combined_segments, ""
    return combined_segments, " [missing]"


def normalize_workspace_name(
    workspace_dir_name: str, verify_local: bool = True, base_path: Path = None
) -> str:
    """Convert workspace directory name to readable path.

    Claude Code encodes workspace paths by replacing '/' with '-'. This function
    reverses that encoding, optionally verifying against the filesystem to
    correctly handle directory names that contain dashes.

    Args:
        workspace_dir_name: Encoded workspace name (e.g., '-home-user-my-project')
        verify_local: If True, verify against local filesystem to handle dashes correctly
        base_path: Base path to prepend when verifying (for WSL: //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., '/home/user/my-project')
    """
    # Handle Windows-style paths (e.g., 'C--sankar-projects-claude-history')
    if _is_windows_encoded_path(workspace_dir_name):
        return _normalize_windows_path(workspace_dir_name, verify_local)

    # Remove leading dash for Unix paths
    encoded = workspace_dir_name[1:] if workspace_dir_name.startswith("-") else workspace_dir_name

    return _normalize_unix_path(encoded, verify_local, base_path)


def get_current_workspace_pattern():
    """
    Detect the current workspace based on the current working directory.
    Returns a pattern that can be used to match the workspace directory.
    """
    cwd = Path.cwd()
    cwd_str = str(cwd)

    # Handle Windows paths (C:\path\to\project -> C--path-to-project)
    if sys.platform == "win32" and len(cwd_str) >= MIN_WINDOWS_PATH_LEN and cwd_str[1] == ":":
        # Extract drive letter and path
        drive = cwd_str[0]
        path_part = cwd_str[2:].lstrip("\\").lstrip("/")
        # Convert backslashes and forward slashes to dashes
        path_part = path_part.replace("\\", "-").replace("/", "-")
        workspace_pattern = f"{drive}--{path_part}"
    else:
        # Unix/Linux paths (/home/user/projects/myapp -> home-user-projects-myapp)
        workspace_pattern = cwd_str.lstrip("/").replace("/", "-")

    return workspace_pattern


def check_current_workspace_exists():
    """
    Check if the current directory corresponds to a known Claude Code workspace.
    Returns (pattern, exists) tuple.
    """
    pattern = get_current_workspace_pattern()
    projects_dir = get_claude_projects_dir()

    if not projects_dir or not projects_dir.exists():
        return pattern, False

    # Check if any workspace directory matches the pattern
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue
        # Skip cached remote/wsl directories
        dirname = workspace_dir.name
        if is_cached_workspace(dirname):
            continue
        if pattern in dirname:
            return pattern, True

    return pattern, False


def _detect_wsl_base_path(projects_dir: Path) -> Path:
    """Detect WSL base path from projects directory, or None if not WSL.

    Handles both str and bytes forms for compatibility with tests that supply
    dummy path objects.
    """
    import os as _os

    p = _os.fspath(projects_dir)
    if isinstance(p, bytes):
        p = p.decode(errors="ignore")

    is_wsl_unc = any(
        p.startswith(prefix)
        for prefix in ("\\\\wsl.localhost\\", "//wsl.localhost/", "\\\\wsl$\\", "//wsl$/")
    )
    if not is_wsl_unc:
        return None

    # Use the UNC anchor (e.g., \\wsl.localhost\\Ubuntu) as the base path
    anchor = PureWindowsPath(p).anchor
    return Path(anchor.rstrip("\\/")) if anchor else None


def _should_skip_workspace(dir_name: str, include_cached: bool) -> bool:
    """Check if a workspace directory should be skipped."""
    if not include_cached:
        if is_cached_workspace(dir_name):
            return True
        if dir_name.startswith("-remote-") or dir_name.startswith("--wsl-"):
            return True
    return False


def _count_file_messages(jsonl_file: Path, skip_count: bool) -> int:
    """Count messages in a JSONL file."""
    if skip_count:
        return 0
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            return sum(1 for _ in f)
    except OSError:
        return 0


def _get_session_from_file(
    jsonl_file: Path, workspace_dir: Path, readable_name: str, skip_message_count: bool
) -> dict:
    """Build session dict from a JSONL file."""
    stat = jsonl_file.stat()
    return {
        "workspace": workspace_dir.name,
        "workspace_readable": readable_name,
        "file": jsonl_file,
        "filename": jsonl_file.name,
        "size_kb": stat.st_size / 1024,
        "modified": datetime.fromtimestamp(stat.st_mtime),
        "message_count": _count_file_messages(jsonl_file, skip_message_count),
    }


def _is_session_in_date_range(session: dict, since_date, until_date) -> bool:
    """Check if session modification date is within the specified range."""
    modified_date = session["modified"].replace(hour=0, minute=0, second=0, microsecond=0)
    if since_date and modified_date < since_date:
        return False
    if until_date and modified_date > until_date:
        return False
    return True


def _workspace_matches_pattern(dir_name: str, workspace_pattern: str, match_all: bool) -> bool:
    """Check if workspace matches the pattern."""
    if match_all:
        return True
    normalized = workspace_pattern.replace("\\", "-").replace("/", "-")
    tail = workspace_pattern.replace("\\", "/").split("/")[-1]
    return workspace_pattern in dir_name or normalized in dir_name or (tail and tail in dir_name)


def get_workspace_sessions(
    workspace_pattern: str,
    quiet: bool = False,
    since_date=None,
    until_date=None,
    include_cached: bool = False,
    projects_dir: Path = None,
    skip_message_count: bool = False,
):
    """Find all Claude Code sessions in workspaces matching the pattern.

    Args:
        workspace_pattern: Substring to match workspace names. Use "", "*", or "all" to match all.
        quiet: Suppress progress output if True
        since_date: Filter to sessions modified on or after this date
        until_date: Filter to sessions modified on or before this date
        include_cached: If True, include remote_* and wsl_* cached workspaces
        projects_dir: Explicit projects directory path (default: auto-detect)
        skip_message_count: If True, skip counting messages (faster for slow filesystems)

    Returns:
        List of session dicts with workspace, file, and metadata info
    """
    if projects_dir is None:
        projects_dir = get_claude_projects_dir()

    match_all = workspace_pattern in ("", "*", "all")
    wsl_base = _detect_wsl_base_path(projects_dir)
    sessions = []

    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue

        dir_name = workspace_dir.name
        if not validate_workspace_name(dir_name) or not is_safe_path(projects_dir, workspace_dir):
            continue
        if _should_skip_workspace(dir_name, include_cached):
            continue
        if not _workspace_matches_pattern(dir_name, workspace_pattern, match_all):
            continue

        readable_name = normalize_workspace_name(dir_name, base_path=wsl_base)

        for jsonl_file in workspace_dir.glob("*.jsonl"):
            session = _get_session_from_file(
                jsonl_file, workspace_dir, readable_name, skip_message_count
            )
            if _is_session_in_date_range(session, since_date, until_date):
                sessions.append(session)

    sessions.sort(key=lambda s: s["modified"])
    return sessions


# ============================================================================
# WSL Access
# ============================================================================


def is_running_in_wsl() -> bool:
    """Detect if we're running inside WSL."""
    try:
        with open("/proc/version") as f:
            content = f.read().lower()
            return "microsoft" in content or "wsl" in content
    except OSError:
        return False


def _find_user_home_on_drives(username: str) -> Path:
    """Find Windows user home by username across all drives."""
    mnt = Path("/mnt")
    if not mnt.exists():
        return None
    for drive in sorted(mnt.iterdir()):
        if drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha():
            user_path = drive / "Users" / username
            if user_path.exists() and (user_path / ".claude" / "projects").exists():
                return user_path
    return None


def _get_userprofile_via_cmd() -> Path:
    """Get Windows home via cmd.exe USERPROFILE variable."""
    try:
        result = subprocess.run(
            [get_command_path("cmd.exe"), "/c", "echo %USERPROFILE%"],
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        win_path = result.stdout.strip()
        if not win_path or win_path == "%USERPROFILE%":
            return None

        result = subprocess.run(
            [get_command_path("wslpath"), win_path],
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        wsl_path = Path(result.stdout.strip())
        if wsl_path.exists() and (wsl_path / ".claude" / "projects").exists():
            return wsl_path
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass
    return None


def _find_claude_user_in_drive(drive: Path) -> Path:
    """Find first user with Claude installed in a drive."""
    users_dir = drive / "Users"
    if not users_dir.exists():
        return None
    for user_dir in users_dir.iterdir():
        if user_dir.is_dir() and not user_dir.is_symlink():
            if (user_dir / ".claude" / "projects").exists():
                return user_dir
    return None


def _scan_drives_for_claude_user() -> Path:
    """Scan all drives for any user with Claude installed."""
    mnt = Path("/mnt")
    if not mnt.exists():
        return None
    for drive in sorted(mnt.iterdir()):
        if not (drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha()):
            continue
        user_home = _find_claude_user_in_drive(drive)
        if user_home:
            return user_home
    return None


def get_windows_home_from_wsl(username: str = None):
    """
    Get Windows user home directory from WSL.

    Args:
        username: Optional Windows username. If None, uses USERPROFILE.

    Returns:
        Path to Windows home directory, or None if not found.
    """
    if username:
        return _find_user_home_on_drives(username)

    # Primary approach: use Windows USERPROFILE
    home = _get_userprofile_via_cmd()
    if home:
        return home

    # Fallback: scan all drives
    return _scan_drives_for_claude_user()


def _is_valid_windows_drive(drive: Path) -> bool:
    """Check if path is a valid single-letter Windows drive mount."""
    return drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha()


def _scan_users_in_drive(drive: Path, results: list):
    """Scan a drive for Windows users with Claude installed."""
    users_dir = drive / "Users"
    if not users_dir.exists():
        return

    for user_dir in users_dir.iterdir():
        if not user_dir.is_dir() or user_dir.is_symlink():
            continue
        claude_dir = user_dir / ".claude" / "projects"
        if claude_dir.exists():
            workspace_count = len([d for d in claude_dir.iterdir() if d.is_dir()])
            results.append(
                {
                    "username": user_dir.name,
                    "drive": drive.name,
                    "path": user_dir,
                    "claude_dir": claude_dir,
                    "workspace_count": workspace_count,
                }
            )


def get_windows_users_with_claude():
    """Get list of all Windows users with Claude Code installed."""
    results = []
    mnt = Path("/mnt")

    if not mnt.exists():
        return results

    for drive in sorted(mnt.iterdir()):
        if _is_valid_windows_drive(drive):
            _scan_users_in_drive(drive, results)

    return results


def is_wsl_remote(remote_spec: str) -> bool:
    """Check if remote spec is a WSL distribution (wsl://DistroName)."""
    return remote_spec.startswith("wsl://")


def is_windows_remote(remote_spec: str) -> bool:
    """Check if remote spec is Windows from WSL (windows or windows://username)."""
    return remote_spec == "windows" or remote_spec.startswith("windows://")


def get_source_tag(remote_spec: str = None) -> str:
    """
    Generate source tag for filename/directory prefixes.

    Args:
        remote_spec: Remote specification (None for local, 'wsl://Ubuntu', 'windows', 'user@host')

    Returns:
        Source tag string:
        - '' for local (no tag)
        - 'wsl_{distro}_' for WSL
        - 'windows_' for Windows from WSL
        - 'remote_{hostname}_' for SSH remotes
    """
    if not remote_spec:
        # Local - no tag
        return ""

    if is_wsl_remote(remote_spec):
        # WSL: wsl_ubuntu_
        distro = remote_spec[6:]  # Remove 'wsl://' prefix
        return f"wsl_{distro.lower()}_"

    if is_windows_remote(remote_spec):
        # Windows from WSL: windows_ or windows_username_
        if remote_spec.startswith("windows://"):
            username = remote_spec[10:]  # Remove 'windows://' prefix
            return f"windows_{username.lower()}_"
        else:
            return "windows_"

    # SSH Remote: remote_hostname_
    # Extract hostname from user@hostname or hostname
    if "@" in remote_spec:
        hostname = remote_spec.split("@")[1]
    else:
        hostname = remote_spec

    # Take first part if FQDN (e.g., dev.example.com -> dev)
    hostname = hostname.split(".")[0].lower()

    return f"remote_{hostname}_"


def get_workspace_name_from_path(workspace_dir_name: str) -> str:
    """
    Extract clean workspace name from directory name.
    Removes source tags and normalizes path.

    Examples:
        'C--sankar-projects-claude-history' -> 'claude-history'
        'remote_ubuntuvm01_home-sankar-projects-claude-skills' -> 'claude-skills'
        'wsl_ubuntu_home-sankar-projects-auth' -> 'auth'
    """
    # Remove source tags if present
    if workspace_dir_name.startswith(CACHED_REMOTE_PREFIX):
        # remote_hostname_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= REMOTE_PARTS_WITH_PATH:
            workspace_dir_name = parts[2]
    elif workspace_dir_name.startswith(CACHED_WSL_PREFIX):
        # wsl_distro_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= REMOTE_PARTS_WITH_PATH:
            workspace_dir_name = parts[2]

    # Get the last path component (workspace name)
    # home-sankar-projects-claude-history -> claude-history
    # C--sankar-projects-claude-history -> claude-history
    path_parts = workspace_dir_name.split("-")

    # Find the workspace name (usually last component or last few)
    # Simple heuristic: take last part, or last 2 if hyphenated (e.g., claude-history)
    if len(path_parts) >= MIN_WINDOWS_PATH_LEN:
        # Check if last 2 parts form a common pattern
        last_two = "-".join(path_parts[-2:])
        # If the second-to-last part is short (likely part of name like "claude-history")
        if len(path_parts[-2]) <= MAX_SHORT_PART_LEN:
            return last_two

    return path_parts[-1] if path_parts else workspace_dir_name


def _get_wsl_distro_names() -> list:
    """Get list of WSL distribution names.

    Notes:
        On Windows, `wsl --list --quiet` typically returns UTF-16 with a BOM and
        may include stray null terminators. We decode with 'utf-16' (to consume
        the BOM correctly), fall back to UTF-8 if needed, and strip any BOM or
        nulls per line before returning clean distro names.
    """
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "--list", "--quiet"],
            check=False,
            capture_output=True,
            timeout=5,
        )
        if result.returncode != 0:
            return []

        raw = result.stdout
        # Prefer utf-16 (consumes BOM). Fallback to utf-8 if decode fails.
        try:
            text = raw.decode("utf-16")
        except UnicodeError:
            text = raw.decode("utf-8", errors="ignore")

        names = []
        for line in text.splitlines():
            # Clean BOM on first line, nulls, and any incidental markers
            cleaned = line.lstrip("\ufeff").replace("\x00", "").strip()
            # Newer WSL may show a leading '* ' for default in some modes; be tolerant
            if cleaned.startswith("* "):
                cleaned = cleaned[2:].strip()
            if cleaned:
                names.append(cleaned)
        return names
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def _get_wsl_distro_info(distro_name: str) -> dict:
    """Get info for a single WSL distribution. Returns None on failure."""
    try:
        user_result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "whoami"],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if user_result.returncode != 0:
            return None

        username = user_result.stdout.strip()
        claude_path = _locate_wsl_projects_dir(distro_name, username)
        has_claude = claude_path is not None

        return {
            "name": distro_name,
            "username": username,
            "has_claude": has_claude,
            "path": str(claude_path) if claude_path else None,
        }
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return None


def get_wsl_distributions() -> list:
    """Get list of available WSL distributions.

    Returns:
        List of dicts with distro info: {'name': str, 'username': str, 'has_claude': bool}

    Test override:
        If environment variables are set:
          - CLAUDE_WSL_TEST_DISTRO: name of a synthetic distro
          - CLAUDE_WSL_PROJECTS_DIR: path to a projects dir for that distro
        then return a single entry using those values. This enables real
        filesystem E2E tests without mocking wsl.exe.
    """
    # Test override
    test_distro = os.environ.get("CLAUDE_WSL_TEST_DISTRO")
    test_projects = os.environ.get("CLAUDE_WSL_PROJECTS_DIR")
    if test_distro and test_projects:
        p = Path(test_projects)
        if p.exists():
            return [{"name": test_distro, "username": "test", "has_claude": True, "path": str(p)}]

    if platform.system() != "Windows":
        return []

    distributions = []
    for distro_name in _get_wsl_distro_names():
        if not distro_name:
            continue
        info = _get_wsl_distro_info(distro_name)
        if info:
            distributions.append(info)
    return distributions


def _get_wsl_candidate_paths(distro_name: str, username: str) -> list:
    """Return candidate UNC paths for a WSL distro."""
    return [
        Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects"),
        Path(f"//wsl$/{distro_name}/home/{username}/.claude/projects"),
    ]


def _locate_wsl_projects_dir(distro_name: str, username: str):
    """Find the first accessible UNC path for a WSL distro."""
    for candidate in _get_wsl_candidate_paths(distro_name, username):
        try:
            if candidate.exists():
                return candidate
        except OSError:
            continue
    return None


def _get_wsl_home_path(distro_name: str) -> Optional[str]:
    """Get the $HOME path inside a specific WSL distro.

    Returns the Linux path (e.g., '/home/user'). Returns None on failure.
    """
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "sh", "-lc", 'printf %s "$HOME"'],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode != 0:
            return None
        home = result.stdout.strip()
        # basic sanity: must look like an absolute path
        return home if home.startswith("/") else None
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return None


def get_wsl_projects_dir(distro_name: str):
    """Get Claude projects directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .claude/projects in WSL, accessible from Windows.
        Returns None if the distribution is inaccessible or Claude is not installed.
    """
    # Test override
    override = os.environ.get("CLAUDE_WSL_PROJECTS_DIR")
    if override and Path(override).exists():
        return Path(override)

    # Get username from WSL
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "whoami"],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )

        if result.returncode != 0:
            return None

        username = result.stdout.strip()
        return _locate_wsl_projects_dir(distro_name, username)

    except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
        return None


def get_windows_projects_dir(username: str = None):
    """Get Claude projects directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .claude/projects in Windows, accessible from WSL.
        Returns None if not running in WSL, user not found, or Claude not installed.
    """
    # Test override
    override = os.environ.get("CLAUDE_WINDOWS_PROJECTS_DIR")
    if override and Path(override).exists():
        return Path(override)

    if not is_running_in_wsl():
        return None

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        return None

    projects_dir = windows_home / ".claude" / "projects"

    if not projects_dir.exists():
        return None

    return projects_dir


# ============================================================================
# Remote Fetching
# ============================================================================


def parse_remote_host(remote_spec: str) -> tuple:
    """Parse remote host specification.

    Args:
        remote_spec: Remote host in format 'user@hostname' or 'hostname'

    Returns:
        Tuple of (user, hostname, full_spec) or (None, hostname, hostname)
    """
    if "@" in remote_spec:
        user, hostname = remote_spec.split("@", 1)
        return (user, hostname, remote_spec)
    else:
        # Just hostname, SSH will use current user
        return (None, remote_spec, remote_spec)


def check_ssh_connection(remote_host: str) -> bool:
    """Check if passwordless SSH connection is possible.

    Args:
        remote_host: Full remote spec (user@hostname or hostname)

    Returns:
        True if connection successful, False otherwise
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return False

    try:
        # Try SSH with BatchMode (no password prompts) and short timeout
        result = subprocess.run(
            [
                get_command_path("ssh"),
                "-o",
                "BatchMode=yes",
                "-o",
                "ConnectTimeout=5",
                remote_host,
                "echo ok",
            ],
            check=False,
            capture_output=True,
            text=True,
            timeout=10,
        )
        return result.returncode == 0 and result.stdout.strip() == "ok"
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def get_remote_hostname(remote_host: str) -> str:
    """Extract hostname from remote spec for use in directory prefix.

    Args:
        remote_host: Remote spec (user@hostname or hostname)

    Returns:
        Hostname portion only
    """
    _, hostname, _ = parse_remote_host(remote_host)
    # Clean hostname for use in directory names (remove dots, etc.)
    return hostname.replace(".", "-")


def get_remote_workspaces_batch(remote_host: str) -> list:
    """Get all workspace info from remote in one batch operation.

    Creates and runs a script on remote to gather all info efficiently.

    Returns:
        List of dicts with workspace info including decoded paths
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # Create a shell script that will run on remote
    script = """#!/bin/bash
cd ~/.claude/projects/ 2>/dev/null || exit 1

for dir in -*/ ; do
    dir=${dir%/}  # Remove trailing slash
    [ -d "$dir" ] || continue

    # Decode the workspace name by testing paths
    encoded="${dir#-}"  # Remove leading dash

    # Try to find the actual path
    IFS='-' read -ra PARTS <<< "$encoded"
    best_path=""

    # Build path by testing combinations
    current=""
    for part in "${PARTS[@]}"; do
        if [ -z "$current" ]; then
            current="$part"
        else
            # Try with dash
            test_with_dash="$current-$part"
            # Try as separate segment
            test_separate="$current/$part"

            if [ -d "/$test_with_dash" ]; then
                current="$test_with_dash"
            elif [ -d "/$current/$part" ]; then
                current="$current/$part"
            else
                current="$test_separate"
            fi
        fi
    done

    # Get session count
    session_count=$(find "$dir" -maxdepth 1 -name "*.jsonl" 2>/dev/null | wc -l)

    # Output: encoded_name|decoded_path|session_count
    echo "$dir|/$current|$session_count"
done
"""

    try:
        # Run the script on remote via SSH
        # Ensure Unix line endings (important on Windows)
        # Replace both \r\n and standalone \r with \n
        script_unix = script.replace("\r\n", "\n").replace("\r", "\n")
        script_bytes = script_unix.encode("utf-8")

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, "bash -s"],
            check=False,
            input=script_bytes,
            capture_output=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        stdout_text = result.stdout.decode("utf-8", errors="replace")
        # Parse results
        workspaces = []
        for line in stdout_text.strip().split("\n"):
            if not line:
                continue
            parts = line.split("|")
            if len(parts) >= SSH_WORKSPACE_PARTS:
                workspaces.append(
                    {
                        "encoded": parts[0],
                        "decoded": parts[1],
                        "session_count": int(parts[2]) if parts[2].isdigit() else 0,
                    }
                )

        return workspaces

    except (subprocess.TimeoutExpired, subprocess.SubprocessError, OSError):
        # SSH command failed, timed out, or couldn't execute
        return []


def normalize_remote_workspace_name(remote_host: str, workspace_dir_name: str) -> str:
    """Convert remote workspace directory name to readable path by verifying via SSH.

    Args:
        remote_host: Remote host specification
        workspace_dir_name: Encoded workspace name

    Returns:
        Decoded path
    """
    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        # Fall back to simple replacement without remote verification
        encoded = (
            workspace_dir_name[1:] if workspace_dir_name.startswith("-") else workspace_dir_name
        )
        return "/" + encoded.replace("-", "/")
    if not validate_workspace_name(workspace_dir_name):
        return workspace_dir_name  # Return as-is if invalid

    # Remove leading dash
    if workspace_dir_name.startswith("-"):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # Generate possible decodings by trying different dash positions
    parts = encoded.split("-")

    # Generate all reasonable path combinations (limit to avoid exponential explosion)
    # We'll try keeping dashes together in common patterns
    candidates = []

    # Strategy: Build path greedily, trying longer segments first
    def generate_paths(parts, start_idx, current_path):
        if start_idx >= len(parts):
            candidates.append("/".join(current_path))
            return

        # Try combining 1 to 3 parts (limit to keep it reasonable)
        for length in range(1, min(4, len(parts) - start_idx + 1)):
            segment = "-".join(parts[start_idx : start_idx + length])
            generate_paths(parts, start_idx + length, [*current_path, segment])

    generate_paths(parts, 0, [])

    # Limit candidates to reasonable number
    candidates = candidates[:MAX_WORKSPACE_CANDIDATES]

    # Test all candidates in ONE SSH call
    test_commands = " || ".join([f'test -e "/{path}" && echo "/{path}"' for path in candidates])

    try:
        result = subprocess.run(
            [get_command_path("ssh"), remote_host, test_commands],
            check=False,
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.stdout.strip():
            # Return the first (longest) match
            return result.stdout.strip().split("\n")[0]
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass

    # Fallback to simple replacement
    return "/" + encoded.replace("-", "/")


def list_remote_workspaces(remote_host: str) -> list:
    """List workspace directories on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of workspace directory names (e.g., ['-home-user-project', ...])
        Excludes remote caches (remote_* and wsl_*) to prevent circular fetching
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    try:
        # List directories in remote ~/.claude/projects/ (simple and fast)
        result = subprocess.run(
            [get_command_path("ssh"), remote_host, 'ls -1 ~/.claude/projects/ | grep "^-"'],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        # Parse output - one directory name per line
        all_workspaces = [
            line.strip() for line in result.stdout.strip().split("\n") if line.strip()
        ]

        # Filter out remote caches to prevent circular fetching:
        # - remote_* = SSH remote caches
        # - wsl_* = WSL caches
        # These are already fetched data and shouldn't be re-fetched
        workspaces = [ws for ws in all_workspaces if is_native_workspace(ws)]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_remote_session_info(remote_host: str, remote_workspace: str) -> list:
    """Get session file information from remote workspace without downloading.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count
    """
    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []
    if not validate_workspace_name(remote_workspace):
        sys.stderr.write(f"Error: Invalid workspace name: {remote_workspace}\n")
        return []

    try:
        # Get file stats from remote using find and stat
        # Output format: filename|size_bytes|mtime_epoch|line_count
        # Use sanitize_for_shell to safely quote the workspace name
        safe_workspace = sanitize_for_shell(remote_workspace)
        cmd = f"""cd ~/.claude/projects/{safe_workspace} && \
                  for f in *.jsonl; do \
                      [ -f "$f" ] || continue; \
                      size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null); \
                      mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null); \
                      lines=$(wc -l < "$f"); \
                      echo "$f|$size|$mtime|$lines"; \
                  done"""

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split("\n"):
            if not line or "|" not in line:
                continue

            parts = line.split("|")
            if len(parts) != SSH_SESSION_PARTS:
                continue

            filename, size_bytes, mtime_epoch, line_count = parts

            try:
                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)

                sessions.append(
                    {
                        "filename": filename,
                        "size_kb": size_kb,
                        "modified": modified,
                        "message_count": message_count,
                    }
                )
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def _validate_fetch_inputs(remote_host: str, remote_workspace: str) -> dict:
    """Validate inputs for fetch_workspace_files. Returns error dict or None."""
    if not validate_remote_host(remote_host):
        return {
            "success": False,
            "files_copied": 0,
            "bytes": 0,
            "error": f"Invalid remote host: {remote_host}",
        }
    if not validate_workspace_name(remote_workspace):
        return {
            "success": False,
            "files_copied": 0,
            "bytes": 0,
            "error": f"Invalid workspace name: {remote_workspace}",
        }
    return None


def _convert_to_rsync_path(local_dir: Path) -> str:
    """Convert local path to rsync-compatible format."""
    import re

    local_path_posix = str(local_dir).replace("\\", "/")

    if ":" in local_path_posix and not local_path_posix.startswith("/"):
        drive_match = re.match(r"([A-Za-z]):(.*)", local_path_posix)
        if drive_match:
            drive_letter = drive_match.group(1).lower()
            path_part = drive_match.group(2)
            return f"/cygdrive/{drive_letter}{path_part}/"
    return local_path_posix + "/"


def _count_rsync_files(output: str) -> int:
    """Count files copied from rsync output."""
    lines = output.split("\n")
    return sum(
        1
        for line in lines
        if line.strip() and not line.startswith(("sending", "sent", "total", "building"))
    )


def fetch_workspace_files(
    remote_host: str, remote_workspace: str, local_projects_dir: Path, hostname: str
) -> dict:
    """Fetch all files from a remote workspace using rsync."""
    validation_error = _validate_fetch_inputs(remote_host, remote_workspace)
    if validation_error:
        return validation_error

    workspace_path = remote_workspace.lstrip("-")
    local_workspace = f"remote_{hostname}_{workspace_path}"
    local_dir = local_projects_dir / local_workspace

    if not is_safe_path(local_projects_dir, local_dir):
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "Path traversal detected"}

    local_dir.mkdir(parents=True, exist_ok=True)
    remote_path = f"{remote_host}:~/.claude/projects/{remote_workspace}/"

    try:
        local_path_str = _convert_to_rsync_path(local_dir)
        rsync_cmd = [
            get_command_path("rsync"),
            "-avh",
            "--include=*.jsonl",
            "--exclude=*",
            remote_path,
            local_path_str,
        ]

        result = subprocess.run(
            rsync_cmd, check=False, capture_output=True, text=True, timeout=SSH_TIMEOUT
        )

        if result.returncode != 0:
            return {"success": False, "files_copied": 0, "bytes": 0, "error": result.stderr}

        return {
            "success": True,
            "files_copied": _count_rsync_files(result.stdout),
            "local_dir": local_workspace,
            "output": result.stdout,
        }

    except subprocess.TimeoutExpired:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "Timeout"}
    except FileNotFoundError:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "rsync not found"}


# ============================================================================
# Alias Storage
# ============================================================================


def get_config_dir() -> Path:
    """Get the config storage directory (~/.claude-history/)."""
    return Path.home() / ".claude-history"


def get_aliases_dir() -> Path:
    """Get the aliases storage directory (~/.claude-history/)."""
    return get_config_dir()


def get_aliases_file() -> Path:
    """Get the aliases storage file path."""
    return get_aliases_dir() / "aliases.json"


def get_config_file() -> Path:
    """Get the config file path."""
    return get_config_dir() / "config.json"


def load_config() -> dict:
    """Load config from storage file. Returns empty structure if not found."""
    config_file = get_config_file()
    if not config_file.exists():
        return {"version": 1, "sources": []}

    try:
        with open(config_file, encoding="utf-8") as f:
            data = json.load(f)
            if "sources" not in data:
                data["sources"] = []
            if "version" not in data:
                data["version"] = 1
            return data
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Warning: Could not load config file: {e}\n")
        return {"version": 1, "sources": []}


def save_config(data: dict) -> bool:
    """Save config to storage file.

    Args:
        data: Config data dictionary with 'version' and settings

    Returns:
        True on success, False on failure (error printed to stderr)

    Side Effects:
        - Creates ~/.claude-history/ directory with mode 0o700 if missing
        - Writes to ~/.claude-history/config.json with mode 0o600
    """
    config_dir = get_config_dir()
    config_file = get_config_file()

    try:
        config_dir.mkdir(parents=True, exist_ok=True)
        # Set secure permissions on config directory (owner-only access)
        os.chmod(config_dir, 0o700)
        if "version" not in data:
            data["version"] = 1
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        # Set secure permissions on config file (owner read/write only)
        os.chmod(config_file, 0o600)
        return True
    except OSError as e:
        sys.stderr.write(f"Error saving config: {e}\n")
        return False


def get_saved_sources() -> list:
    """Get list of saved remote sources."""
    config = load_config()
    return config.get("sources", [])


def load_aliases() -> dict:
    """Load aliases from storage file. Returns empty structure if not found."""
    aliases_file = get_aliases_file()
    if not aliases_file.exists():
        return {"version": 1, "aliases": {}}

    try:
        with open(aliases_file, encoding="utf-8") as f:
            data = json.load(f)
            # Ensure structure is valid
            if "aliases" not in data:
                data["aliases"] = {}
            if "version" not in data:
                data["version"] = 1
            return _normalize_aliases(data)
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Warning: Could not load aliases file: {e}\n")
        return {"version": 1, "aliases": {}}


def save_aliases(data: dict) -> bool:
    """Save aliases to storage file.

    Args:
        data: Alias data dictionary with 'version' and 'aliases' keys

    Returns:
        True on success, False on failure (error printed to stderr)

    Side Effects:
        - Creates ~/.claude-history/ directory with mode 0o700 if missing
        - Writes to ~/.claude-history/aliases.json
    """
    aliases_dir = get_aliases_dir()
    aliases_file = get_aliases_file()

    try:
        # Create directory if needed
        aliases_dir.mkdir(parents=True, exist_ok=True)
        # Set secure permissions on aliases directory (owner-only access)
        os.chmod(aliases_dir, 0o700)

        # Ensure version is set
        if "version" not in data:
            data["version"] = 1

        with open(aliases_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        # Set secure permissions on aliases file (owner read/write only)
        os.chmod(aliases_file, 0o600)
        return True
    except OSError as e:
        sys.stderr.write(f"Error: Could not save aliases: {e}\n")
        return False


def _sanitize_alias_workspace_entry(workspace: str) -> str:
    """Normalize alias workspace entries, handling legacy absolute paths."""
    if not workspace:
        return workspace

    lowered = workspace.lower()

    if lowered.startswith("/mnt/"):
        return path_to_encoded_workspace(workspace)

    if workspace.startswith("/"):
        return path_to_encoded_workspace(workspace)

    if lowered.startswith("-mnt-") and len(workspace) > MNT_ENCODED_PREFIX_LEN:
        drive_letter = workspace[5]
        remainder = workspace[MNT_ENCODED_PREFIX_LEN:]
        if drive_letter.isalpha():
            return f"{drive_letter.upper()}--{remainder}"

    return workspace


def _normalize_aliases(data: dict) -> dict:
    """Normalize stored alias workspace entries for cross-platform compatibility."""
    aliases = data.get("aliases", {})
    changed = False

    for sources in aliases.values():
        for source_key, workspaces in list(sources.items()):
            normalized = []
            for workspace in workspaces:
                new_value = _sanitize_alias_workspace_entry(workspace)
                if new_value != workspace:
                    changed = True
                normalized.append(new_value)
            sources[source_key] = normalized

    if changed:
        save_aliases(data)
    return data


def _try_encode_workspace(candidate: str) -> str | None:
    """Attempt to encode a workspace path/name to standard encoded form."""
    try:
        lower = candidate.lower()
        if candidate.startswith("-") or _is_windows_encoded_path(candidate):
            return candidate
        if lower.startswith("/mnt/") or candidate.startswith("/"):
            return path_to_encoded_workspace(candidate)
        if len(candidate) > 1 and candidate[1] == ":":
            return _convert_windows_path_to_encoded(candidate)
    except Exception:
        pass
    return None


def _get_readable_from_encoded(encoded: str | None, fallback: str) -> str | None:
    """Convert encoded workspace to readable form, or use fallback."""
    if encoded:
        try:
            return normalize_workspace_name(encoded, verify_local=False)
        except Exception:
            return None
    return fallback


def _normalize_alias_workspace_input(workspace_input: str) -> tuple[str | None, str | None]:
    """Return (encoded, readable) forms for flexible alias operations."""
    if workspace_input is None:
        return None, None

    candidate = workspace_input.strip()
    if not candidate:
        return None, None

    # Support remote-style input user@host:/path
    if ":" in candidate and "@" in candidate.split(":", 1)[0]:
        candidate = candidate.split(":", 1)[1].strip()

    encoded = _try_encode_workspace(candidate)
    readable = _get_readable_from_encoded(encoded, candidate)

    return encoded, readable


def _match_alias_workspace(workspaces: list, workspace_input: str) -> str | None:
    """Find matching workspace entry from alias list."""
    encoded_input, readable_input = _normalize_alias_workspace_input(workspace_input)
    candidates = {workspace_input}
    if encoded_input:
        candidates.add(encoded_input)

    for entry in workspaces:
        if entry in candidates:
            return entry
        try:
            entry_readable = normalize_workspace_name(entry, verify_local=False)
        except Exception:
            entry_readable = None
        if readable_input and entry_readable == readable_input:
            return entry
    return None


def get_source_key(remote_host=None, wsl_distro=None, windows_user=None) -> str:
    """Get the source key for aliases based on current context."""
    if remote_host:
        if remote_host.startswith("wsl://"):
            return f"wsl:{remote_host[WSL_PREFIX_LEN:]}"
        elif remote_host.startswith("windows:"):
            return (
                f"windows:{remote_host[WINDOWS_PREFIX_LEN:]}"
                if len(remote_host) > WINDOWS_PREFIX_LEN
                else "windows"
            )
        elif remote_host == "windows":
            return "windows"
        else:
            # Preserve full remote spec (user@host) for SSH authentication
            return f"remote:{remote_host}"
    elif wsl_distro:
        return f"wsl:{wsl_distro}"
    elif windows_user is not None:
        return f"windows:{windows_user}" if windows_user else "windows"
    else:
        return "local"


def get_alias_session_count(alias_name: str, aliases_data: dict) -> int:
    """Count total sessions across all homes for an alias."""
    if alias_name not in aliases_data.get("aliases", {}):
        return 0

    alias_config = aliases_data["aliases"][alias_name]
    total = 0

    for _source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            try:
                # Try to get session count for this workspace
                sessions = get_workspace_sessions(workspace)
                total += len(sessions)
            except (OSError, PermissionError):
                pass  # Workspace may not exist or be accessible

    return total


def resolve_alias_workspaces(alias_name: str, source_filter: str = None) -> list:
    """
    Resolve an alias to a list of (source_key, workspace, sessions) tuples.
    If source_filter is provided, only include matching sources.
    """
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        return []

    alias_config = aliases_data["aliases"][alias_name]
    results = []

    for source_key, workspaces in alias_config.items():
        if source_filter and source_key != source_filter:
            continue

        for workspace in workspaces:
            results.append((source_key, workspace))

    return results


def get_alias_for_workspace(workspace: str, source_key: str = "local") -> str:
    """
    Find the alias that contains a given workspace, if any.

    Args:
        workspace: Encoded workspace name (e.g., '-home-user-project')
        source_key: Source identifier like 'local', 'windows', 'wsl:distro', 'remote:hostname'

    Returns:
        Alias name if found, None otherwise
    """
    aliases_data = load_aliases()

    for alias_name, alias_config in aliases_data.get("aliases", {}).items():
        # Check each source in the alias
        for src_key, workspaces in alias_config.items():
            # Handle source key matching (e.g., 'local' matches 'local', 'wsl:Ubuntu' matches 'wsl')
            if (
                src_key == source_key
                or source_key.startswith(src_key + ":")
                or src_key.startswith(source_key + ":")
            ):
                if workspace in workspaces:
                    return alias_name
            # Also check if workspace name (without source prefix) matches
            ws_name = get_workspace_name_from_path(workspace)
            for ws in workspaces:
                if get_workspace_name_from_path(ws) == ws_name:
                    return alias_name

    return None


def _get_remote_sessions(
    remote_spec: str, workspace: str, since_date, until_date, fetch_remote: bool
) -> list:
    """Get sessions from a remote source."""
    try:
        local_projects = get_claude_projects_dir()
    except (OSError, SystemExit):
        return []

    # Use hostname only for cache path (no @ symbol in directory names)
    hostname = get_remote_hostname(remote_spec)
    cached_workspace = f"remote_{hostname}_{workspace.lstrip('-')}"
    cached_path = local_projects / cached_workspace

    if cached_path.exists():
        return get_workspace_sessions(
            cached_workspace,
            quiet=True,
            since_date=since_date,
            until_date=until_date,
            include_cached=True,
        )

    if not fetch_remote:
        return []

    # Use full remote_spec for SSH operations (includes username)
    if not check_ssh_connection(remote_spec):
        sys.stderr.write(f"Cannot connect to remote '{remote_spec}' - skipping\n")
        return []

    sys.stderr.write(f"Fetching from {remote_spec}:{workspace}...\n")
    result = fetch_workspace_files(remote_spec, workspace, local_projects, hostname)

    if result.get("success"):
        return get_workspace_sessions(
            cached_workspace,
            quiet=True,
            since_date=since_date,
            until_date=until_date,
            include_cached=True,
        )
    return []


def get_sessions_for_source(
    source_key: str, workspace: str, since_date=None, until_date=None, fetch_remote: bool = False
) -> list:
    """Get sessions from a workspace on a specific source."""
    try:
        if source_key == "local":
            return get_workspace_sessions(
                workspace, quiet=True, since_date=since_date, until_date=until_date
            )

        if source_key == "windows" or source_key.startswith("windows:"):
            username = source_key.split(":", 1)[1] if ":" in source_key else None
            projects_dir = get_windows_projects_dir(username)
            if not projects_dir:
                return []
            return get_workspace_sessions(
                workspace,
                quiet=True,
                since_date=since_date,
                until_date=until_date,
                projects_dir=projects_dir,
            )

        if source_key.startswith("wsl:"):
            distro = source_key.split(":", 1)[1]
            wsl_projects_dir = get_wsl_projects_dir(distro)
            if wsl_projects_dir is None:
                return []
            return get_workspace_sessions(
                workspace,
                quiet=True,
                since_date=since_date,
                until_date=until_date,
                projects_dir=wsl_projects_dir,
            )

        if source_key.startswith("remote:"):
            remote_spec = source_key.split(":", 1)[1]
            return _get_remote_sessions(
                remote_spec, workspace, since_date, until_date, fetch_remote
            )

        return []

    except (OSError, PermissionError):
        return []


# ============================================================================
# Alias Commands
# ============================================================================


def cmd_alias_list(args):
    """List all aliases with their workspaces and session counts."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    if not aliases:
        print("No aliases defined.")
        print("\nCreate an alias with:")
        print("  agent-history alias create <name>")
        print("  agent-history alias add <name> <workspace>")
        return

    for alias_name, sources in sorted(aliases.items()):
        total_sessions = 0
        print(f"\n{alias_name}:")

        for source_key, workspaces in sorted(sources.items()):
            if workspaces:
                print(f"  {source_key}:")
                for workspace in workspaces:
                    # Try to get session count using source-aware function
                    sessions = get_sessions_for_source(source_key, workspace)
                    count = len(sessions)
                    total_sessions += count
                    ws_readable = normalize_workspace_name(workspace)
                    if count > 0 or source_key == "local":
                        print(f"    {ws_readable}\t{count} sessions")
                    else:
                        print(f"    {ws_readable}\t{count} sessions (not cached)")

        print(f"  Total: {total_sessions} sessions")


def cmd_alias_show(args):
    """Show details of a single alias."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    alias_name = args.name
    if alias_name not in aliases:
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    sources = aliases[alias_name]
    total_sessions = 0

    print(f"{alias_name}:")

    for source_key, workspaces in sorted(sources.items()):
        if workspaces:
            print(f"  {source_key}:")
            for workspace in workspaces:
                # Use source-aware function to get sessions
                sessions = get_sessions_for_source(source_key, workspace)
                count = len(sessions)
                total_sessions += count
                ws_readable = normalize_workspace_name(workspace)
                if count > 0 or source_key == "local":
                    print(f"    {ws_readable}\t{count} sessions")
                else:
                    print(f"    {ws_readable}\t{count} sessions (not cached)")

    print(f"  Total: {total_sessions} sessions")


def cmd_alias_create(args):
    """Create a new empty alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' already exists\n")
        sys.exit(1)

    aliases_data["aliases"][alias_name] = {}

    if save_aliases(aliases_data):
        print(f"Created alias '{alias_name}'")
    else:
        sys.exit(1)


def cmd_alias_delete(args):
    """Delete an alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    del aliases_data["aliases"][alias_name]

    if save_aliases(aliases_data):
        print(f"Deleted alias '{alias_name}'")
    else:
        sys.exit(1)


def _build_all_homes_sources(args) -> list:
    """Build list of all homes to check for alias add."""
    sources = [("local", None)]

    # WSL distributions (if on Windows) or Windows users (if on WSL/Linux)
    if sys.platform == "win32":
        try:
            wsl_distros = get_wsl_distributions()
            for distro in wsl_distros:
                # distro is a dict with 'name', 'username', 'has_claude' keys
                distro_name = distro["name"]
                sources.append((f"wsl:{distro_name}", distro_name))
        except (OSError, subprocess.SubprocessError):
            pass
    else:
        try:
            windows_users = get_windows_users_with_claude()
            for user_info in windows_users:
                username = user_info.get("username", "")
                sources.append((f"windows:{username}" if username else "windows", username))
        except (OSError, subprocess.SubprocessError):
            pass

    # SSH remotes - store full spec (user@host) for authentication
    remotes = getattr(args, "remotes", None) or []
    if getattr(args, "remote", None):
        remotes.append(args.remote)
    for remote in remotes:
        sources.append((f"remote:{remote}", remote))

    return sources


def _resolve_workspaces_for_source(
    raw_workspaces: list, source_key: str, source_param, use_picker: bool, args
) -> list:
    """Resolve workspaces to add for a given source."""
    if use_picker:
        return interactive_workspace_picker(source_key, args)

    workspaces_to_add = []
    for ws_input in raw_workspaces:

        class MockArgs:
            remote = source_param if source_key.startswith("remote:") else None

        resolved = resolve_workspace_input(ws_input, source_key, MockArgs())
        if resolved:
            workspaces_to_add.extend(resolved)
    return workspaces_to_add


def _add_workspaces_to_alias(
    aliases_data: dict, alias_name: str, source_key: str, workspaces: list
) -> int:
    """Add workspaces to an alias, avoiding duplicates. Returns count added."""
    if source_key not in aliases_data["aliases"][alias_name]:
        aliases_data["aliases"][alias_name][source_key] = []

    added = 0
    for workspace in workspaces:
        if workspace not in aliases_data["aliases"][alias_name][source_key]:
            aliases_data["aliases"][alias_name][source_key].append(workspace)
            added += 1
    return added


def _alias_add_all_homes(
    args, aliases_data: dict, alias_name: str, raw_workspaces: list, use_picker: bool
):
    """Handle --ah (all homes) mode for alias add."""
    sources_to_check = _build_all_homes_sources(args)

    total_added = 0
    for source_key, source_param in sources_to_check:
        workspaces_to_add = _resolve_workspaces_for_source(
            raw_workspaces, source_key, source_param, use_picker, args
        )
        if not workspaces_to_add:
            continue

        added = _add_workspaces_to_alias(aliases_data, alias_name, source_key, workspaces_to_add)
        if added > 0:
            print(f"[{source_key}] Added {added} workspace(s)")
            total_added += added

    if save_aliases(aliases_data):
        print(f"Added {total_added} workspace(s) to alias '{alias_name}' from all homes")
    else:
        sys.exit(1)


def _alias_add_single_source(
    args, aliases_data: dict, alias_name: str, raw_workspaces: list, use_picker: bool
):
    """Handle single source mode for alias add."""
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,
        windows_user="" if windows_flag else None,
    )

    # Resolve workspaces
    if use_picker:
        workspaces_to_add = interactive_workspace_picker(source_key, args)
        if not workspaces_to_add:
            print("No workspaces selected.")
            return
    else:
        workspaces_to_add = []
        for ws_input in raw_workspaces:
            resolved = resolve_workspace_input(ws_input, source_key, args)
            if resolved:
                workspaces_to_add.extend(resolved)
            else:
                sys.stderr.write(f"Warning: No workspace found matching '{ws_input}'\n")

        if not workspaces_to_add:
            sys.stderr.write("Error: No valid workspaces found.\n")
            sys.exit(1)

    added = _add_workspaces_to_alias(aliases_data, alias_name, source_key, workspaces_to_add)

    if save_aliases(aliases_data):
        print(f"Added {added} workspace(s) to alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def cmd_alias_add(args):
    """Add workspace(s) to an alias."""
    aliases_data = load_aliases()

    # Determine alias name (auto-detect from cwd if not provided)
    alias_name = args.name
    if not alias_name:
        alias_name = Path.cwd().name

    # Get workspace patterns
    raw_workspaces = args.workspaces if args.workspaces else []
    use_picker = getattr(args, "pick", False)

    if not raw_workspaces and not use_picker:
        sys.stderr.write("Error: No workspaces specified. Use --pick for interactive mode.\n")
        sys.exit(1)

    # Ensure alias exists
    if alias_name not in aliases_data.get("aliases", {}):
        aliases_data["aliases"][alias_name] = {}

    # Dispatch to appropriate handler
    if getattr(args, "all_homes", False):
        _alias_add_all_homes(args, aliases_data, alias_name, raw_workspaces, use_picker)
    else:
        _alias_add_single_source(args, aliases_data, alias_name, raw_workspaces, use_picker)


def _display_workspace_list(workspaces: list, source_key: str):
    """Display numbered list of workspaces."""
    print(f"\nAvailable workspaces [{source_key}]:")
    for i, ws in enumerate(sorted(workspaces), 1):
        ws_readable = normalize_workspace_name(ws)
        print(f"  {i}. {ws_readable}")


def _parse_workspace_selection(selection: str, sorted_workspaces: list) -> list:
    """Parse user selection into list of workspace names."""
    selected = []
    for part in selection.split():
        try:
            idx = int(part) - 1
            if 0 <= idx < len(sorted_workspaces):
                selected.append(sorted_workspaces[idx])
        except ValueError:
            continue
    return selected


def interactive_workspace_picker(source_key: str, args) -> list:
    """Interactive workspace picker. Returns list of selected workspace names."""
    try:
        workspaces = _get_workspaces_for_source(source_key, args)
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error listing workspaces: {e}\n")
        return []

    if not workspaces:
        print("No workspaces found.")
        return []

    _display_workspace_list(workspaces, source_key)

    print("\nEnter workspace numbers (space-separated), or 'q' to quit:")
    try:
        selection = input("> ").strip()
    except (EOFError, KeyboardInterrupt):
        return []

    if selection.lower() == "q":
        return []

    return _parse_workspace_selection(selection, sorted(workspaces))


def cmd_alias_remove(args):
    """Remove workspace from an alias."""
    aliases_data = load_aliases()

    alias_name = args.name
    workspace = args.workspace

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    # Determine source key
    # For alias remove, --wsl and --windows are boolean flags
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,  # Empty string means auto-detect
        windows_user="" if windows_flag else None,  # Empty string means auto-detect
    )

    alias_config = aliases_data["aliases"][alias_name]

    if source_key not in alias_config:
        sys.stderr.write(f"Error: No workspaces from '{source_key}' in alias '{alias_name}'\n")
        sys.exit(1)

    target = _match_alias_workspace(alias_config[source_key], workspace)
    if not target:
        sys.stderr.write(
            f"Error: Workspace '{workspace}' not in alias '{alias_name}' [{source_key}]\n"
        )
        sys.exit(1)

    alias_config[source_key].remove(target)

    # Clean up empty source
    if not alias_config[source_key]:
        del alias_config[source_key]

    if not alias_config:
        del aliases_data["aliases"][alias_name]

    if save_aliases(aliases_data):
        print(f"Removed workspace from alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def cmd_alias_config_export(args):
    """Export aliases configuration to JSON (stdout)."""
    aliases_data = load_aliases()
    print(json.dumps(aliases_data, indent=2))


def _collect_alias_sessions(alias_config: dict, since_date, until_date) -> list:
    """Collect sessions from all workspaces in an alias configuration."""
    all_sessions = []
    for source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            sessions = get_sessions_for_source(
                source_key, workspace, since_date=since_date, until_date=until_date
            )
            for session in sessions:
                session["source"] = source_key
                all_sessions.append(session)
    return all_sessions


def _format_date_range_message(since_date, until_date) -> str:
    """Format date range for error messages."""
    parts = []
    if since_date:
        parts.append(f"from {since_date.strftime('%Y-%m-%d')}")
    if until_date:
        parts.append(f"until {until_date.strftime('%Y-%m-%d')}")
    return " ".join(parts)


def cmd_alias_lss(alias_name: str, since_date=None, until_date=None, agent: str = "auto"):
    """List sessions across all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    alias_config = aliases_data["aliases"][alias_name]
    all_sessions = _collect_alias_sessions(alias_config, since_date, until_date)

    # Filter by agent type if specified
    if agent != "auto":
        all_sessions = [s for s in all_sessions if s.get("agent", AGENT_CLAUDE) == agent]

    if not all_sessions:
        if since_date or until_date:
            sys.stderr.write(
                f"No sessions found {_format_date_range_message(since_date, until_date)}\n"
            )
        else:
            agent_msg = f" for agent '{agent}'" if agent != "auto" else ""
            sys.stderr.write(f"Error: No sessions found in alias '{alias_name}'{agent_msg}\n")
        sys.exit(1)

    print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
    for session in all_sessions:
        session_agent = session.get("agent", AGENT_CLAUDE)
        print(
            f"{session_agent}\t{session['source']}\t{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
        )


def _get_alias_export_options(args) -> dict:
    """Extract export options from args."""
    return {
        "since_date": parse_date_string(getattr(args, "since", None)),
        "until_date": parse_date_string(getattr(args, "until", None)),
        "force": getattr(args, "force", False),
        "minimal": getattr(args, "minimal", False),
        "split_lines": getattr(args, "split", None),
        "flat": getattr(args, "flat", False),
        "agent": getattr(args, "agent", None) or "auto",
    }


def _get_ws_output_path(output_path: Path, workspace: str, flat: bool) -> Path:
    """Get output path for workspace, creating directory if needed."""
    if flat:
        return output_path
    ws_name = normalize_workspace_name(workspace).replace("/", "-").replace("\\", "-")
    if ws_name.startswith("-"):
        ws_name = ws_name[1:]
    ws_path = output_path / ws_name
    ws_path.mkdir(parents=True, exist_ok=True)
    return ws_path


def _build_output_filename(jsonl_file: Path, source_tag: str, messages: list) -> str:
    """Build output filename with optional timestamp prefix."""
    ts_prefix = None
    if messages and messages[0].get("timestamp"):
        try:
            dt = datetime.fromisoformat(messages[0]["timestamp"].replace("Z", "+00:00"))
            ts_prefix = dt.strftime("%Y%m%d%H%M%S")
        except (ValueError, AttributeError):
            pass
    return (
        f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
        if ts_prefix
        else f"{source_tag}{jsonl_file.stem}.md"
    )


def _write_export_parts(
    messages: list, jsonl_file: Path, ws_output_path: Path, output_name: str, opts: dict
) -> bool:
    """Write split parts if applicable. Returns True if parts were written."""
    if not opts["split_lines"] or not messages or len(messages) <= MIN_MESSAGES_FOR_SPLIT:
        return False

    parts = generate_markdown_parts(messages, jsonl_file, opts["minimal"], opts["split_lines"])
    if not parts:
        return False

    base_name = output_name[:-3]
    for part_num, _, part_md, _, _ in parts:
        part_file = ws_output_path / f"{base_name}_part{part_num}.md"
        part_file.write_text(part_md, encoding="utf-8")
        print(part_file)
    return True


def _export_session_file(
    session: dict, ws_output_path: Path, source_tag: str, opts: dict, stats: dict
):
    """Export a single session file. Updates stats dict in place."""
    jsonl_file = session["file"]

    try:
        messages = read_jsonl_messages(jsonl_file)
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error reading {jsonl_file}: {e}\n")
        stats["failed"] += 1
        return

    output_name = _build_output_filename(jsonl_file, source_tag, messages)
    output_file = ws_output_path / output_name

    # Check incremental skip
    if not opts["force"] and output_file.exists():
        if output_file.stat().st_mtime >= jsonl_file.stat().st_mtime:
            stats["skipped"] += 1
            return

    try:
        if _write_export_parts(messages, jsonl_file, ws_output_path, output_name, opts):
            stats["exported"] += 1
            return

        markdown = parse_jsonl_to_markdown(jsonl_file, minimal=opts["minimal"], messages=messages)
        output_file.write_text(markdown, encoding="utf-8")
        print(output_file)
        stats["exported"] += 1
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error exporting {jsonl_file}: {e}\n")
        stats["failed"] += 1


def cmd_alias_export(alias_name: str, output_dir: str, args):
    """Export sessions from all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        exit_with_error(f"Alias '{alias_name}' not found")

    alias_config = aliases_data["aliases"][alias_name]
    opts = _get_alias_export_options(args)
    agent_filter = opts.get("agent", "auto")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    stats = {"exported": 0, "skipped": 0, "failed": 0}

    for source_key, workspaces in alias_config.items():
        source_tag = "" if source_key == "local" else source_key.replace(":", "_") + "_"

        for workspace in workspaces:
            sessions = get_sessions_for_source(
                source_key,
                workspace,
                since_date=opts["since_date"],
                until_date=opts["until_date"],
                fetch_remote=True,
            )
            if not sessions:
                continue

            # Filter by agent type if specified
            if agent_filter != "auto":
                sessions = [s for s in sessions if s.get("agent", AGENT_CLAUDE) == agent_filter]
                if not sessions:
                    continue

            ws_output_path = _get_ws_output_path(output_path, workspace, opts["flat"])
            for session in sessions:
                _export_session_file(session, ws_output_path, source_tag, opts, stats)

    if stats["exported"] > 0 or stats["skipped"] > 0:
        print(
            f"\nAlias '{alias_name}': {stats['exported']} exported, "
            f"{stats['skipped']} skipped, {stats['failed']} failed"
        )


def _load_import_data(import_file: str) -> dict:
    """Load import data from file or stdin."""
    try:
        if import_file:
            with open(import_file, encoding="utf-8") as f:
                return json.load(f)
        return json.load(sys.stdin)
    except json.JSONDecodeError as e:
        sys.stderr.write(f"Error: Invalid JSON: {e}\n")
        sys.exit(1)
    except OSError as e:
        sys.stderr.write(f"Error reading file: {e}\n")
        sys.exit(1)


def _merge_alias_sources(existing_sources: dict, new_sources: dict):
    """Merge new sources into existing alias sources."""
    for source_key, workspaces in new_sources.items():
        if source_key not in existing_sources:
            existing_sources[source_key] = workspaces
        else:
            for ws in workspaces:
                if ws not in existing_sources[source_key]:
                    existing_sources[source_key].append(ws)


def cmd_alias_config_import(args):
    """Import aliases from JSON file or stdin."""
    import_file = getattr(args, "file", None)
    replace_mode = getattr(args, "replace", False)

    import_data = _load_import_data(import_file)

    if "aliases" not in import_data:
        sys.stderr.write("Error: Invalid aliases format (missing 'aliases' key)\n")
        sys.exit(1)

    if replace_mode:
        new_data = import_data
    else:
        existing_data = load_aliases()
        for alias_name, sources in import_data["aliases"].items():
            if alias_name not in existing_data["aliases"]:
                existing_data["aliases"][alias_name] = sources
            else:
                _merge_alias_sources(existing_data["aliases"][alias_name], sources)
        new_data = existing_data

    if save_aliases(new_data):
        alias_count = len(new_data.get("aliases", {}))
        mode_str = "Replaced" if replace_mode else "Merged"
        print(f"{mode_str} aliases ({alias_count} aliases)")
    else:
        sys.exit(1)


# ============================================================================
# Metrics Database (SQLite)
# ============================================================================

METRICS_DB_VERSION = 4


def get_metrics_db_path() -> Path:
    """Get the metrics database file path."""
    return get_aliases_dir() / "metrics.db"


def init_metrics_db(db_path: Path = None) -> sqlite3.Connection:
    """Initialize the metrics database, creating tables if needed.

    Opens (or creates) the SQLite metrics database and ensures the schema
    is up to date. Handles migrations from older schema versions.

    Args:
        db_path: Path to database file. Defaults to ~/.claude-history/metrics.db

    Returns:
        Open sqlite3.Connection with row_factory set to sqlite3.Row

    Side Effects:
        - Creates parent directory (~/.claude-history/) with mode 0o700 if missing
        - Creates database file with mode 0o600 if missing
        - Runs schema migrations if database version is outdated
    """
    if db_path is None:
        db_path = get_metrics_db_path()

    # Ensure directory exists with secure permissions
    db_path.parent.mkdir(parents=True, exist_ok=True)
    os.chmod(db_path.parent, 0o700)

    # Track if this is a new database
    is_new_db = not db_path.exists()

    conn = sqlite3.connect(str(db_path))

    # Set secure permissions on new database file
    if is_new_db:
        os.chmod(db_path, 0o600)
    conn.row_factory = sqlite3.Row  # Enable column access by name

    # Create tables
    # Note: file_path is the primary key since multiple files can share the same session_id
    # (e.g., main session + agent files spawned from it)
    conn.executescript("""
        -- Schema version tracking
        CREATE TABLE IF NOT EXISTS schema_version (
            version INTEGER PRIMARY KEY
        );

        -- Sessions table (one row per JSONL file)
        CREATE TABLE IF NOT EXISTS sessions (
            file_path TEXT PRIMARY KEY,
            session_id TEXT,
            workspace TEXT NOT NULL,
            source TEXT NOT NULL DEFAULT 'local',
            agent TEXT NOT NULL DEFAULT 'claude',
            file_mtime REAL,
            is_agent INTEGER DEFAULT 0,
            parent_session_id TEXT,
            start_time TEXT,
            end_time TEXT,
            message_count INTEGER DEFAULT 0,
            git_branch TEXT,
            claude_version TEXT,
            cwd TEXT,
            work_period_seconds REAL DEFAULT 0,
            num_work_periods INTEGER DEFAULT 1
        );

        -- Messages table (aggregated stats per message)
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            parent_uuid TEXT,
            type TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            model TEXT,
            stop_reason TEXT,
            input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0,
            cache_creation_tokens INTEGER DEFAULT 0,
            cache_read_tokens INTEGER DEFAULT 0,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Tool uses table
        CREATE TABLE IF NOT EXISTS tool_uses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_use_id TEXT,
            message_uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            tool_name TEXT NOT NULL,
            is_error INTEGER DEFAULT 0,
            timestamp TEXT,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Synced files tracking (for incremental sync)
        CREATE TABLE IF NOT EXISTS synced_files (
            file_path TEXT PRIMARY KEY,
            mtime REAL NOT NULL,
            synced_at TEXT NOT NULL
        );

        -- Create indexes for common queries
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace ON sessions(workspace);
        CREATE INDEX IF NOT EXISTS idx_sessions_source ON sessions(source);
        CREATE INDEX IF NOT EXISTS idx_sessions_agent ON sessions(agent);
        CREATE INDEX IF NOT EXISTS idx_sessions_start_time ON sessions(start_time);
        CREATE INDEX IF NOT EXISTS idx_sessions_session_id ON sessions(session_id);
        CREATE INDEX IF NOT EXISTS idx_messages_file_path ON messages(file_path);
        CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);
        CREATE INDEX IF NOT EXISTS idx_messages_model ON messages(model);
        CREATE INDEX IF NOT EXISTS idx_messages_session ON messages(session_id);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_file_path ON tool_uses(file_path);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_tool_name ON tool_uses(tool_name);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_session ON tool_uses(session_id);

        -- Composite indexes for common filter combinations
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace_source ON sessions(workspace, source);
        CREATE INDEX IF NOT EXISTS idx_sessions_source_time ON sessions(source, start_time);
    """)

    # Check/set schema version and handle migrations
    cursor = conn.execute("SELECT version FROM schema_version LIMIT 1")
    row = cursor.fetchone()
    current_version = row["version"] if row else 0

    if current_version < METRICS_DB_VERSION:
        # Migrate from version 2 to 3: add time tracking columns
        if current_version < DB_VERSION_TIME_TRACKING:
            try:
                conn.execute("ALTER TABLE sessions ADD COLUMN work_period_seconds REAL DEFAULT 0")
                conn.execute("ALTER TABLE sessions ADD COLUMN num_work_periods INTEGER DEFAULT 1")
            except sqlite3.OperationalError:
                pass  # Columns already exist

        # Migrate from version 3 to 4: add agent column
        if current_version < DB_VERSION_AGENT_COLUMN:
            try:
                conn.execute("ALTER TABLE sessions ADD COLUMN agent TEXT DEFAULT 'claude'")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_agent ON sessions(agent)")
            except sqlite3.OperationalError as e:
                # Only ignore "duplicate column" error, re-raise others
                if "duplicate column" not in str(e).lower():
                    sys.stderr.write(f"Warning: Migration to v4 failed: {e}\n")

        # Update version
        if row is None:
            conn.execute("INSERT INTO schema_version (version) VALUES (?)", (METRICS_DB_VERSION,))
        else:
            conn.execute("UPDATE schema_version SET version = ?", (METRICS_DB_VERSION,))

    conn.commit()
    return conn


def calculate_work_periods(
    timestamps: list, gap_threshold: float = WORK_PERIOD_GAP_THRESHOLD
) -> tuple:
    """Calculate work period time from a list of timestamps.

    Args:
        timestamps: List of ISO 8601 timestamp strings
        gap_threshold: Gap in seconds that marks end of a work period

    Returns:
        (work_period_seconds, num_work_periods, start_time, end_time)

    Note: Timestamps are sorted first since JSONL files may have
    out-of-order timestamps due to context restoration.
    """
    if not timestamps:
        return 0.0, 0, None, None

    if len(timestamps) == 1:
        return 0.0, 1, timestamps[0], timestamps[0]

    # Sort timestamps chronologically
    sorted_ts = sorted(timestamps)

    # Parse timestamps
    def parse_ts(ts):
        return datetime.fromisoformat(ts.replace("Z", "+00:00"))

    total_duration = 0.0
    num_periods = 1
    period_start = parse_ts(sorted_ts[0])
    prev_ts = period_start

    for ts_str in sorted_ts[1:]:
        ts = parse_ts(ts_str)
        gap = (ts - prev_ts).total_seconds()

        if gap > gap_threshold:
            # End current period, start new one
            total_duration += (prev_ts - period_start).total_seconds()
            period_start = ts
            num_periods += 1

        prev_ts = ts

    # Add final period
    total_duration += (prev_ts - period_start).total_seconds()

    return total_duration, num_periods, sorted_ts[0], sorted_ts[-1]


def _init_session_info(jsonl_file: Path, source: str) -> dict:
    """Initialize session info dictionary."""
    return {
        "session_id": None,
        "workspace": None,
        "source": source,
        "file_path": str(jsonl_file),
        "file_mtime": jsonl_file.stat().st_mtime if jsonl_file.exists() else None,
        "is_agent": False,
        "parent_session_id": None,
        "start_time": None,
        "end_time": None,
        "message_count": 0,
        "git_branch": None,
        "claude_version": None,
        "cwd": None,
        "work_period_seconds": 0.0,
        "num_work_periods": 0,
    }


def _update_session_from_entry(session_info: dict, entry: dict):
    """Update session info from first message entry."""
    if session_info["session_id"] is not None:
        return
    session_info["session_id"] = entry.get("sessionId")
    session_info["git_branch"] = entry.get("gitBranch")
    session_info["claude_version"] = entry.get("version")
    session_info["cwd"] = entry.get("cwd")
    session_info["is_agent"] = entry.get("isSidechain", False)
    if session_info["is_agent"]:
        session_info["parent_session_id"] = entry.get("sessionId")


def _extract_message_record(entry: dict, entry_type: str, timestamp: str) -> dict:
    """Extract message record from entry."""
    message_obj = entry.get("message", {})
    usage = message_obj.get("usage", {})
    return {
        "uuid": entry.get("uuid"),
        "session_id": entry.get("sessionId"),
        "parent_uuid": entry.get("parentUuid"),
        "type": entry_type,
        "timestamp": timestamp,
        "model": message_obj.get("model"),
        "stop_reason": message_obj.get("stop_reason"),
        "input_tokens": usage.get("input_tokens", 0),
        "output_tokens": usage.get("output_tokens", 0),
        "cache_creation_tokens": usage.get("cache_creation_input_tokens", 0),
        "cache_read_tokens": usage.get("cache_read_input_tokens", 0),
    }


def _extract_tool_uses_from_content(entry: dict, timestamp: str) -> list:
    """Extract tool uses from assistant message content."""
    message_obj = entry.get("message", {})
    content = message_obj.get("content", [])
    if not isinstance(content, list):
        return []
    tool_uses = []
    for block in content:
        if isinstance(block, dict) and block.get("type") == "tool_use":
            tool_uses.append(
                {
                    "tool_use_id": block.get("id"),
                    "message_uuid": entry.get("uuid"),
                    "session_id": entry.get("sessionId"),
                    "tool_name": block.get("name", "unknown"),
                    "is_error": False,
                    "timestamp": timestamp,
                }
            )
    return tool_uses


def _update_tool_errors_from_results(entry: dict, tool_uses: list):
    """Update tool use errors from tool result blocks."""
    message_obj = entry.get("message", {})
    content = message_obj.get("content", [])
    if not isinstance(content, list):
        return
    for block in content:
        if isinstance(block, dict) and block.get("type") == "tool_result":
            tool_use_id = block.get("tool_use_id")
            is_error = block.get("is_error", False)
            for tu in tool_uses:
                if tu["tool_use_id"] == tool_use_id:
                    tu["is_error"] = is_error
                    break


def extract_metrics_from_jsonl(jsonl_file: Path, source: str = "local") -> dict:
    """Extract metrics data from a JSONL file.

    Returns a dict with session, messages, and tool_uses.
    """
    session_info = _init_session_info(jsonl_file, source)
    messages = []
    tool_uses = []
    all_timestamps = []

    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for raw_line in f:
                line = raw_line.strip()
                if not line:
                    continue

                try:
                    entry = json.loads(line)
                    entry_type = entry.get("type")
                    if entry_type not in ("user", "assistant"):
                        continue

                    timestamp = entry.get("timestamp", "")
                    _update_session_from_entry(session_info, entry)

                    if timestamp:
                        all_timestamps.append(timestamp)
                    session_info["message_count"] += 1

                    messages.append(_extract_message_record(entry, entry_type, timestamp))

                    if entry_type == "assistant":
                        tool_uses.extend(_extract_tool_uses_from_content(entry, timestamp))
                    elif entry_type == "user":
                        _update_tool_errors_from_results(entry, tool_uses)

                except json.JSONDecodeError:
                    continue
    except OSError as e:
        sys.stderr.write(f"Warning: Error reading {jsonl_file}: {e}\n")

    work_secs, num_periods, start_time, end_time = calculate_work_periods(all_timestamps)
    session_info["work_period_seconds"] = work_secs
    session_info["num_work_periods"] = num_periods
    session_info["start_time"] = start_time
    session_info["end_time"] = end_time
    session_info["workspace"] = get_workspace_name_from_path(jsonl_file.parent.name)

    return {"session": session_info, "messages": messages, "tool_uses": tool_uses}


def sync_file_to_db(  # noqa: C901, PLR0912, PLR0915
    conn: sqlite3.Connection, jsonl_file: Path, source: str = "local", force: bool = False
) -> bool:
    """Sync a single JSONL file to the database.

    Returns True if file was synced, False if skipped (already up to date).
    """
    file_path = str(jsonl_file)
    current_mtime = jsonl_file.stat().st_mtime if jsonl_file.exists() else 0

    # Check if already synced and up to date
    if not force:
        cursor = conn.execute("SELECT mtime FROM synced_files WHERE file_path = ?", (file_path,))
        row = cursor.fetchone()
        if row and row["mtime"] >= current_mtime:
            return False  # Already up to date

    # Detect agent type from file path
    agent = detect_agent_from_path(jsonl_file)

    # Extract metrics based on agent type
    if agent == AGENT_GEMINI:
        metrics = gemini_extract_metrics_from_json(jsonl_file)
        session = metrics["session"]
        # Gemini uses projectHash - try to get readable path from hash index
        project_hash = session.get("projectHash", "")
        if project_hash:
            real_path = gemini_get_path_for_hash(project_hash)
            if real_path:
                # Use encoded path as workspace (consistent with Claude/Codex)
                session["workspace"] = path_to_encoded_workspace(real_path)
            else:
                # Fallback to hash-based workspace
                session["workspace"] = project_hash
        else:
            session["workspace"] = "gemini-unknown"
        session["source"] = source
        session["file_mtime"] = current_mtime
        session["is_agent"] = False
        session["parent_session_id"] = None
        session["message_count"] = len(metrics["messages"])
        session["git_branch"] = None
        session["claude_version"] = None  # Gemini doesn't have version info in sessions
        session["work_period_seconds"] = 0
        session["num_work_periods"] = 1
        # Get timestamps from session metadata
        session["start_time"] = session.get("startTime")
        session["end_time"] = session.get("lastUpdated")
    elif agent == AGENT_CODEX:
        metrics = codex_extract_metrics_from_jsonl(jsonl_file)
        session = metrics["session"]
        # Codex uses cwd directly, need to convert to workspace
        if session.get("cwd"):
            session["workspace"] = path_to_encoded_workspace(session["cwd"])
        else:
            session["workspace"] = "unknown"
        session["source"] = source
        session["file_mtime"] = current_mtime
        session["is_agent"] = False
        session["parent_session_id"] = None
        session["start_time"] = None
        session["end_time"] = None
        session["message_count"] = len(metrics["messages"])
        session["git_branch"] = None
        session["claude_version"] = session.get("cli_version")
        session["work_period_seconds"] = 0
        session["num_work_periods"] = 1
        # Get timestamps from messages
        if metrics["messages"]:
            timestamps = [m.get("timestamp") for m in metrics["messages"] if m.get("timestamp")]
            if timestamps:
                session["start_time"] = min(timestamps)
                session["end_time"] = max(timestamps)
    else:
        metrics = extract_metrics_from_jsonl(jsonl_file, source)
        session = metrics["session"]

    if session.get("message_count", 0) == 0 and len(metrics.get("messages", [])) == 0:
        return False  # No messages in file

    # Delete existing data for this file (for re-sync)
    conn.execute("DELETE FROM tool_uses WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM messages WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM sessions WHERE file_path = ?", (file_path,))

    # Insert session
    conn.execute(
        """
        INSERT INTO sessions (
            file_path, session_id, workspace, source, agent, file_mtime,
            is_agent, parent_session_id, start_time, end_time,
            message_count, git_branch, claude_version, cwd,
            work_period_seconds, num_work_periods
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """,
        (
            file_path,
            session.get("session_id") or session.get("id"),
            session["workspace"],
            session["source"],
            agent,
            session["file_mtime"],
            1 if session.get("is_agent") else 0,
            session.get("parent_session_id"),
            session.get("start_time"),
            session.get("end_time"),
            session.get("message_count", len(metrics.get("messages", []))),
            session.get("git_branch"),
            session.get("claude_version"),
            session.get("cwd"),
            session.get("work_period_seconds", 0),
            session.get("num_work_periods", 1),
        ),
    )

    # Insert messages
    for msg in metrics["messages"]:
        conn.execute(
            """
            INSERT INTO messages (
                uuid, file_path, session_id, parent_uuid, type, timestamp,
                model, stop_reason, input_tokens, output_tokens,
                cache_creation_tokens, cache_read_tokens
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                msg.get("uuid"),
                file_path,
                msg.get("session_id") or session.get("session_id") or session.get("id"),
                msg.get("parent_uuid"),
                msg.get("type") or msg.get("role"),
                msg.get("timestamp"),
                msg.get("model"),
                msg.get("stop_reason"),
                msg.get("input_tokens", 0),
                msg.get("output_tokens", 0),
                msg.get("cache_creation_tokens", 0),
                msg.get("cache_read_tokens", 0),
            ),
        )

    # Insert tool uses
    for tu in metrics["tool_uses"]:
        conn.execute(
            """
            INSERT INTO tool_uses (
                tool_use_id, message_uuid, file_path, session_id, tool_name, is_error, timestamp
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                tu.get("tool_use_id"),
                tu.get("message_uuid"),
                file_path,
                tu.get("session_id") or session.get("session_id") or session.get("id"),
                tu.get("tool_name") or tu.get("name"),
                1 if tu.get("is_error") else 0,
                tu.get("timestamp"),
            ),
        )

    # Update synced files tracking
    conn.execute(
        """
        INSERT OR REPLACE INTO synced_files (file_path, mtime, synced_at)
        VALUES (?, ?, ?)
    """,
        (file_path, current_mtime, datetime.now().isoformat()),
    )

    conn.commit()
    return True


def _matches_pattern(name: str, patterns: list) -> bool:
    """Check if name matches any of the patterns."""
    if not patterns or not patterns[0]:
        return True
    return any(p in name for p in patterns)


def _sync_source_to_db(
    conn, projects_dir: Path, source: str, source_label: str, patterns: list, force: bool
) -> dict:
    """Sync a single source to the metrics database. Returns stats dict."""
    stats = {"synced": 0, "skipped": 0, "errors": 0}

    if not projects_dir or not projects_dir.exists():
        return stats

    print(f"Scanning {source_label}...")

    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir() or is_cached_workspace(workspace_dir.name):
            continue
        if not _matches_pattern(workspace_dir.name, patterns):
            continue

        _sync_workspace_files_to_db(conn, workspace_dir, source, stats, force)

    return stats


def _sync_workspace_files_to_db(conn, local_dir: Path, source_key: str, stats: dict, force: bool):
    """Sync all JSONL files in a workspace directory to database."""
    if not local_dir.exists():
        return
    for jsonl_file in local_dir.glob("*.jsonl"):
        try:
            if sync_file_to_db(conn, jsonl_file, source_key, force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {jsonl_file.name}: {e}\n")
            stats["errors"] += 1


def _sync_codex_to_db(conn, patterns: list, force: bool) -> dict:
    """Sync Codex sessions from ~/.codex/sessions/ to database.

    Codex stores sessions in ~/.codex/sessions/YYYY/MM/DD/rollout-*.jsonl
    """
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    sessions_dir = codex_get_home_dir()

    if not sessions_dir.exists():
        return stats

    print("Scanning Codex sessions...")

    # Walk through YYYY/MM/DD structure
    for jsonl_file in sessions_dir.glob("*/*/*/rollout-*.jsonl"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = codex_get_workspace_from_session(jsonl_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, jsonl_file, "codex", force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {jsonl_file.name}: {e}\n")
            stats["errors"] += 1

    return stats


def _sync_gemini_to_db(conn, patterns: list, force: bool) -> dict:
    """Sync Gemini sessions from ~/.gemini/tmp/*/chats/ to database.

    Gemini stores sessions in ~/.gemini/tmp/<project_hash>/chats/session-*.json
    """
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    sessions_dir = gemini_get_home_dir()

    if not sessions_dir.exists():
        return stats

    print("Scanning Gemini sessions...")

    # Walk through <hash>/chats/ structure
    for json_file in sessions_dir.glob("*/chats/session-*.json"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = gemini_get_workspace_from_session(json_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, json_file, "gemini", force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {json_file.name}: {e}\n")
            stats["errors"] += 1

    return stats


def _filter_workspaces_by_pattern(workspaces: list, patterns: list) -> list:
    """Filter workspaces by patterns (substring match)."""
    if not patterns or not patterns[0]:
        return workspaces
    return [ws for ws in workspaces if any(p in ws for p in patterns)]


def _sync_remote_workspace(
    conn, remote: str, ws: str, local_projects_dir: Path, hostname: str, stats: dict, force: bool
):
    """Sync a single remote workspace to database."""
    fetch_result = fetch_workspace_files(remote, ws, local_projects_dir, hostname)
    if not fetch_result.get("success"):
        return
    workspace_path = ws.lstrip("-")
    local_dir = local_projects_dir / f"remote_{hostname}_{workspace_path}"
    _sync_workspace_files_to_db(conn, local_dir, f"remote:{hostname}", stats, force)


def _sync_ssh_remote_to_db(conn, remote: str, patterns: list, force: bool) -> dict:
    """Sync SSH remote to the metrics database. Returns stats dict."""
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    hostname = get_remote_hostname(remote)
    print(f"Fetching from remote {remote}...")

    if not check_ssh_connection(remote):
        sys.stderr.write(f"  Cannot connect to {remote}\n")
        stats["errors"] += 1
        return stats

    remote_workspaces = list_remote_workspaces(remote)
    if not remote_workspaces:
        print(f"  No workspaces found on {remote}")
        return stats

    remote_workspaces = _filter_workspaces_by_pattern(remote_workspaces, patterns)
    local_projects_dir = get_claude_projects_dir()

    for ws in remote_workspaces:
        try:
            _sync_remote_workspace(conn, remote, ws, local_projects_dir, hostname, stats, force)
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"  Error fetching {ws}: {e}\n")
            stats["errors"] += 1

    return stats


def _accumulate_stats(totals: dict, new_stats: dict):
    """Accumulate sync stats into totals."""
    totals["synced"] += new_stats["synced"]
    totals["skipped"] += new_stats["skipped"]
    totals["errors"] += new_stats["errors"]


def _sync_wsl_windows_to_db(conn, totals: dict, patterns: list, force: bool):
    """Sync WSL and Windows sources to database."""
    for distro in get_wsl_distributions():
        if distro.get("has_claude"):
            wsl_projects = get_wsl_projects_dir(distro["name"])
            if wsl_projects:
                stats = _sync_source_to_db(
                    conn,
                    wsl_projects,
                    f"wsl:{distro['name']}",
                    f"WSL ({distro['name']})",
                    patterns,
                    force,
                )
                _accumulate_stats(totals, stats)

    try:
        win_projects = get_windows_projects_dir()
        if win_projects:
            stats = _sync_source_to_db(conn, win_projects, "windows", "Windows", patterns, force)
            _accumulate_stats(totals, stats)
    except (OSError, PermissionError):
        pass


def _sync_remote_to_db(conn, remote: str, totals: dict, patterns: list, force: bool):
    """Sync a single remote (WSL, Windows, or SSH) to database."""
    if remote.startswith("wsl://"):
        distro = remote[6:]
        wsl_projects = get_wsl_projects_dir(distro)
        if wsl_projects:
            stats = _sync_source_to_db(
                conn, wsl_projects, f"wsl:{distro}", f"WSL ({distro})", patterns, force
            )
            _accumulate_stats(totals, stats)
    elif remote == "windows" or remote.startswith("windows:"):
        win_projects = get_windows_projects_dir()
        if win_projects:
            stats = _sync_source_to_db(conn, win_projects, "windows", "Windows", patterns, force)
            _accumulate_stats(totals, stats)
    else:
        stats = _sync_ssh_remote_to_db(conn, remote, patterns, force)
        _accumulate_stats(totals, stats)


def cmd_stats_sync(args):
    """Sync JSONL files to metrics database."""
    force = getattr(args, "force", False)
    all_homes = getattr(args, "all_homes", False)
    remotes = getattr(args, "remotes", None) or []
    patterns = getattr(args, "patterns", [""])

    # Include saved sources when --ah is used
    if all_homes:
        for src in get_saved_sources():
            if src not in remotes:
                remotes.append(src)

    conn = init_metrics_db()
    totals = {"synced": 0, "skipped": 0, "errors": 0}

    # Sync local Claude sessions
    stats = _sync_source_to_db(conn, get_claude_projects_dir(), "local", "local", patterns, force)
    _accumulate_stats(totals, stats)

    # Sync local Codex sessions (~/.codex/sessions/)
    stats = _sync_codex_to_db(conn, patterns, force)
    _accumulate_stats(totals, stats)

    # Sync local Gemini sessions (~/.gemini/tmp/)
    stats = _sync_gemini_to_db(conn, patterns, force)
    _accumulate_stats(totals, stats)

    # Sync WSL/Windows if --ah flag
    if all_homes:
        _sync_wsl_windows_to_db(conn, totals, patterns, force)

    # Sync explicit remotes
    for remote in remotes:
        _sync_remote_to_db(conn, remote, totals, patterns, force)

    conn.close()
    print(
        f"\nSync complete: {totals['synced']} synced, {totals['skipped']} up-to-date, {totals['errors']} errors"
    )


def _get_stats_workspace_patterns(args) -> list:
    """Get workspace patterns for stats command, applying alias detection."""
    workspace_patterns = getattr(args, "workspace", None) or []
    all_workspaces = getattr(args, "all_workspaces", False)
    this_only = getattr(args, "this_only", False)

    if workspace_patterns or all_workspaces:
        return workspace_patterns

    full_pattern = get_current_workspace_pattern()
    ws_name = get_workspace_name_from_path(full_pattern)

    if not this_only:
        alias_name = get_alias_for_workspace(full_pattern, "local")
        if alias_name:
            sys.stderr.write(f"Using alias @{alias_name} (use --this for current workspace only)\n")
            return [f"@{alias_name}"]

    return [ws_name]


def _expand_alias_to_conditions(alias_name: str, aliases: dict) -> tuple:
    """Expand alias to SQL conditions and params. Returns (conditions, params)."""
    if alias_name not in aliases:
        print(f" Alias '{alias_name}' not found. Treating as workspace pattern.")
        return (["s.workspace LIKE ?"], [f"%{alias_name}%"])

    conditions = []
    params = []
    for source_key, workspaces in aliases[alias_name].items():
        if isinstance(workspaces, list):
            for ws in workspaces:
                conditions.append("s.workspace = ?")
                params.append(get_workspace_name_from_path(ws))
    return conditions, params


def _build_stats_where_clause(args, workspace_patterns: list) -> tuple:
    """Build WHERE clause for stats query. Returns (where_sql, params)."""
    where_clauses = []
    params = []

    if workspace_patterns:
        ws_conditions = []
        aliases = load_aliases().get("aliases", {})

        for pattern in workspace_patterns:
            if pattern.startswith("@"):
                conds, prms = _expand_alias_to_conditions(pattern[1:], aliases)
                ws_conditions.extend(conds)
                params.extend(prms)
            else:
                ws_conditions.append("s.workspace LIKE ?")
                params.append(f"%{pattern}%")

        if ws_conditions:
            where_clauses.append(f"({' OR '.join(ws_conditions)})")

    source_filter = getattr(args, "source", None)
    if source_filter:
        where_clauses.append("s.source = ?")
        params.append(source_filter)

    since_str = getattr(args, "since", None)
    if since_str:
        where_clauses.append("s.start_time >= ?")
        params.append(since_str)

    until_str = getattr(args, "until", None)
    if until_str:
        where_clauses.append("s.start_time <= ?")
        params.append(until_str + "T23:59:59")

    where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"
    return where_sql, params


def _display_selected_stats(conn, args, where_sql: str, params: list):
    """Display the appropriate stats view based on args."""
    top_limit = getattr(args, "top_ws", None)
    if top_limit is not None:
        try:
            top_limit = int(top_limit)
            if top_limit < 1:
                raise ValueError
        except (TypeError, ValueError):
            sys.stderr.write("Error: --top-ws must be a positive integer\n")
            sys.exit(1)

    if getattr(args, "tools", False):
        display_tool_stats(conn, where_sql, params)
    elif getattr(args, "models", False):
        display_model_stats(conn, where_sql, params)
    elif getattr(args, "time", False):
        display_time_stats(conn, where_sql, params)
    elif getattr(args, "by_workspace", False):
        display_workspace_stats(conn, where_sql, params)
    elif getattr(args, "by_day", False):
        display_daily_stats(conn, where_sql, params)
    else:
        display_summary_stats(conn, where_sql, params, top_limit=top_limit)


def cmd_stats(args):
    """Display metrics statistics."""
    db_path = get_metrics_db_path()

    if not db_path.exists():
        print("No metrics database found. Run 'stats --sync' first.")
        print("\nUsage:")
        print("  agent-history stats --sync        # Sync local sessions")
        print("  agent-history stats --sync --ah   # Sync from all homes")
        sys.exit(1)

    conn = init_metrics_db(db_path)
    workspace_patterns = _get_stats_workspace_patterns(args)
    where_sql, params = _build_stats_where_clause(args, workspace_patterns)
    _display_selected_stats(conn, args, where_sql, params)
    conn.close()


def _query_session_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query session statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_sessions,
            SUM(CASE WHEN is_agent = 0 THEN 1 ELSE 0 END) as main_sessions,
            SUM(CASE WHEN is_agent = 1 THEN 1 ELSE 0 END) as agent_sessions,
            SUM(message_count) as total_messages
        FROM sessions s
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _query_token_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query token statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COALESCE(SUM(m.input_tokens), 0) as total_input,
            COALESCE(SUM(m.output_tokens), 0) as total_output,
            COALESCE(SUM(m.cache_creation_tokens), 0) as total_cache_creation,
            COALESCE(SUM(m.cache_read_tokens), 0) as total_cache_read
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _query_tool_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query tool statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_tool_uses,
            SUM(CASE WHEN is_error = 1 THEN 1 ELSE 0 END) as tool_errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _build_workspace_to_alias_map(aliases: dict = None) -> dict:
    """Build mapping from (workspace, source) to alias name."""
    if aliases is None:
        aliases_data = load_aliases()
        aliases = aliases_data.get("aliases", {})
    else:
        aliases = aliases or {}

    workspace_to_alias = {}
    for alias_name, alias_config in aliases.items():
        for source_key, workspaces in alias_config.items():
            if isinstance(workspaces, list):
                normalized_source = "windows" if source_key.startswith("windows:") else source_key
                for ws in workspaces:
                    ws_name = get_workspace_name_from_path(ws)
                    workspace_to_alias[(ws_name, normalized_source)] = alias_name
    return workspace_to_alias


def _aggregate_workspace_stats(all_workspace_stats, workspace_to_alias: dict) -> list:
    """Aggregate workspace stats, grouping by alias where applicable."""
    alias_stats = {}
    unaliased_workspaces = []

    for row in all_workspace_stats:
        ws_name = row["workspace"]
        source = row["source"]
        sessions = row["sessions"]
        messages = row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {
                    "sessions": 0,
                    "messages": 0,
                    "workspaces": set(),
                    "sources": set(),
                }
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["workspaces"].add(ws_name)
            alias_stats[alias_name]["sources"].add(source)
        else:
            unaliased_workspaces.append(
                {
                    "name": ws_name,
                    "sessions": sessions,
                    "messages": messages,
                    "is_alias": False,
                    "source": source,
                }
            )

    top_workspaces = []
    for alias_name, stats in alias_stats.items():
        top_workspaces.append(
            {
                "name": f"@{alias_name}",
                "sessions": stats["sessions"],
                "messages": stats["messages"],
                "is_alias": True,
                "workspace_count": len(stats["workspaces"]),
                "source": ", ".join(sorted(stats["sources"])) if stats["sources"] else "alias",
            }
        )
    top_workspaces.extend(unaliased_workspaces)
    top_workspaces.sort(key=lambda x: x["sessions"], reverse=True)
    return top_workspaces


def _print_session_section(session_stats):
    """Print session statistics section."""
    print("\nSessions")
    print(f"   Total: {(session_stats['total_sessions'] or 0):,}")
    print(f"   Main sessions: {(session_stats['main_sessions'] or 0):,}")
    print(f"   Agent tasks: {(session_stats['agent_sessions'] or 0):,}")
    print(f"   Total messages: {(session_stats['total_messages'] or 0):,}")


def _print_token_section(token_stats):
    """Print token statistics section."""
    total_input = token_stats["total_input"] or 0
    total_output = token_stats["total_output"] or 0
    cache_creation = token_stats["total_cache_creation"] or 0
    cache_read = token_stats["total_cache_read"] or 0
    cache_total = cache_creation + cache_read
    cache_hit_ratio = (cache_read / cache_total * 100) if cache_total > 0 else 0

    print("\nTokens")
    print(f"   Input: {total_input:,}")
    print(f"   Output: {total_output:,}")
    print(f"   Cache created: {cache_creation:,}")
    print(f"   Cache read: {cache_read:,}")
    print(f"   Cache hit ratio: {cache_hit_ratio:.1f}%")


def _print_tool_section(tool_stats):
    """Print tool statistics section."""
    total_tool_uses = tool_stats["total_tool_uses"] or 0
    tool_errors = tool_stats["tool_errors"] or 0

    print("\nTools")
    print(f"   Total uses: {total_tool_uses:,}")
    print(f"   Errors: {tool_errors:,}")
    if total_tool_uses > 0:
        print(f"   Error rate: {tool_errors / total_tool_uses * 100:.1f}%")


def _print_sources_section(sources):
    """Print homes/sources section."""
    if not sources:
        return
    print("\nHomes")
    for src in sources:
        print(f"   {src['source']}: {src['count']:,} sessions")


def _print_models_section(models):
    """Print models section."""
    if not models:
        return
    print("\nModels (top 3)")
    for model in models:
        model_name = model["model"] or "unknown"
        short_name = (
            model_name.replace("claude-", "").replace("-20250929", "").replace("-20251001", "")
        )
        print(f"   {short_name}: {model['count']:,} messages")


def _print_workspaces_section(top_workspaces):
    """Print top workspaces section."""
    if not top_workspaces:
        return
    print("\nTop Workspaces")
    for ws in top_workspaces:
        if ws.get("is_alias"):
            ws_count = ws.get("workspace_count", 0)
            print(
                f"   {ws['name']} ({ws_count} workspaces): {ws['sessions']} sessions, {ws['messages']} msgs"
            )
        else:
            print(f"   {ws['name']}: {ws['sessions']} sessions, {ws['messages']} msgs")


def _parse_top_limit(top_limit) -> int | None:
    """Parse top_limit to a valid integer or None."""
    if top_limit is None:
        return None
    try:
        result = int(top_limit)
        return result if result >= 1 else None
    except (TypeError, ValueError):
        return None


def _print_workspace_item(ws: dict):
    """Print a single workspace item with appropriate formatting."""
    if ws.get("is_alias"):
        ws_count = ws.get("workspace_count", 0)
        print(
            f"      Workspace: {ws['name']} ({ws_count} workspaces)"
            f" — {ws['sessions']} sessions, {ws['messages']} msgs"
        )
    else:
        print(
            f"      Workspace: {ws['name']}" f" — {ws['sessions']} sessions, {ws['messages']} msgs"
        )


def _print_homes_and_workspaces(sources, top_workspaces, top_limit=None):
    """Print homes and workspaces together at the top of stats output."""
    if not sources and not top_workspaces:
        return
    print("\nHomes & Workspaces")

    source_map = {s.get("source"): s.get("count", 0) for s in (sources or [])}

    grouped = {}
    for ws in top_workspaces or []:
        home_label = ws.get("source") or "home"
        grouped.setdefault(home_label, []).append(ws)

    top_limit_int = _parse_top_limit(top_limit)

    for ws_items in grouped.values():
        ws_items.sort(key=lambda x: x.get("sessions", 0), reverse=True)

    ordered_homes = list(source_map.keys()) + [h for h in grouped if h not in source_map]

    for home in ordered_homes:
        total = source_map.get(home)
        print(f"   Home: {home} ({total:,} sessions)" if total is not None else f"   Home: {home}")
        ws_items = grouped.get(home, [])[:top_limit_int] if top_limit_int else grouped.get(home, [])
        for ws in ws_items:
            _print_workspace_item(ws)


def _print_summary_stats(
    session_stats,
    token_stats,
    tool_stats,
    sources,
    models,
    top_workspaces,
    top_limit=None,
    time_stats=None,
):
    """Print the formatted summary statistics."""
    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("CLAUDE CODE METRICS SUMMARY")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    _print_homes_and_workspaces(sources, top_workspaces, top_limit=top_limit)
    _print_session_section(session_stats)
    _print_token_section(token_stats)
    _print_tool_section(tool_stats)
    _print_models_section(models)
    if time_stats is not None:
        _print_time_summary(time_stats, include_breakdown=False)
    print()


def display_summary_stats(conn: sqlite3.Connection, where_sql: str, params: list, top_limit=None):
    """Display summary statistics dashboard."""
    session_stats = _query_session_stats(conn, where_sql, params)
    token_stats = _query_token_stats(conn, where_sql, params)
    tool_stats = _query_tool_stats(conn, where_sql, params)

    # Source breakdown
    cursor = conn.execute(
        f"SELECT source, COUNT(*) as count FROM sessions s WHERE {where_sql} GROUP BY source ORDER BY count DESC",
        params,
    )
    sources = [dict(row) for row in cursor.fetchall()]

    # Model breakdown (top 3)
    cursor = conn.execute(
        f"""SELECT m.model, COUNT(*) as count FROM messages m JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL GROUP BY m.model ORDER BY count DESC LIMIT 3""",
        params,
    )
    models = cursor.fetchall()

    # Top workspaces with alias aggregation
    cursor = conn.execute(
        f"""SELECT workspace, source, COUNT(*) as sessions, SUM(message_count) as messages FROM sessions s
        WHERE {where_sql} GROUP BY workspace, source ORDER BY sessions DESC""",
        params,
    )
    all_workspace_stats = cursor.fetchall()
    workspace_to_alias = _build_workspace_to_alias_map()
    top_workspaces = _aggregate_workspace_stats(all_workspace_stats, workspace_to_alias)

    time_stats = _compute_time_stats(conn, where_sql, params)

    _print_summary_stats(
        session_stats,
        token_stats,
        tool_stats,
        sources,
        models,
        top_workspaces,
        top_limit=top_limit,
        time_stats=time_stats,
    )


def display_tool_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display tool usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            t.tool_name,
            COUNT(*) as uses,
            SUM(CASE WHEN t.is_error = 1 THEN 1 ELSE 0 END) as errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
        GROUP BY t.tool_name
        ORDER BY uses DESC
    """,
        params,
    )
    tools = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("TOOL USAGE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    print(f"\n{'Tool':<25} {'Uses':>10} {'Errors':>10} {'Error %':>10}")
    print("-" * 55)

    for row in tools:
        uses = row["uses"]
        errors = row["errors"]
        error_pct = (errors / uses * 100) if uses > 0 else 0
        print(f"{row['tool_name']:<25} {uses:>10,} {errors:>10,} {error_pct:>9.1f}%")

    print()


def display_model_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display model usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            m.model,
            COUNT(*) as messages,
            COUNT(DISTINCT m.session_id) as sessions,
            SUM(m.input_tokens) as input_tokens,
            SUM(m.output_tokens) as output_tokens
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL
        GROUP BY m.model
        ORDER BY messages DESC
    """,
        params,
    )
    models = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("MODEL USAGE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    print(f"\n{'Model':<35} {'Messages':>10} {'Sessions':>10} {'Avg Out':>10}")
    print("-" * 65)

    for row in models:
        model = row["model"] or "unknown"
        short_model = (
            model.replace("claude-", "")
            .replace("-20250929", "")
            .replace("-20251001", "")
            .replace("-20251101", "")
        )
        messages = row["messages"]
        avg_output = (row["output_tokens"] / messages) if messages > 0 else 0
        print(f"{short_model:<35} {messages:>10,} {row['sessions']:>10,} {avg_output:>10,.0f}")


def _aggregate_workspace_stats_detailed(all_workspaces: list, workspace_to_alias: dict) -> tuple:
    """Aggregate workspace stats by alias. Returns (alias_stats, unaliased)."""
    alias_stats = {}
    unaliased = []

    for row in all_workspaces:
        ws_name, source = row["workspace"], row["source"]
        sessions, messages = row["sessions"], row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {"sessions": 0, "messages": 0, "sources": set()}
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["sources"].add(source)
        else:
            unaliased.append(
                {"name": ws_name, "source": source, "sessions": sessions, "messages": messages}
            )

    return alias_stats, unaliased


def display_workspace_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display per-workspace statistics with alias aggregation."""
    cursor = conn.execute(
        f"""
        SELECT s.workspace, s.source, COUNT(*) as sessions, SUM(s.message_count) as messages,
               SUM(m_stats.input_tokens) as input_tokens, SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (SELECT session_id, SUM(input_tokens) as input_tokens, SUM(output_tokens) as output_tokens
                   FROM messages GROUP BY session_id) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql}
        GROUP BY s.workspace, s.source ORDER BY sessions DESC
    """,
        params,
    )
    all_workspaces = cursor.fetchall()

    aliases_data = load_aliases()
    workspace_to_alias = _build_workspace_to_alias_map(aliases_data.get("aliases", {}))
    alias_stats, unaliased = _aggregate_workspace_stats_detailed(all_workspaces, workspace_to_alias)

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("WORKSPACE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    if alias_stats:
        print("\nAliases")
        print(f"{'Alias':<30} {'Sources':<15} {'Sessions':>8} {'Messages':>10}")
        print("-" * 65)
        for alias_name, stats in sorted(
            alias_stats.items(), key=lambda x: x[1]["sessions"], reverse=True
        ):
            sources_str = ",".join(sorted(stats["sources"]))[:13]
            print(
                f"@{alias_name:<29} {sources_str:<15} {stats['sessions']:>8,} {stats['messages']:>10,}"
            )

    print(f"\nIndividual Workspaces (Top {MAX_TOP_WORKSPACES})")
    print(f"{'Workspace':<30} {'Source':<10} {'Sessions':>8} {'Messages':>10}")
    print("-" * DISPLAY_SEPARATOR_WIDTH)

    for row in unaliased[:MAX_TOP_WORKSPACES]:
        ws_name = (
            row["name"][:MAX_WORKSPACE_NAME_DISPLAY]
            if len(row["name"]) > MAX_WORKSPACE_NAME_DISPLAY
            else row["name"]
        )
        print(f"{ws_name:<30} {row['source']:<10} {row['sessions']:>8,} {row['messages']:>10,}")

    print()


def display_daily_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display daily usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            DATE(s.start_time) as date,
            COUNT(*) as sessions,
            SUM(s.message_count) as messages,
            SUM(m_stats.input_tokens) as input_tokens,
            SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (
            SELECT session_id,
                   SUM(input_tokens) as input_tokens,
                   SUM(output_tokens) as output_tokens
            FROM messages
            GROUP BY session_id
        ) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql} AND s.start_time IS NOT NULL
        GROUP BY DATE(s.start_time)
        ORDER BY date DESC
        LIMIT 30
    """,
        params,
    )
    days = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("DAILY STATISTICS (Last 30 days)")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    max_sessions = max((row["sessions"] or 0) for row in days) if days else 0
    bar_header = "Bar (sessions)"
    print(f"\n{'Date':<12} {'Sessions':>10} {'Messages':>10} {'Input Tokens':>15}  {bar_header}")
    print("-" * 70)

    for row in days:
        date_str = row["date"] or "unknown"
        sessions = row["sessions"]
        messages = row["messages"] or 0
        tokens = row["input_tokens"] or 0
        bar = _build_bar(sessions, max_sessions)
        print(f"{date_str:<12} {sessions:>10,} {messages:>10,} {tokens:>15,}  {bar}")

    print()


def _build_bar(value: int, max_value: int, width: int = 20) -> str:
    """Return an ASCII bar scaled to the maximum value."""
    if max_value <= 0 or value <= 0:
        return ""
    ratio = value / max_value
    length = max(1, min(width, math.ceil(ratio * width)))
    return "#" * length


def format_duration_hm(seconds: float) -> str:
    """Format seconds as Xh Ym."""
    if seconds < SECONDS_PER_MINUTE:
        return f"{seconds:.0f}s"
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    if hours > 0:
        return f"{hours}h {minutes}m"
    return f"{minutes}m"


def _parse_timestamp_strings(raw_timestamps: list) -> list:
    """Parse timestamp strings to datetime objects, filtering invalid ones."""
    parsed = []
    for ts_str in raw_timestamps:
        if not ts_str:
            continue
        try:
            parsed.append(datetime.fromisoformat(ts_str.replace("Z", "+00:00")))
        except (ValueError, TypeError):
            continue
    return parsed


def _ensure_date_entry(stats: dict, date_str: str):
    """Ensure a date entry exists in the stats dict."""
    if date_str not in stats:
        stats[date_str] = {"work_seconds": 0.0, "messages": 0, "work_periods": 0}


def _add_period_time_to_stats(
    stats: dict, start_dt: datetime, end_dt: datetime, is_new_period: bool = False
):
    """Add work period time to stats, splitting across day boundaries if needed."""
    if start_dt >= end_dt:
        return

    current = start_dt
    first_day = True
    while current < end_dt:
        date_str = current.strftime("%Y-%m-%d")
        _ensure_date_entry(stats, date_str)

        if is_new_period and first_day:
            stats[date_str]["work_periods"] += 1
            first_day = False

        next_midnight = (current + timedelta(days=1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )

        if next_midnight >= end_dt:
            stats[date_str]["work_seconds"] += (end_dt - current).total_seconds()
            break
        stats[date_str]["work_seconds"] += (next_midnight - current).total_seconds()
        current = next_midnight


def calculate_daily_work_time(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Calculate work time per day from message timestamps.

    This properly distributes work time across days, even for sessions
    that span multiple days. Uses gap detection (30 min threshold) to
    identify work periods.

    IMPORTANT: Merges ALL timestamps across all files to avoid double-counting
    overlapping time from concurrent agents.

    Returns dict: {date_str: {'work_seconds': float, 'messages': int, 'work_periods': int}}
    """
    cursor = conn.execute(
        f"""
        SELECT m.timestamp
        FROM messages m
        JOIN sessions s ON m.file_path = s.file_path
        WHERE {where_sql} AND m.timestamp IS NOT NULL
        ORDER BY m.timestamp
    """,
        params,
    )

    parsed_timestamps = _parse_timestamp_strings([row["timestamp"] for row in cursor])
    if not parsed_timestamps:
        return {}
    parsed_timestamps.sort()

    daily_stats = {}
    prev_ts = None
    period_start = None

    for ts in parsed_timestamps:
        date_str = ts.strftime("%Y-%m-%d")
        _ensure_date_entry(daily_stats, date_str)
        daily_stats[date_str]["messages"] += 1

        if prev_ts is None:
            period_start = ts
            daily_stats[date_str]["work_periods"] += 1
        elif (ts - prev_ts).total_seconds() > WORK_PERIOD_GAP_THRESHOLD:
            _add_period_time_to_stats(daily_stats, period_start, prev_ts)
            period_start = ts
            daily_stats[date_str]["work_periods"] += 1

        prev_ts = ts

    if period_start and prev_ts:
        _add_period_time_to_stats(daily_stats, period_start, prev_ts)

    return daily_stats


def _calculate_time_totals(daily_stats: dict) -> tuple:
    """Calculate total work, messages, and periods from daily stats."""
    total_work = sum(d["work_seconds"] for d in daily_stats.values())
    total_messages = sum(d["messages"] for d in daily_stats.values())
    total_periods = sum(d["work_periods"] for d in daily_stats.values())
    return total_work, total_messages, total_periods


def _print_daily_breakdown(
    daily_stats: dict, total_work: int, total_periods: int, total_messages: int
):
    """Print daily breakdown table."""
    print("\nDaily Breakdown")
    print(f"\n{'Date':<12} {'Work Time':>12} {'Periods':>10} {'Messages':>10}  Bar (time)")
    print("-" * 70)

    max_work = (
        max((stats["work_seconds"] or 0) for stats in daily_stats.values()) if daily_stats else 0
    )

    for date_str in sorted(daily_stats.keys(), reverse=True):
        stats = daily_stats[date_str]
        work_seconds = stats["work_seconds"]
        work_time = format_duration_hm(work_seconds)
        bar = _build_bar(int(work_seconds), int(max_work))
        print(
            f"{date_str:<12} {work_time:>12} {stats['work_periods']:>10} {stats['messages']:>10}  {bar}"
        )

    print("-" * 70)
    print(
        f"{'TOTAL':<12} {format_duration_hm(total_work):>12} {total_periods:>10} {total_messages:>10}"
    )


def _compute_time_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Compute time stats used by both summary and --time output."""
    daily_stats = calculate_daily_work_time(conn, where_sql, params)
    total_work, total_messages, total_periods = _calculate_time_totals(daily_stats)

    cursor = conn.execute(f"SELECT COUNT(*) as num_files FROM sessions s WHERE {where_sql}", params)
    num_files = cursor.fetchone()["num_files"] or 0

    dates = sorted(daily_stats.keys()) if daily_stats else []
    first_date, last_date = (dates[0], dates[-1]) if dates else (None, None)

    return {
        "daily_stats": daily_stats,
        "total_work": total_work,
        "total_messages": total_messages,
        "total_periods": total_periods,
        "num_files": num_files,
        "first_date": first_date,
        "last_date": last_date,
    }


def _print_time_summary(time_stats: dict, include_breakdown: bool = False):
    """Print time tracking summary (optionally with daily breakdown)."""
    daily_stats = time_stats["daily_stats"]
    total_work = time_stats["total_work"]
    total_messages = time_stats["total_messages"]
    total_periods = time_stats["total_periods"]
    num_files = time_stats["num_files"]
    first_date = time_stats["first_date"]
    last_date = time_stats["last_date"]

    print("\nTime")
    print(f"   Total work time: {format_duration_hm(total_work)}")
    print(f"   Work periods: {total_periods}")
    print(f"   Session files: {num_files}")
    if first_date and last_date:
        print(f"   Date range: {first_date} to {last_date}")

    if include_breakdown and daily_stats:
        _print_daily_breakdown(daily_stats, total_work, total_periods, total_messages)


def display_time_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display time tracking statistics with daily breakdown."""
    time_stats = _compute_time_stats(conn, where_sql, params)

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("TIME TRACKING")
    print("=" * DISPLAY_SEPARATOR_WIDTH)
    _print_time_summary(time_stats, include_breakdown=True)
    print()


# ============================================================================
# Commands
# ============================================================================


def _list_windows_sessions(args, patterns: list, since_date, until_date):
    """List sessions from Windows (called from WSL).

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
    """
    remote_host = getattr(args, "remote", None)
    username = None
    if remote_host and remote_host.startswith("windows://"):
        username = remote_host[10:]

    windows_projects_dir = get_windows_projects_dir(username)
    if not windows_projects_dir:
        sys.stderr.write("Error: Cannot access Windows projects directory\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  • Make sure Claude Code is installed on Windows\n")
        sys.stderr.write("  • Try: agent-history lsh --windows\n")
        sys.exit(1)

    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        projects_dir=windows_projects_dir,
        skip_message_count=True,  # Slow over WSL mount
        agent=agent,
    )

    # If no native sessions, fall back to include cached (e.g., wsl_*) so users still see content
    if not sessions:
        sessions = collect_sessions_with_dedup(
            patterns,
            since_date,
            until_date,
            projects_dir=windows_projects_dir,
            skip_message_count=True,
            include_cached=True,
            agent=agent,
        )

    if not sessions:
        sys.stderr.write("Error: No sessions found in Windows\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  \a Check that Windows has native workspaces (not just cached WSL)\n")
        sys.stderr.write("  \a Try: agent-history lsh --windows\n")
        sys.exit(1)

    source_label = f"Windows ({username})" if username else "Windows"
    workspaces_only = getattr(args, "workspaces_only", False)
    print_sessions_output(sessions, source_label, workspaces_only)


def _list_wsl_sessions(args, patterns: list, since_date, until_date):
    """List sessions from WSL (called from Windows).

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
    """
    remote_host = getattr(args, "remote", None)
    distro_name = remote_host[WSL_PREFIX_LEN:]  # Remove 'wsl://' prefix

    wsl_projects_dir = get_wsl_projects_dir(distro_name)
    if not wsl_projects_dir:
        sys.stderr.write(f"Error: Cannot access WSL distribution '{distro_name}'\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  • Make sure Claude Code is installed in WSL\n")
        sys.stderr.write("  • Try: agent-history lsh --wsl\n")
        sys.exit(1)

    workspaces_only = getattr(args, "workspaces_only", False)
    agent = getattr(args, "agent", None) or "auto"

    if workspaces_only:
        raw_workspaces = _get_workspaces_from_dir(wsl_projects_dir)
        workspaces = _filter_workspaces_by_patterns(raw_workspaces, patterns)
        base_path = _detect_wsl_base_path(wsl_projects_dir)
        sessions = [
            {
                "workspace": ws,
                "workspace_readable": normalize_workspace_name(ws, base_path=base_path),
            }
            for ws in workspaces
        ]
    else:
        sessions = collect_sessions_with_dedup(
            patterns,
            since_date,
            until_date,
            projects_dir=wsl_projects_dir,
            skip_message_count=True,  # Slow over Windows/WSL boundary
            agent=agent,
        )

    if not sessions:
        sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
        sys.exit(1)

    source_label = f"WSL ({distro_name})"
    print_sessions_output(sessions, source_label, workspaces_only)


def _list_remote_workspaces_only(remote_host: str, patterns: list):
    """List workspaces only for remote host."""
    batch_workspaces = get_remote_workspaces_batch(remote_host)
    if not batch_workspaces:
        exit_with_error(f"No workspaces found on {remote_host}")

    batch_workspaces = [
        ws for ws in batch_workspaces if matches_any_pattern(ws["encoded"], patterns)
    ]
    if not batch_workspaces:
        pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
        exit_with_error(f"No workspaces matching pattern '{pattern_str}'")

    for ws_info in sorted(batch_workspaces, key=lambda x: x["decoded"]):
        print(ws_info["decoded"])


def _collect_remote_session_details(remote_host: str, patterns: list, since_date, until_date):
    """Collect detailed session info from remote workspaces."""
    remote_workspaces = list_remote_workspaces(remote_host)
    if not remote_workspaces:
        exit_with_error(f"No workspaces found on {remote_host}")

    remote_workspaces = [ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)]
    if not remote_workspaces:
        pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
        exit_with_error(f"No workspaces matching pattern '{pattern_str}'")

    sessions = []
    for remote_workspace in remote_workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        workspace_sessions = get_remote_session_info(remote_host, remote_workspace)

        for session_info in workspace_sessions:
            session = _filter_sessions_by_date([session_info], since_date, until_date)
            if session:
                sessions.append(
                    {
                        "workspace": remote_workspace,
                        "workspace_readable": readable_name,
                        "filename": session_info["filename"],
                        "size_kb": session_info["size_kb"],
                        "modified": session_info["modified"],
                        "message_count": session_info["message_count"],
                    }
                )
    return sessions


def _list_ssh_remote_sessions(args, patterns: list, since_date, until_date):
    """List sessions from SSH remote."""
    remote_host = getattr(args, "remote", None)

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        exit_with_error(f"Setup: ssh-copy-id {remote_host}")

    workspaces_only = getattr(args, "workspaces_only", False)

    if workspaces_only:
        _list_remote_workspaces_only(remote_host, patterns)
    else:
        sessions = _collect_remote_session_details(remote_host, patterns, since_date, until_date)
        if not sessions:
            exit_with_error("No sessions found")
        print_sessions_output(sessions, f"Remote ({remote_host})", workspaces_only=False)


def _list_local_sessions(args, patterns: list, since_date, until_date):
    """List sessions from local filesystem.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
    """
    projects_dir = getattr(args, "projects_dir", None)
    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        include_cached=True,
        projects_dir=projects_dir,
        agent=agent,
    )

    workspaces_only = getattr(args, "workspaces_only", False)

    if not sessions:
        if not workspaces_only:
            pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
            if since_date or until_date:
                date_range = []
                if since_date:
                    date_range.append(f"from {since_date.strftime('%Y-%m-%d')}")
                if until_date:
                    date_range.append(f"until {until_date.strftime('%Y-%m-%d')}")
                sys.stderr.write(f"No sessions found {' '.join(date_range)}\n")
            else:
                sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
            sys.exit(1)
        return

    in_wsl = is_running_in_wsl()
    source_label = "Local (WSL)" if in_wsl else "Local"
    print_sessions_output(sessions, source_label, workspaces_only)


def cmd_list(args):
    """List sessions for a workspace."""
    remote_host = getattr(args, "remote", None)

    # Parse and validate date filters
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)
    since_date, until_date = parse_and_validate_dates(since_str, until_str)

    # Get patterns
    patterns = get_patterns_from_args(args)

    # Dispatch to appropriate handler based on source type
    if remote_host:
        if is_windows_remote(remote_host):
            _list_windows_sessions(args, patterns, since_date, until_date)
        elif is_wsl_remote(remote_host):
            _list_wsl_sessions(args, patterns, since_date, until_date)
        else:
            _list_ssh_remote_sessions(args, patterns, since_date, until_date)
    else:
        _list_local_sessions(args, patterns, since_date, until_date)


def cmd_convert(args):  # noqa: C901
    """Convert a single .jsonl file to markdown."""
    remote_host = getattr(args, "remote", None)

    if remote_host:
        # Handle remote file conversion
        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Download file to temporary location
        with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            # Use scp to download the file
            result = subprocess.run(
                [get_command_path("scp"), f"{remote_host}:{args.jsonl_file}", str(tmp_path)],
                check=False,
                capture_output=True,
                text=True,
                timeout=SCP_TIMEOUT,
            )

            if result.returncode != 0:
                sys.stderr.write(f"Error downloading file: {result.stderr}\n")
                tmp_path.unlink()
                sys.exit(1)

            # Convert the downloaded file
            jsonl_file = tmp_path
            filename = Path(args.jsonl_file).name
            output_file = Path(args.output) if args.output else Path(filename).with_suffix(".md")

            # Detect agent type from original remote path and use appropriate parser
            agent_type = detect_agent_from_path(Path(args.jsonl_file))
            if agent_type == AGENT_GEMINI:
                markdown = gemini_parse_json_to_markdown(jsonl_file)
            elif agent_type == AGENT_CODEX:
                markdown = codex_parse_jsonl_to_markdown(jsonl_file)
            else:
                markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding="utf-8")

            print(output_file)

        finally:
            # Clean up temporary file
            if tmp_path.exists():
                tmp_path.unlink()

    else:
        # Handle local file conversion
        jsonl_file = Path(args.jsonl_file)

        if not jsonl_file.exists():
            sys.stderr.write(f"Error: {jsonl_file} not found\n")
            sys.exit(1)

        output_file = Path(args.output) if args.output else jsonl_file.with_suffix(".md")

        try:
            # Detect agent type and use appropriate parser
            agent_type = detect_agent_from_path(jsonl_file)
            if agent_type == AGENT_GEMINI:
                markdown = gemini_parse_json_to_markdown(jsonl_file)
            elif agent_type == AGENT_CODEX:
                markdown = codex_parse_jsonl_to_markdown(jsonl_file)
            else:
                markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding="utf-8")

            print(output_file)
        except (OSError, json.JSONDecodeError) as e:
            sys.stderr.write(f"Error converting file: {e}\n")
            sys.exit(1)


def _get_batch_windows_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from Windows for batch export.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter

    Returns:
        List of session dictionaries.
    """
    remote_host = getattr(args, "remote", None)
    username = None
    if remote_host and remote_host.startswith("windows://"):
        username = remote_host[10:]

    windows_projects_dir = get_windows_projects_dir(username)
    if not windows_projects_dir:
        sys.stderr.write("Error: Cannot access Windows projects directory\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  • Make sure Claude Code is installed on Windows\n")
        sys.stderr.write("  • Try: agent-history lsh --windows\n")
        sys.exit(1)

    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        projects_dir=windows_projects_dir,
        skip_message_count=True,  # Slow over WSL mount
        agent=agent,
    )

    if not sessions:
        sys.stderr.write("Error: No native sessions found in Windows\n")
        sys.exit(1)

    return sessions


def _get_batch_wsl_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from WSL for batch export.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter

    Returns:
        List of session dictionaries.
    """
    remote_host = getattr(args, "remote", None)
    distro_name = remote_host[WSL_PREFIX_LEN:]  # Remove 'wsl://' prefix

    wsl_projects_dir = get_wsl_projects_dir(distro_name)
    if not wsl_projects_dir:
        sys.stderr.write(f"Error: Cannot access WSL distribution '{distro_name}'\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  • Make sure Claude Code is installed in WSL\n")
        sys.stderr.write("  • Try: agent-history lsh --wsl\n")
        sys.exit(1)

    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        projects_dir=wsl_projects_dir,
        skip_message_count=True,  # Slow over Windows/WSL boundary
        agent=agent,
    )

    if not sessions:
        sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
        sys.exit(1)

    return sessions


def _fetch_remote_workspaces(
    remote_host: str, workspaces: list, local_projects_dir: Path, hostname: str
):
    """Fetch multiple remote workspaces to local cache."""
    for remote_workspace in workspaces:
        result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)
        if not result["success"]:
            sys.stderr.write(
                f"Error fetching {remote_workspace}: {result.get('error', 'Unknown error')}\n"
            )


def _deduplicate_sessions(sessions: list, patterns: list) -> list:
    """Filter sessions by pattern and remove duplicates."""
    result = []
    seen_files = set()
    for session in sessions:
        if not matches_any_pattern(session["workspace"], patterns):
            continue
        file_key = str(session["file"])
        if file_key not in seen_files:
            seen_files.add(file_key)
            result.append(session)
    return result


def _get_batch_ssh_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from SSH remote for batch export."""
    remote_host = getattr(args, "remote", None)

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    all_remote_workspaces = list_remote_workspaces(remote_host)
    if not all_remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    remote_workspaces = [ws for ws in all_remote_workspaces if matches_any_pattern(ws, patterns)]
    if not remote_workspaces:
        return []

    local_projects_dir = get_claude_projects_dir()
    hostname = get_remote_hostname(remote_host)
    _fetch_remote_workspaces(remote_host, remote_workspaces, local_projects_dir, hostname)

    cached_prefix = f"remote_{hostname}_"
    all_cached_sessions = get_workspace_sessions(
        cached_prefix, quiet=True, since_date=since_date, until_date=until_date, include_cached=True
    )
    return _deduplicate_sessions(all_cached_sessions, patterns)


def _get_batch_local_sessions(patterns: list, since_date, until_date, agent: str = "auto") -> list:
    """Get sessions from local filesystem for batch export.

    Args:
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
        agent: Agent backend - 'auto', 'claude', or 'codex'

    Returns:
        List of session dictionaries.
    """
    return collect_sessions_with_dedup(
        patterns, since_date, until_date, include_cached=True, agent=agent
    )


def _extract_timestamp_prefix(messages: list) -> str:
    """Extract timestamp prefix from first message for filename."""
    if messages and messages[0].get("timestamp"):
        try:
            dt = datetime.fromisoformat(messages[0]["timestamp"].replace("Z", "+00:00"))
            return dt.strftime("%Y%m%d%H%M%S")
        except ValueError:
            pass
    return None


def _get_export_output_path(
    session: dict, ts_prefix: str, output_dir: Path, flat: bool, remote_host: str
) -> tuple:
    """Get output path and name for export file.

    Returns:
        Tuple of (output_file, output_name)
    """
    jsonl_file = session["file"]

    if flat:
        output_name = f"{ts_prefix}_{jsonl_file.stem}.md" if ts_prefix else f"{jsonl_file.stem}.md"
        return output_dir / output_name, output_name

    source_tag = get_source_tag(remote_host)
    output_name = (
        f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
        if ts_prefix
        else f"{source_tag}{jsonl_file.stem}.md"
    )
    workspace_name = get_workspace_name_from_path(session["workspace"])
    workspace_subdir = output_dir / workspace_name
    workspace_subdir.mkdir(parents=True, exist_ok=True)
    return workspace_subdir / output_name, output_name


def _export_sessions_to_markdown(sessions: list, args, output_dir: Path):  # noqa: C901
    """Export sessions to markdown files.

    Args:
        sessions: List of session dictionaries
        args: Parsed arguments (for force, flat, minimal, split, remote)
        output_dir: Output directory path
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    force = getattr(args, "force", False)
    flat = getattr(args, "flat", False)
    remote_host = getattr(args, "remote", None)
    minimal = getattr(args, "minimal", False)
    split_lines = getattr(args, "split", None)

    for session in sessions:
        jsonl_file = session["file"]

        # Detect agent type and use appropriate reader/parser
        agent_type = detect_agent_from_path(jsonl_file)

        try:
            if agent_type == AGENT_GEMINI:
                messages, _ = gemini_read_json_messages(jsonl_file)
            elif agent_type == AGENT_CODEX:
                messages, _ = codex_read_jsonl_messages(jsonl_file)
            else:
                messages = read_jsonl_messages(jsonl_file)
        except (OSError, json.JSONDecodeError) as e:
            sys.stderr.write(f"Error reading {jsonl_file.name}: {e}\n")
            continue

        ts_prefix = _extract_timestamp_prefix(messages)
        output_file, output_name = _get_export_output_path(
            session, ts_prefix, output_dir, flat, remote_host
        )

        # Skip if up-to-date (incremental export)
        if not force and output_file.exists():
            if output_file.stat().st_mtime >= jsonl_file.stat().st_mtime:
                continue

        try:
            # Codex/Gemini files don't support splitting yet; use direct export
            if agent_type in (AGENT_CODEX, AGENT_GEMINI):
                _write_single_file(
                    jsonl_file, output_file, minimal, messages=messages, agent=agent_type
                )
            elif split_lines and messages:
                parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)
                if parts:
                    _write_split_parts(parts, output_name, output_file)
                else:
                    _write_single_file(jsonl_file, output_file, minimal, messages=messages)
            else:
                _write_single_file(jsonl_file, output_file, minimal, messages=messages)
        except (OSError, json.JSONDecodeError) as e:
            sys.stderr.write(f"Error converting {jsonl_file.name}: {e}\n")


def _write_split_parts(parts: list, output_name: str, output_file: Path):
    """Write split conversation parts with navigation links."""
    base_name = output_name.rsplit(".md", 1)[0]
    total_parts = len(parts)

    for part_num, _, part_md, _, _ in parts:
        part_filename = f"{base_name}_part{part_num}.md"
        part_file = output_file.parent / part_filename

        # Build navigation footer
        nav_parts = [f"**Part {part_num} of {total_parts}**"]
        if part_num > 1:
            nav_parts.append(f"[← Part {part_num - 1}]({base_name}_part{part_num - 1}.md)")
        if part_num < total_parts:
            nav_parts.append(f"[Part {part_num + 1} →]({base_name}_part{part_num + 1}.md)")

        part_file.write_text(part_md + "\n---\n\n> " + " | ".join(nav_parts), encoding="utf-8")
        print(part_file)


def _write_single_file(
    jsonl_file: Path,
    output_file: Path,
    minimal: bool,
    messages: list = None,
    agent: str = AGENT_CLAUDE,
):
    """Write a single markdown file.

    Args:
        jsonl_file: Source JSONL/JSON file path
        output_file: Destination markdown file path
        minimal: If True, produce minimal output without metadata
        messages: Pre-read messages (optional, avoids re-reading file)
        agent: Agent type (claude, codex, or gemini)
    """
    if agent == AGENT_GEMINI:
        markdown = gemini_parse_json_to_markdown(jsonl_file, minimal=minimal)
    elif agent == AGENT_CODEX:
        markdown = codex_parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
    else:
        markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal, messages=messages)
    output_file.write_text(markdown, encoding="utf-8")
    print(output_file)


def _get_batch_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get batch sessions based on source type."""
    remote_host = getattr(args, "remote", None)
    agent = getattr(args, "agent", None) or "auto"
    if not remote_host:
        return _get_batch_local_sessions(patterns, since_date, until_date, agent=agent)
    if is_windows_remote(remote_host):
        return _get_batch_windows_sessions(args, patterns, since_date, until_date)
    if is_wsl_remote(remote_host):
        return _get_batch_wsl_sessions(args, patterns, since_date, until_date)
    return _get_batch_ssh_sessions(args, patterns, since_date, until_date)


def _handle_empty_sessions(patterns: list, since_date, until_date, lenient: bool) -> bool:
    """Handle case when no sessions found.

    Returns:
        True if should continue (lenient mode), False if error (caller should exit).
    """
    if lenient:
        return True

    pattern_str = (
        ", ".join(patterns) if patterns and len(patterns) > 1 else (patterns[0] if patterns else "")
    )
    if since_date or until_date:
        date_range = []
        if since_date:
            date_range.append(f"from {since_date.strftime('%Y-%m-%d')}")
        if until_date:
            date_range.append(f"until {until_date.strftime('%Y-%m-%d')}")
        sys.stderr.write(f"No sessions found {' '.join(date_range)}\n")
    else:
        sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
    return False


def cmd_batch(args) -> bool:
    """Batch convert all sessions from a workspace.

    Returns:
        True on success, False if no sessions found (and not lenient).
    """
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)
    since_date, until_date = parse_and_validate_dates(since_str, until_str)

    patterns = get_patterns_from_args(args)
    sessions = _get_batch_sessions(args, patterns, since_date, until_date)

    if not sessions:
        return _handle_empty_sessions(
            patterns, since_date, until_date, getattr(args, "lenient", False)
        )

    _export_sessions_to_markdown(sessions, args, Path(args.output_dir))
    return True


def cmd_version(args):
    """Show version information."""
    print(f"agent-history {__version__}")


def cmd_list_wsl(args):
    """List available WSL distributions with Claude Code workspaces."""
    distributions = get_wsl_distributions()

    if not distributions:
        sys.stderr.write("No WSL distributions found or WSL is not available.\n")
        sys.stderr.write("Make sure WSL is installed on Windows.\n")
        sys.exit(1)

    # Filter to only distributions with Claude Code
    claude_distros = [d for d in distributions if d["has_claude"]]

    if not claude_distros:
        sys.stderr.write("No WSL distributions with Claude Code workspaces found.\n")
        sys.exit(1)

    # Print tab-separated output
    print("DISTRO\tUSERNAME\tPATH")
    for distro in claude_distros:
        print(f"{distro['name']}\t{distro['username']}\t{distro['path']}")


def cmd_list_windows(args):
    """List available Windows users with Claude Code workspaces (from WSL)."""
    if not is_running_in_wsl():
        sys.stderr.write("Error: --list-windows is only available from WSL.\n")
        sys.stderr.write("To access Windows sessions from Windows, use the tool directly.\n")
        sys.exit(1)

    users = get_windows_users_with_claude()

    if not users:
        sys.stderr.write("No Windows users with Claude Code workspaces found.\n")
        sys.stderr.write("Make sure Claude Code is installed on Windows.\n")
        sys.exit(1)

    # Print tab-separated output
    print("USERNAME\tDRIVE\tWORKSPACES\tPATH")
    for user in users:
        print(f"{user['username']}\t{user['drive']}\t{user['workspace_count']}\t{user['path']}")


def _collect_local_sessions(patterns, since_date, until_date, in_wsl, agent: str = "auto"):
    """Collect sessions from local environment."""
    results = []
    try:
        local_label = "Local (WSL)" if in_wsl else "Local"
        native_sessions = collect_sessions_with_dedup(
            patterns,
            since_date=since_date,
            until_date=until_date,
            include_cached=False,
            agent=agent,
        )
        if native_sessions:
            results.append((local_label, native_sessions))
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")
    return results


def _collect_windows_sessions_from_wsl(patterns, since_date, until_date, agent: str = "auto"):
    """Collect sessions from Windows when running in WSL."""
    results = []
    try:
        windows_users = get_windows_users_with_claude()
        for user in windows_users:
            try:
                windows_projects_dir = get_windows_projects_dir(user["username"])
                native_sessions = collect_sessions_with_dedup(
                    patterns,
                    since_date=since_date,
                    until_date=until_date,
                    projects_dir=windows_projects_dir,
                    include_cached=False,
                    skip_message_count=True,
                    agent=agent,
                )
                if native_sessions:
                    results.append((f"Windows ({user['username']})", native_sessions))
            except (OSError, PermissionError) as e:
                sys.stderr.write(f"Error accessing Windows ({user['username']}): {e}\n")
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error listing Windows users: {e}\n")
    return results


def _collect_wsl_sessions_from_windows(patterns, since_date, until_date, agent: str = "auto"):
    """Collect sessions from WSL when running on Windows."""
    results = []
    try:
        wsl_distros = get_wsl_distributions()
        claude_distros = [d for d in wsl_distros if d["has_claude"]]
        for distro in claude_distros:
            try:
                wsl_projects_dir = get_wsl_projects_dir(distro["name"])
                native_sessions = collect_sessions_with_dedup(
                    patterns,
                    since_date=since_date,
                    until_date=until_date,
                    projects_dir=wsl_projects_dir,
                    include_cached=False,
                    skip_message_count=True,
                    agent=agent,
                )
                if native_sessions:
                    results.append((f"WSL ({distro['name']})", native_sessions))
            except (OSError, PermissionError) as e:
                sys.stderr.write(f"Error accessing WSL ({distro['name']}): {e}\n")
    except (OSError, subprocess.SubprocessError) as e:
        sys.stderr.write(f"Error listing WSL distributions: {e}\n")
    return results


def _filter_workspaces_by_patterns(workspaces, patterns):
    """Filter workspace list by patterns (any pattern matches)."""
    if not patterns or patterns == [""]:
        return workspaces
    filtered = []
    for ws in workspaces:
        for pattern in patterns:
            if pattern in ws:
                filtered.append(ws)
                break
    return filtered


def _session_in_date_range(session: dict, since_date, until_date) -> bool:
    """Check if session falls within date range."""
    modified = session.get("modified")
    if not modified:
        return True
    session_date = modified.date()
    if since_date and session_date < since_date:
        return False
    if until_date and session_date > until_date:
        return False
    return True


def _enrich_and_filter_sessions(ws_info, ws_name, since_date, until_date) -> list:
    """Filter sessions by date and add workspace info."""
    sessions = []
    for session in ws_info:
        if not _session_in_date_range(session, since_date, until_date):
            continue
        session["workspace"] = ws_name
        session["workspace_readable"] = normalize_workspace_name(ws_name)
        sessions.append(session)
    return sessions


def _collect_remote_sessions(remote, patterns, since_date, until_date):
    """Collect sessions from a single SSH remote."""
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Cannot connect to remote: {remote}\n")
        return None

    hostname = get_remote_hostname(remote)
    all_remote_workspaces = list_remote_workspaces(remote)
    remote_workspaces = _filter_workspaces_by_patterns(all_remote_workspaces, patterns)

    if not remote_workspaces:
        return None

    remote_sessions = []
    for ws_name in remote_workspaces:
        ws_info = get_remote_session_info(remote, ws_name)
        remote_sessions.extend(
            _enrich_and_filter_sessions(ws_info, ws_name, since_date, until_date)
        )

    if remote_sessions:
        return (f"Remote ({hostname})", remote_sessions)
    return None


def _collect_all_remote_sessions(remotes, patterns, since_date, until_date):
    """Collect sessions from all SSH remotes."""
    results = []
    for remote in remotes:
        try:
            result = _collect_remote_sessions(remote, patterns, since_date, until_date)
            if result:
                results.append(result)
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")
    return results


def _print_all_homes_results(all_results, workspaces_only):
    """Print results from all homes listing."""
    if not all_results:
        msg = "No workspaces found.\n" if workspaces_only else "No sessions found.\n"
        sys.stderr.write(msg)
        return

    if workspaces_only:
        for source_label, sessions in all_results:
            print(f"# {source_label}")
            workspaces = sorted({s["workspace_readable"] for s in sessions})
            for ws in workspaces:
                print(f"  {ws}")
            print()
    else:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for source_label, sessions in all_results:
            for session in sessions:
                agent = session.get("agent", AGENT_CLAUDE)
                print(
                    f"{agent}\t{source_label}\t{session['workspace_readable']}\t{session['filename']}\t"
                    f"{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                )


def cmd_list_all_homes(args):
    """List workspaces or sessions from all homes: local, WSL/Windows, and remotes."""
    in_wsl = is_running_in_wsl()
    workspaces_only = getattr(args, "workspaces_only", False)
    agent = getattr(args, "agent", None) or "auto"

    patterns = getattr(args, "patterns", None)
    if patterns is None:
        patterns = [getattr(args, "workspace", "")]

    since_date = getattr(args, "since_date", None)
    until_date = getattr(args, "until_date", None)
    remotes = getattr(args, "remotes", []) or []

    saved_sources = get_saved_sources()
    for src in saved_sources:
        if src not in remotes:
            remotes.append(src)

    all_results = []

    # Collect from all sources
    all_results.extend(_collect_local_sessions(patterns, since_date, until_date, in_wsl, agent))

    if in_wsl:
        all_results.extend(
            _collect_windows_sessions_from_wsl(patterns, since_date, until_date, agent)
        )
    else:
        all_results.extend(
            _collect_wsl_sessions_from_windows(patterns, since_date, until_date, agent)
        )

    all_results.extend(_collect_all_remote_sessions(remotes, patterns, since_date, until_date))

    _print_all_homes_results(all_results, workspaces_only)


def _lsh_show_local():
    """Show local home folder."""
    try:
        projects_dir = get_claude_projects_dir()
        if projects_dir.exists():
            workspace_count = len(
                [d for d in projects_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]
            )
            print("Local:")
            print(f"  {projects_dir.parent}\t{workspace_count} workspaces")
            print()
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")


def _lsh_show_wsl():
    """Show WSL distributions (when on Windows)."""
    print("WSL Distributions:")
    try:
        wsl_distros = get_wsl_distributions()
        claude_distros = [d for d in wsl_distros if d["has_claude"]]

        if claude_distros:
            for distro in claude_distros:
                projects_dir = get_wsl_projects_dir(distro["name"])
                workspace_count = len(
                    [d for d in projects_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]
                )
                print(
                    f"  {distro['name']}\t{distro['username']}\t{workspace_count} workspaces\t{projects_dir}"
                )
        else:
            print("  No WSL distributions with Claude Code found")
        print()
    except (OSError, subprocess.SubprocessError) as e:
        sys.stderr.write(f"Error accessing WSL: {e}\n")


def _lsh_show_windows():
    """Show Windows users (when on WSL)."""
    print("Windows Users:")
    try:
        windows_users = get_windows_users_with_claude()

        if windows_users:
            for user in windows_users:
                print(f"  {user['username']}\t{user['path']}\t{user['workspace_count']} workspaces")
        else:
            print("  No Windows users with Claude Code found")
        print()
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing Windows: {e}\n")


def _lsh_show_remotes():
    """Show configured SSH remotes."""
    sources = get_saved_sources()
    print("SSH Remotes:")
    if sources:
        for source in sources:
            print(f"  {source}")
    else:
        print("  (none configured)")
    print()


def _get_lsh_display_flags(args) -> tuple:
    """Get flags for what to display in lsh command.

    Returns:
        Tuple of (show_local, show_wsl, show_windows, show_remotes)
    """
    no_filters = not args.wsl and not args.windows and not getattr(args, "remotes", False)
    show_local = args.local or (no_filters and not args.local)
    show_remotes = getattr(args, "remotes", False) or (
        not args.local and not args.wsl and not args.windows
    )
    return show_local, args.wsl, args.windows, show_remotes


def cmd_lsh(args):
    """List all home folders with Claude Code installations."""
    # Check for subcommands first
    lsh_action = getattr(args, "lsh_action", None)

    if lsh_action == "add":
        cmd_lsh_add(args)
        return
    if lsh_action == "remove":
        cmd_lsh_remove(args)
        return
    if lsh_action == "clear":
        cmd_lsh_clear(args)
        return

    # Default: list all hosts
    in_wsl = is_running_in_wsl()
    show_local, show_wsl, show_windows, show_remotes = _get_lsh_display_flags(args)

    if show_local:
        _lsh_show_local()
    if show_wsl and not in_wsl:
        _lsh_show_wsl()
    if show_windows and in_wsl:
        _lsh_show_windows()
    if show_remotes:
        _lsh_show_remotes()


def cmd_lsh_add(args):
    """Add a remote source to configuration."""
    source = args.source

    # WSL and Windows are auto-detected, no need to save them
    if source.startswith("wsl://") or source == "windows":
        sys.stderr.write(f"Note: {source} is auto-detected by --ah flag, no need to add it.\n")
        sys.stderr.write("This command is for SSH remotes (user@hostname).\n")
        return

    # Validate source format (SSH remote)
    if "@" not in source:
        sys.stderr.write(f"Error: Invalid source format: {source}\n")
        sys.stderr.write("Expected: user@hostname (e.g., alice@server.example.com)\n")
        sys.exit(1)

    config = load_config()
    sources = config.get("sources", [])

    if source in sources:
        print(f"Source '{source}' already configured.")
        return

    sources.append(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Added source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_remove(args):
    """Remove a remote source from configuration."""
    source = args.source

    config = load_config()
    sources = config.get("sources", [])

    if source not in sources:
        sys.stderr.write(f"Error: Source '{source}' not found.\n")
        sys.stderr.write(f"Configured sources: {', '.join(sources) if sources else '(none)'}\n")
        sys.exit(1)

    sources.remove(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Removed source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_clear(args):
    """Clear all saved remote sources."""
    config = load_config()
    config["sources"] = []

    if save_config(config):
        print("Cleared all SSH remotes.")
    else:
        sys.exit(1)


def _get_reset_items(what: str, config_dir: Path) -> list:
    """Get list of (description, path) tuples for reset operation."""
    items = []
    reset_all = what in (None, "all")

    if reset_all or what == "db":
        db_path = get_metrics_db_path()
        if db_path.exists():
            items.append(("Metrics database", db_path))
    if reset_all or what == "settings":
        config_file = config_dir / "config.json"
        if config_file.exists():
            items.append(("Settings (SSH remotes)", config_file))
    if reset_all or what == "aliases":
        aliases_file = config_dir / "aliases.json"
        if aliases_file.exists():
            items.append(("Aliases", aliases_file))

    return items


def _confirm_reset(items: list, yes: bool) -> bool:
    """Confirm reset operation. Returns True if confirmed."""
    print("This will delete:")
    for desc, path in items:
        print(f"  - {desc}: {path}")

    if yes:
        return True

    try:
        response = input("\nProceed? [y/N] ").strip().lower()
        return response in ("y", "yes")
    except (EOFError, KeyboardInterrupt):
        print("\nCancelled.")
        return False


def cmd_reset(args):
    """Reset agent-history data (database, settings, aliases)."""
    items = _get_reset_items(getattr(args, "what", None), get_config_dir())

    if not items:
        print("Nothing to reset.")
        return

    if not _confirm_reset(items, getattr(args, "yes", False)):
        print("Cancelled.")
        return

    print()
    for desc, path in items:
        path.unlink()
        print(f"Deleted {desc.lower()}")


def _gemini_index_list(full_hash: bool):
    """List all mappings in the Gemini hash index."""
    index = gemini_load_hash_index()
    mappings = index.get("hashes", {})

    if not mappings:
        print("Hash index is empty. Use 'gemini-index <path>' to add mappings.")
        return

    print(f"Hash Index Mappings ({len(mappings)} entries):\n")
    for project_hash, path in sorted(mappings.items(), key=lambda x: x[1]):
        if full_hash:
            print(f"  {project_hash}")
            print(f"    → {path}")
        else:
            print(f"  [hash:{project_hash[:HASH_DISPLAY_LEN]}] → {path}")


def _gemini_index_add(paths: list):
    """Add paths to the Gemini hash index."""
    # Convert to Path objects (allow non-existent paths - hash may still have sessions)
    path_objects = []
    for p in paths:
        path = Path(p).expanduser().resolve()
        path_objects.append(path)

    if not path_objects:
        sys.stderr.write("Error: No paths provided\n")
        sys.exit(1)

    print(f"Adding {len(path_objects)} path(s) to Gemini index...\n")

    result = gemini_add_paths_to_index(path_objects)

    # Report results for each path
    for mapping in result["mappings"]:
        status = mapping["status"]
        path = mapping["path"]
        hash_short = mapping["hash"]
        path_missing = mapping.get("path_missing", False)
        missing_note = " [path doesn't exist]" if path_missing else ""

        if status == "added":
            print(f"  ✅ {path}{missing_note}")
            print(f"     → [hash:{hash_short}] (added)")
        elif status == "existing":
            print(f"  ⏭️  {path}{missing_note}")
            print(f"     → [hash:{hash_short}] (already in index)")
        elif status == "no_sessions":
            print(f"  ❌ {path}{missing_note}")
            print(f"     → [hash:{hash_short}] (no Gemini sessions found)")

    # Summary
    print(
        f"\nSummary: {result['added']} added, {result['existing']} existing, "
        f"{result['no_sessions']} skipped"
    )

    # Show current index stats
    index = gemini_load_hash_index()
    print(f"Total mappings in index: {len(index['hashes'])}")


def cmd_gemini_index(args):
    """Add project paths to Gemini hash→path index, or list existing mappings."""
    list_index = getattr(args, "list_index", False)

    if list_index:
        _gemini_index_list(getattr(args, "full_hash", False))
    else:
        paths = getattr(args, "paths", None) or ["."]
        _gemini_index_add(paths)


def _filter_fetch_workspaces(workspaces: list, pattern: str) -> list:
    """Filter workspaces by pattern for fetch command."""
    if not pattern or pattern in ("", "*", "all"):
        return workspaces
    filtered = [ws for ws in workspaces if pattern in ws]
    if not filtered:
        sys.stderr.write(f"Error: No workspaces matching pattern '{pattern}'\n")
        sys.exit(1)
    return filtered


def _fetch_and_report(remote_host: str, workspaces: list, local_projects_dir: Path, hostname: str):
    """Fetch each workspace and report results."""
    for remote_workspace in workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        workspace_path = remote_workspace.lstrip("-")
        local_name = f"remote_{hostname}_{workspace_path}"

        result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)
        if result["success"]:
            print(normalize_workspace_name(local_name, verify_local=False))
        else:
            error = result.get("error", "Unknown error")
            sys.stderr.write(f"Error fetching {readable_name}: {error}\n")


def cmd_fetch(args):
    """Fetch remote conversation sessions via SSH."""
    remote_host = args.remote_host
    workspace_pattern = getattr(args, "pattern", None) or ""

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    hostname = get_remote_hostname(remote_host)
    remote_workspaces = list_remote_workspaces(remote_host)

    if not remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    remote_workspaces = _filter_fetch_workspaces(remote_workspaces, workspace_pattern)
    local_projects_dir = get_claude_projects_dir()
    _fetch_and_report(remote_host, remote_workspaces, local_projects_dir, hostname)


def _validate_local_access() -> list:
    """Validate local Claude projects access. Returns list of errors."""
    try:
        projects_dir = get_claude_projects_dir()
        if not projects_dir.exists():
            return [f"Local: Claude projects directory not found: {projects_dir}"]
    except (OSError, PermissionError) as e:
        return [f"Local: {e}"]
    return []


def _validate_windows_from_wsl() -> list:
    """Validate Windows access from WSL. Returns list of errors."""
    errors = []
    try:
        for user in get_windows_users_with_claude():
            try:
                projects_dir = get_windows_projects_dir(user["username"])
                if not projects_dir or not projects_dir.exists():
                    errors.append(
                        f"Windows ({user['username']}): Projects directory not accessible"
                    )
            except (OSError, PermissionError) as e:
                errors.append(f"Windows ({user['username']}): {e}")
    except (OSError, PermissionError) as e:
        errors.append(f"Windows: {e}")
    return errors


def _validate_wsl_from_windows() -> list:
    """Validate WSL access from Windows. Returns list of errors."""
    errors = []
    try:
        for distro in get_wsl_distributions():
            if not distro["has_claude"]:
                continue
            try:
                projects_dir = get_wsl_projects_dir(distro["name"])
                if not projects_dir or not projects_dir.exists():
                    errors.append(f"WSL ({distro['name']}): Projects directory not accessible")
            except (OSError, PermissionError) as e:
                errors.append(f"WSL ({distro['name']}): {e}")
    except (OSError, subprocess.SubprocessError) as e:
        errors.append(f"WSL: {e}")
    return errors


def _validate_ssh_remotes(args) -> list:
    """Validate SSH remote access. Returns list of errors."""
    if not hasattr(args, "remotes") or not args.remotes:
        return []
    errors = []
    for remote_host in args.remotes:
        if is_windows_remote(remote_host):
            continue
        if not check_ssh_connection(remote_host):
            errors.append(f"Remote ({remote_host}): Cannot connect via passwordless SSH")
    return errors


def validate_export_all_homes(args, in_wsl):
    """
    Validate all homes before starting export.
    Returns (success, errors) where errors is a list of error messages.
    """
    errors = _validate_local_access()

    if in_wsl:
        errors.extend(_validate_windows_from_wsl())
    else:
        errors.extend(_validate_wsl_from_windows())

    errors.extend(_validate_ssh_remotes(args))

    return (len(errors) == 0, errors)


def _export_local_sessions(args, output_dir: Path, patterns: list, local_label: str) -> dict:
    """Export local sessions and return result stats."""
    result = {"sessions": 0, "errors": 0, "source": None}
    print(f"[Local] Exporting from {local_label}...")
    try:
        config = ExportConfig.from_args(
            args,
            output_dir=str(output_dir),
            patterns=patterns,
            flat=False,  # Always organized for export-all
            remote=None,  # Local
            lenient=True,  # Don't fail on empty - export-all handles this
        )

        # Get local sessions count before export
        agent = getattr(args, "agent", None) or "auto"
        local_sessions = collect_sessions_with_dedup(patterns, include_cached=False, agent=agent)
        if local_sessions:
            cmd_batch(config)
            result["source"] = {"source": local_label, "sessions": len(local_sessions)}
            result["sessions"] = len(local_sessions)
            print(f"[OK] Local: {len(local_sessions)} sessions exported\n")
        else:
            print("[OK] Local: No matching sessions\n")
    except (OSError, json.JSONDecodeError) as e:
        print(f"[ERROR] Local export failed: {e}\n")
        result["errors"] = 1
    return result


def _export_windows_from_wsl(args, output_dir: Path, patterns: list) -> dict:
    """Export from Windows users when running in WSL."""
    result = {"sessions": 0, "errors": 0, "sources": []}
    print("[Windows] Checking Windows users with Claude...")
    try:
        windows_users = get_windows_users_with_claude()

        if windows_users:
            for user in windows_users:
                print(f"  Exporting from Windows: {user['username']}...")
                try:
                    username = user["username"]

                    # Count sessions first
                    windows_projects_dir = get_windows_projects_dir(username)
                    agent = getattr(args, "agent", None) or "auto"
                    native_sessions = collect_sessions_with_dedup(
                        patterns,
                        projects_dir=windows_projects_dir,
                        include_cached=False,
                        agent=agent,
                    )
                    session_count = len(native_sessions)

                    if session_count > 0:
                        config = ExportConfig.from_args(
                            args,
                            output_dir=str(output_dir),
                            patterns=patterns,
                            flat=False,  # Always organized
                            remote=f"windows://{username}",
                            lenient=True,
                        )
                        cmd_batch(config)
                        result["sources"].append(
                            {"source": f"Windows: {username}", "sessions": session_count}
                        )
                        result["sessions"] += session_count
                        print(f"  [OK] {username}: {session_count} sessions exported")
                    else:
                        print(f"  [OK] {username}: No matching sessions")
                except (OSError, json.JSONDecodeError) as e:
                    print(f"  [ERROR] {username} failed: {e}")
                    result["errors"] += 1
            print()
        else:
            print("  No Windows users with Claude Code found\n")
    except (OSError, PermissionError) as e:
        print(f"[ERROR] Windows export failed: {e}\n")
    return result


def _export_wsl_from_windows(args, output_dir: Path, patterns: list) -> dict:
    """Export from WSL distributions when running on Windows."""
    result = {"sessions": 0, "errors": 0, "sources": []}
    print("[WSL] Checking WSL distributions...")
    try:
        wsl_distros = get_wsl_distributions()
        claude_distros = [d for d in wsl_distros if d["has_claude"]]

        if claude_distros:
            for distro in claude_distros:
                print(f"  Exporting from WSL: {distro['name']}...")
                try:
                    distro_name = distro["name"]

                    # Count sessions first
                    wsl_projects_dir = get_wsl_projects_dir(distro["name"])
                    agent = getattr(args, "agent", None) or "auto"
                    wsl_sessions = collect_sessions_with_dedup(
                        patterns,
                        projects_dir=wsl_projects_dir,
                        include_cached=False,
                        agent=agent,
                    )
                    session_count = len(wsl_sessions)

                    if session_count > 0:
                        config = ExportConfig.from_args(
                            args,
                            output_dir=str(output_dir),
                            patterns=patterns,
                            flat=False,  # Always organized
                            remote=f"wsl://{distro_name}",
                            lenient=True,
                        )
                        cmd_batch(config)
                        result["sources"].append(
                            {"source": f"WSL: {distro['name']}", "sessions": session_count}
                        )
                        result["sessions"] += session_count
                        print(f"  [OK] {distro['name']}: {session_count} sessions exported")
                    else:
                        print(f"  [OK] {distro['name']}: No matching sessions")
                except (OSError, json.JSONDecodeError) as e:
                    print(f"  [ERROR] {distro['name']} failed: {e}")
                    result["errors"] += 1
            print()
        else:
            print("  No WSL distributions with Claude Code found\n")
    except (OSError, subprocess.SubprocessError) as e:
        print(f"[ERROR] WSL export failed: {e}\n")
    return result


def _export_remote_host(args, output_dir: Path, patterns: list, remote_host: str) -> dict:
    """Export from a single remote host."""
    result = {"sessions": 0, "errors": 0, "source": None}
    print(f"  Exporting from {remote_host}...")
    try:
        config = ExportConfig.from_args(
            args,
            output_dir=str(output_dir),
            patterns=patterns,
            flat=False,
            remote=remote_host,
            lenient=True,
        )

        # Export - with lenient=True, cmd_batch returns True even if no sessions
        cmd_batch(config)

        # Count sessions (cached)
        hostname = get_remote_hostname(remote_host)
        cached_pattern = f"remote_{hostname}_"
        # Filter by workspace patterns if specified
        cached_sessions = get_workspace_sessions(cached_pattern, quiet=True)
        if patterns:
            # Further filter by patterns
            filtered = []
            for s in cached_sessions:
                for p in patterns:
                    if p in s["workspace"] or p in s.get("workspace_readable", ""):
                        filtered.append(s)
                        break
            session_count = len(filtered)
        else:
            session_count = len(cached_sessions)

        if session_count > 0:
            result["source"] = {"source": f"Remote: {remote_host}", "sessions": session_count}
            result["sessions"] = session_count
            print(f"  [OK] {remote_host}: {session_count} sessions exported")
        else:
            print(f"  [OK] {remote_host}: No matching sessions")
    except (OSError, subprocess.SubprocessError, json.JSONDecodeError) as e:
        print(f"  [ERROR] {remote_host} failed: {e}")
        result["errors"] = 1
    return result


def _export_all_remotes(args, output_dir: Path, patterns: list) -> dict:
    """Export from all remote hosts."""
    result = {"sessions": 0, "errors": 0, "sources": []}
    if not hasattr(args, "remotes") or not args.remotes:
        return result

    print("[Remote] Exporting from remote hosts...")
    for remote_host in args.remotes:
        r = _export_remote_host(args, output_dir, patterns, remote_host)
        result["sessions"] += r["sessions"]
        result["errors"] += r["errors"]
        if r["source"]:
            result["sources"].append(r["source"])
    print()
    return result


def _print_export_all_summary(
    sources_processed: list, total_sessions: int, total_errors: int, output_dir: Path
):
    """Print summary of export-all operation."""
    print("=" * 80)
    print("Export-All Summary")
    print("=" * 80)
    for source_info in sources_processed:
        print(f"{source_info['source']:.<40} {source_info['sessions']:>5} sessions")
    print("-" * 80)
    print(f"Total sessions exported: {total_sessions}")
    if total_errors > 0:
        print(f"Errors encountered: {total_errors}")
    print(f"Output directory: {output_dir.absolute()}")
    print("=" * 80)


def _accumulate_export_results(totals: dict, result: dict, is_multi: bool = False):
    """Accumulate export results into totals dict."""
    if is_multi:
        totals["sources"].extend(result["sources"])
    elif result.get("source"):
        totals["sources"].append(result["source"])
    totals["sessions"] += result["sessions"]
    totals["errors"] += result["errors"]


def _get_export_all_patterns(args) -> list:
    """Get patterns list for export all command."""
    patterns = getattr(args, "patterns", None)
    if patterns is not None:
        return patterns
    ws = getattr(args, "workspace", "")
    return [ws] if ws else []


def cmd_export_all(args):
    """Export from all homes: local, WSL distributions (Windows), Windows (WSL), and optionally remotes."""
    in_wsl = is_running_in_wsl()
    local_label = "Local WSL" if in_wsl else "Local Windows"
    patterns = _get_export_all_patterns(args)

    # Include saved sources in remotes
    remotes = getattr(args, "remotes", []) or []
    for src in get_saved_sources():
        if src not in remotes:
            remotes.append(src)
    args.remotes = remotes

    # Pre-flight validation
    print("[Validation] Checking access to all homes...")
    valid, errors = validate_export_all_homes(args, in_wsl)
    if not valid:
        print("\nExport aborted: Cannot access all homes\n")
        for error in errors:
            print(error)
        print("\nPlease fix the issues above and try again.")
        sys.exit(1)
    print("All homes accessible\n")

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    totals = {"sources": [], "sessions": 0, "errors": 0}

    _accumulate_export_results(
        totals, _export_local_sessions(args, output_dir, patterns, local_label)
    )

    if in_wsl:
        _accumulate_export_results(
            totals, _export_windows_from_wsl(args, output_dir, patterns), is_multi=True
        )
    else:
        _accumulate_export_results(
            totals, _export_wsl_from_windows(args, output_dir, patterns), is_multi=True
        )

    _accumulate_export_results(
        totals, _export_all_remotes(args, output_dir, patterns), is_multi=True
    )

    _print_export_all_summary(totals["sources"], totals["sessions"], totals["errors"], output_dir)

    if getattr(args, "index", True):
        generate_index_manifest(output_dir, totals["sources"])


def _classify_session_source(filename: str) -> str:
    """Classify session source based on filename prefix."""
    if filename.startswith(CACHED_WSL_PREFIX):
        return "wsl"
    if filename.startswith(CACHED_WINDOWS_PREFIX):
        return "windows"
    if filename.startswith(CACHED_REMOTE_PREFIX):
        return "remote"
    return "local"


def _scan_workspace_directories(output_dir: Path) -> dict:
    """Scan workspace directories and group sessions by source."""
    workspaces = {}
    for item in output_dir.iterdir():
        if not item.is_dir():
            continue

        sessions = list(item.glob("*.md"))
        sources = {"local": 0, "wsl": 0, "windows": 0, "remote": 0}
        for session_file in sessions:
            sources[_classify_session_source(session_file.name)] += 1

        workspaces[item.name] = {"total": len(sessions), "sources": sources, "sessions": sessions}
    return workspaces


def _format_workspace_sources(sources: dict) -> list:
    """Format workspace sources as markdown lines."""
    lines = []
    for source, label in [
        ("local", "Local"),
        ("wsl", "WSL"),
        ("windows", "Windows"),
        ("remote", "Remote"),
    ]:
        if sources[source] > 0:
            lines.append(f"- **{label}:** {sources[source]} sessions")
    return lines


def generate_index_manifest(output_dir: Path, sources_info: list):
    """Generate index.md manifest file summarizing all exported sessions."""
    workspaces = _scan_workspace_directories(output_dir)

    lines = [
        "# Claude Conversation Export Index",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Total Workspaces:** {len(workspaces)}",
        f"**Total Sessions:** {sum(ws['total'] for ws in workspaces.values())}",
        "",
        "## Sources",
        "",
    ]

    for source_info in sources_info:
        lines.append(f"- **{source_info['source']}**: {source_info['sessions']} sessions")

    lines.extend(["", "## Workspaces", ""])

    for workspace_name in sorted(workspaces.keys()):
        ws_info = workspaces[workspace_name]
        lines.append(f"### {workspace_name} ({ws_info['total']} sessions)")
        lines.append("")
        lines.extend(_format_workspace_sources(ws_info["sources"]))
        lines.append("")

    index_path = output_dir / "index.md"
    index_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"\n[Index] Generated: {index_path}")


# ============================================================================
# Main
# ============================================================================

# Help text for argument parser
HELP_EPILOG = """
EXAMPLES:

  List workspaces:
    agent-history lsw                        # all local workspaces
    agent-history lsw myproject              # filter by pattern
    agent-history lsw -r user@server         # remote workspaces

  List sessions:
    agent-history lss                        # current workspace
    agent-history lss myproject              # specific workspace
    agent-history lss myproject -r user@server    # remote sessions

  Export (unified interface with orthogonal flags):
    agent-history export                     # current workspace, local home
    agent-history export --ah                # current workspace, all homes
    agent-history export --aw                # all workspaces, local home
    agent-history export --ah --aw           # all workspaces, all homes

    agent-history export myproject           # specific workspace, local
    agent-history export myproject --ah      # specific workspace, all homes
    agent-history export file.jsonl         # export single file

    agent-history export -o /tmp/backup      # current workspace, custom output
    agent-history export myproject -o ./out  # specific workspace, custom output

    agent-history export -r user@server      # current workspace, specific remote
    agent-history export --ah -r user@vm01   # current workspace, all homes + SSH

  Date filtering:
    agent-history lss myproject --since 2025-11-01
    agent-history export myproject --since 2025-11-01 --until 2025-11-30

  Export options:
    agent-history export myproject --minimal       # minimal mode
    agent-history export myproject --split 500     # split long conversations
    agent-history export myproject --flat          # flat structure (no subdirs)

  WSL access (Windows):
    agent-history lsh --wsl                        # list WSL distributions
    agent-history lsw --wsl                        # list WSL workspaces
    agent-history lsw --wsl Ubuntu                 # list from specific distro
    agent-history lss myproject --wsl              # list WSL sessions
    agent-history export myproject --wsl           # export from WSL

  Windows access (from WSL):
    agent-history lsh --windows                    # list Windows users with Claude
    agent-history lsw --windows                    # list Windows workspaces
    agent-history lss myproject --windows          # list Windows sessions
    agent-history export myproject --windows       # export from Windows

  Notes:
    - Outputs may show '[missing]' when a workspace directory no longer exists; the path
      is the closest match based on the stored workspace name.
"""


def _setup_windows_encoding():
    """Fix Unicode encoding for Windows console (emojis in output)."""
    if sys.platform == "win32":
        try:
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        except AttributeError:
            # Python < 3.7 doesn't have reconfigure
            import codecs

            sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
            sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")


def _resolve_remote_flag(args):
    """Convert --wsl and --windows flags to remote format.

    Args:
        args: Parsed arguments with wsl, windows, and remotes attributes

    Returns:
        Remote string (e.g., 'wsl://Ubuntu', 'windows', 'user@host') or None
    """
    remotes = getattr(args, "remotes", None)
    remote = remotes[0] if remotes else None
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)

    if wsl_flag:
        # If already in WSL, --wsl is a no-op (use local)
        if is_running_in_wsl():
            return None
        wsl_distros = get_wsl_distributions()
        if wsl_distros:
            claude_distros = [d for d in wsl_distros if d["has_claude"]]
            if claude_distros:
                return f"wsl://{claude_distros[0]['name']}"
        sys.stderr.write("Error: No WSL distributions with Claude Code found\n")
        sys.exit(1)

    if windows_flag:
        # If already on Windows, --windows is a no-op (use local)
        if platform.system() == "Windows":
            return None
        return "windows"

    return remote


def _get_wsl_remote_for_export() -> str:
    """Get WSL remote string for export. Returns None if in WSL or no Claude distros."""
    if is_running_in_wsl():
        return None
    wsl_distros = get_wsl_distributions()
    claude_distros = [d for d in wsl_distros if d.get("has_claude")]
    if claude_distros:
        return f"wsl://{claude_distros[0]['name']}"
    return None


def _resolve_remote_list(args):
    """Convert --wsl and --windows flags to remote list for export command."""
    remotes = list(getattr(args, "remotes", None) or [])

    if getattr(args, "wsl", False):
        wsl_remote = _get_wsl_remote_for_export()
        if wsl_remote:
            remotes.append(wsl_remote)

    if getattr(args, "windows", False) and platform.system() != "Windows":
        remotes.append("windows")

    return remotes if remotes else None


def _add_lsw_parser(subparsers):
    """Add lsw (list workspaces) subparser."""
    parser = subparsers.add_parser(
        "lsw", help="List workspaces", description="List all workspaces (local or remote)"
    )

    parser.add_argument(
        "pattern",
        nargs="*",
        help="Optional pattern(s) to filter workspaces (multiple patterns supported)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List workspaces from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="List workspaces from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="List workspaces from Windows (auto-detects user)"
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local workspaces (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        dest="all_homes",
        action="store_true",
        help="List workspaces from all homes (local + WSL/Windows + remotes)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_lss_parser(subparsers):
    """Add lss (list sessions) subparser."""
    parser = subparsers.add_parser(
        "lss",
        help="List sessions",
        description="List sessions in a workspace (defaults to current workspace)",
    )

    parser.add_argument(
        "workspace",
        nargs="*",
        help="Workspace name(s) (default: current workspace, multiple patterns supported)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List sessions from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="List sessions from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="List sessions from Windows (auto-detects user)"
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local sessions (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--since",
        metavar="DATE",
        help="Only include sessions modified on or after this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--until",
        metavar="DATE",
        help="Only include sessions modified on or before this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        dest="all_homes",
        action="store_true",
        help="List sessions from all homes (local + WSL/Windows + remotes)",
    )
    parser.add_argument(
        "--alias",
        metavar="NAME",
        help="Use alias instead of workspace pattern (alternative to @name syntax)",
    )
    parser.add_argument(
        "--aw",
        "--all-workspaces",
        dest="all_workspaces",
        action="store_true",
        help="Include all workspaces for the selected homes (default: current workspace only)",
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_lsh_parser(subparsers):
    """Add lsh (list homes) subparser."""
    parser = subparsers.add_parser(
        "lsh",
        help="List homes and manage SSH remotes",
        description="List all Claude Code installations (local, WSL, Windows, SSH remotes)",
    )

    lsh_subparsers = parser.add_subparsers(dest="lsh_action")

    # lsh add
    parser_add = lsh_subparsers.add_parser(
        "add", help="Add an SSH remote", description="Add an SSH remote (user@hostname)"
    )
    parser_add.add_argument("source", help="SSH remote (user@hostname)")

    # lsh remove
    parser_remove = lsh_subparsers.add_parser(
        "remove", help="Remove an SSH remote", description="Remove an SSH remote from configuration"
    )
    parser_remove.add_argument("source", help="SSH remote to remove")

    # lsh clear
    lsh_subparsers.add_parser(
        "clear", help="Clear all SSH remotes", description="Remove all saved SSH remotes"
    )

    # Filter flags for listing (default action)
    parser.add_argument("--wsl", action="store_true", help="Show WSL distributions only")
    parser.add_argument("--windows", action="store_true", help="Show Windows users only")
    parser.add_argument("--local", action="store_true", help="Show local home folder only")
    parser.add_argument("--remotes", action="store_true", help="Show SSH remotes only")
    return parser


def _add_export_parser(subparsers):
    """Add export subparser."""
    parser = subparsers.add_parser(
        "export",
        help="Export to markdown",
        description="Export workspace or single file to markdown",
    )

    parser.add_argument(
        "target",
        nargs="*",
        default=[],
        help="Workspace pattern(s) or file.jsonl (optional, multiple patterns supported)",
    )
    parser.add_argument(
        "output_dir",
        nargs="?",
        default="./claude-conversations",
        help="Output directory (default: ./claude-conversations)",
    )
    parser.add_argument(
        "-o",
        "--output",
        metavar="DIR",
        dest="output_override",
        help="Output directory (overrides positional output_dir)",
    )
    parser.add_argument(
        "-a",
        "--all",
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Export all workspaces (default: current workspace)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Export from all homes: local + WSL + Windows + remotes (default: local only)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="Export from remote host via SSH (user@hostname) - can be used multiple times",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="Export from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="Export from Windows (auto-detects user)"
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--force", action="store_true", help="Force re-export (default: incremental)"
    )
    parser.add_argument(
        "--minimal",
        action="store_true",
        help="Minimal export: omit metadata, keep only conversation content",
    )
    parser.add_argument(
        "--split",
        metavar="LINES",
        type=validate_split_lines,
        help="Split long conversations into parts (e.g., --split 500)",
    )
    parser.add_argument(
        "--since",
        metavar="DATE",
        help="Only include sessions modified on or after this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--until",
        metavar="DATE",
        help="Only include sessions modified on or before this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--flat",
        action="store_true",
        help="Use flat directory structure (default: organized by workspace with source tags)",
    )
    parser.add_argument(
        "--alias", metavar="NAME", help="Export using alias (alternative to @name syntax)"
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_alias_parser(subparsers):
    """Add alias subparser."""
    parser = subparsers.add_parser(
        "alias",
        help="Manage workspace aliases",
        description="Create and manage workspace aliases for consolidated exports",
    )

    alias_subparsers = parser.add_subparsers(dest="alias_command", help="Alias subcommand")

    # alias list
    alias_subparsers.add_parser(
        "list",
        help="List all aliases",
        description="List all aliases with their workspaces and session counts",
    )

    # alias show
    parser_show = alias_subparsers.add_parser(
        "show", help="Show alias details", description="Show details of a single alias"
    )
    parser_show.add_argument("name", help="Alias name")

    # alias create
    parser_create = alias_subparsers.add_parser(
        "create", help="Create a new alias", description="Create a new empty alias"
    )
    parser_create.add_argument("name", help="Alias name")

    # alias delete
    parser_delete = alias_subparsers.add_parser(
        "delete",
        help="Delete an alias",
        description="Delete an alias and all its workspace associations",
    )
    parser_delete.add_argument("name", help="Alias name")

    # alias add
    parser_add = alias_subparsers.add_parser(
        "add",
        help="Add workspace(s) to alias",
        description="Add one or more workspaces to an alias",
    )
    parser_add.add_argument(
        "name",
        nargs="?",
        default="",
        help="Alias name (default: auto-detect from current directory)",
    )
    parser_add.add_argument(
        "workspaces", nargs="*", help="Workspace names to add (encoded directory names)"
    )
    parser_add.add_argument("--pick", action="store_true", help="Interactive workspace picker")
    parser_add.add_argument(
        "-r", "--remote", metavar="HOST", help="Add workspace from remote host (user@hostname)"
    )
    parser_add.add_argument(
        "--wsl", action="store_true", help="Add workspace from WSL (auto-detects distribution)"
    )
    parser_add.add_argument(
        "--windows", action="store_true", help="Add workspace from Windows (auto-detects user)"
    )
    parser_add.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Add from all homes (local + WSL/Windows + remotes)",
    )

    # alias remove
    parser_remove = alias_subparsers.add_parser(
        "remove", help="Remove workspace from alias", description="Remove a workspace from an alias"
    )
    parser_remove.add_argument("name", help="Alias name")
    parser_remove.add_argument(
        "workspace", help="Workspace name to remove (encoded directory name)"
    )
    parser_remove.add_argument(
        "-r", "--remote", metavar="HOST", help="Remove workspace from remote source"
    )
    parser_remove.add_argument(
        "--wsl", action="store_true", help="Remove workspace from WSL source"
    )
    parser_remove.add_argument(
        "--windows", action="store_true", help="Remove workspace from Windows source"
    )

    # alias export
    alias_subparsers.add_parser(
        "export",
        help="Export aliases to JSON",
        description="Export aliases configuration to JSON (stdout)",
    )

    # alias import
    parser_import = alias_subparsers.add_parser(
        "import",
        help="Import aliases from JSON",
        description="Import aliases from JSON file or stdin",
    )
    parser_import.add_argument("file", nargs="?", help="JSON file to import (default: stdin)")
    parser_import.add_argument(
        "--replace", action="store_true", help="Replace all aliases instead of merging"
    )

    return parser


def _add_stats_parser(subparsers):
    """Add stats subparser."""
    parser = subparsers.add_parser(
        "stats",
        help="Show usage statistics and metrics",
        description="Display Claude Code usage statistics from synced metrics database",
    )

    parser.add_argument(
        "workspace", nargs="*", default=None, help="Filter by workspace pattern(s) or @alias"
    )
    parser.add_argument("--sync", action="store_true", help="Sync JSONL files to metrics database")
    parser.add_argument(
        "--force", action="store_true", help="Force re-sync all files (ignore mtime)"
    )
    parser.add_argument("--tools", action="store_true", help="Show tool usage statistics")
    parser.add_argument("--models", action="store_true", help="Show model usage statistics")
    parser.add_argument(
        "--time", action="store_true", help="Show time tracking with daily breakdown"
    )
    parser.add_argument(
        "--by-workspace", action="store_true", help="Show statistics grouped by workspace"
    )
    parser.add_argument("--by-day", action="store_true", help="Show daily statistics")
    parser.add_argument(
        "--source",
        metavar="SOURCE",
        help="Filter by source (local, wsl:distro, windows, remote:host)",
    )
    parser.add_argument(
        "--since", metavar="DATE", help="Filter sessions starting from this date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--until", metavar="DATE", help="Filter sessions until this date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Show stats for all workspaces (default: current workspace)",
    )
    parser.add_argument(
        "--top-ws",
        type=int,
        default=None,
        help="Limit the number of workspaces shown per home (default: all)",
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Sync from all homes (local + WSL/Windows)",
    )
    parser.add_argument("--wsl", action="store_true", help="Include WSL source")
    parser.add_argument("--windows", action="store_true", help="Include Windows source")
    parser.add_argument(
        "-r",
        "--remote",
        action="append",
        dest="remotes",
        metavar="HOST",
        help="Include remote source (can be repeated)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_reset_parser(subparsers):
    """Add reset subparser."""
    parser = subparsers.add_parser(
        "reset",
        help="Reset stored data (database, settings, aliases)",
        description="Delete metrics database, settings, and/or aliases.",
    )

    parser.add_argument(
        "what",
        nargs="?",
        choices=["db", "settings", "aliases", "all"],
        default="all",
        help="What to reset: db (metrics), settings (SSH remotes), aliases, or all (default: all)",
    )
    parser.add_argument("-y", "--yes", action="store_true", help="Skip confirmation prompt")
    return parser


def _create_argument_parser():
    """Create and configure the argument parser.

    Returns:
        Configured argparse.ArgumentParser instance
    """
    parser = argparse.ArgumentParser(
        prog="agent-history",
        description="Browse and export AI coding assistant conversation history (Claude Code, Codex CLI)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=HELP_EPILOG,
    )

    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    # Global agent selection flag (before subcommand)
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default="auto",
        help="Agent backend to use (default: auto-detect based on available data)",
    )

    # Create subparsers for commands
    subparsers = parser.add_subparsers(dest="command", help="Command to execute", required=False)

    # Add all command parsers
    _add_lsw_parser(subparsers)
    _add_lss_parser(subparsers)
    _add_lsh_parser(subparsers)
    _add_export_parser(subparsers)
    _add_alias_parser(subparsers)
    _add_stats_parser(subparsers)
    _add_reset_parser(subparsers)
    _add_install_parser(subparsers)
    _add_gemini_index_parser(subparsers)

    return parser


def _add_install_parser(subparsers):
    """Add install subparser."""
    parser = subparsers.add_parser(
        "install",
        help="Install CLI and Claude skill",
        description="Install the agent-history CLI to ~/.local/bin (default) and copy the skill files for Claude Code.",
    )

    parser.add_argument(
        "--bin-dir",
        default=str(DEFAULT_BIN_DIR),
        help="Destination directory for the CLI binary (default: ~/.local/bin)",
    )
    parser.add_argument(
        "--cli-name",
        default=DEFAULT_CLI_NAME,
        help="Binary name to use for the CLI install (default: agent-history)",
    )
    parser.add_argument(
        "--skill-dir",
        default=str(DEFAULT_SKILL_DIR),
        help="Destination directory for the Claude skill files (default: ~/.claude/skills/agent-history)",
    )
    parser.add_argument(
        "--skill-name",
        default=DEFAULT_SKILL_NAME,
        help="Binary name to use inside the Claude skill directory (default: agent-history)",
    )
    parser.add_argument(
        "--skip-cli",
        action="store_true",
        help="Skip installing the CLI binary (only update skill/settings)",
    )
    parser.add_argument(
        "--skip-skill",
        action="store_true",
        help="Skip installing the Claude skill files",
    )
    parser.add_argument(
        "--skip-settings",
        action="store_true",
        help="Skip updating ~/.claude/settings.json",
    )
    return parser


def _add_gemini_index_parser(subparsers):
    """Add gemini-index subparser."""
    parser = subparsers.add_parser(
        "gemini-index",
        help="Add project paths to Gemini hash→path index",
        description=(
            "Add project directory paths to the Gemini hash-to-path index. "
            "For each path, computes its SHA-256 hash and checks if Gemini "
            "has sessions for that project. If sessions exist, adds the mapping "
            "so agent-history can display readable workspace paths instead of hashes."
        ),
    )

    parser.add_argument(
        "paths",
        nargs="*",
        default=[],
        help="Project directories to add (default: current directory if no --list)",
    )
    parser.add_argument(
        "--list",
        "-l",
        action="store_true",
        dest="list_index",
        help="List all mappings in the hash index",
    )
    parser.add_argument(
        "--full-hash",
        action="store_true",
        help="Show full SHA-256 hashes instead of truncated (with --list)",
    )
    return parser


# ============================================================================
# Command Dispatch
# ============================================================================


def _get_remote_workspaces_for_lsw(remote: str, patterns: list) -> tuple:
    """Get filtered workspaces from a remote for lsw.

    Returns:
        Tuple of (hostname, sessions_list) or (None, []) on error
    """
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Cannot connect to remote: {remote}\n")
        return None, []

    hostname = get_remote_hostname(remote)
    remote_workspaces = list_remote_workspaces(remote)

    # Filter by patterns
    if patterns and patterns != [""]:
        remote_workspaces = [ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)]

    # Create session-like dicts for output
    sessions = [
        {"workspace": ws, "workspace_readable": normalize_workspace_name(ws)}
        for ws in remote_workspaces
    ]
    return hostname, sessions


def _print_lsw_results(all_results: list, workspaces_only: bool):
    """Print lsw results in the appropriate format."""
    for source_label, sessions in all_results:
        if workspaces_only:
            print(f"# {source_label}")
            seen_workspaces = set()
            for session in sessions:
                ws = session["workspace_readable"]
                # On Windows, only show paths that actually exist to avoid stale/invalid entries
                if sys.platform == "win32":
                    try:
                        p = Path(ws)
                        # Only filter for Windows and WSL sources
                        if (
                            source_label.startswith("Windows") or source_label.startswith("WSL ")
                        ) and not p.exists():
                            continue
                    except Exception:
                        pass
                if ws not in seen_workspaces:
                    seen_workspaces.add(ws)
                    print(f"  {ws}")
            print()
        else:
            for session in sessions:
                print(format_session_line(session, source_label))


def _dispatch_lsw_additive(args):
    """Handle --local with -r for additive lsw (local + remotes).

    Args:
        args: Arguments with patterns, remotes, and local_only flag
    """
    patterns = getattr(args, "patterns", [""])
    remotes = getattr(args, "remotes", [])
    workspaces_only = getattr(args, "workspaces_only", False)
    agent = getattr(args, "agent", None) or "auto"

    all_results = []

    # 1. Get local workspaces
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    try:
        local_sessions = collect_sessions_with_dedup(patterns, include_cached=False, agent=agent)
        if local_sessions:
            all_results.append((local_label, local_sessions))
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")

    # 2. Get from SSH remotes
    for remote in remotes:
        try:
            hostname, sessions = _get_remote_workspaces_for_lsw(remote, patterns)
            if hostname and sessions:
                all_results.append((f"Remote ({hostname})", sessions))
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")

    _print_lsw_results(all_results, workspaces_only)


def _filter_sessions_by_date(sessions: list, since_date, until_date) -> list:
    """Filter sessions by date range."""
    if not since_date and not until_date:
        return sessions
    filtered = []
    for session in sessions:
        mtime = session.get("modified")
        if mtime:
            mdate = mtime.replace(hour=0, minute=0, second=0, microsecond=0)
            if since_date and mdate < since_date:
                continue
            if until_date and mdate > until_date:
                continue
        filtered.append(session)
    return filtered


def _get_remote_sessions_for_additive(remote: str, patterns: list, since_date, until_date) -> tuple:
    """Get sessions from a single remote for additive mode.

    Returns:
        Tuple of (hostname, sessions_list) or (None, []) on error
    """
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Cannot connect to remote: {remote}\n")
        return None, []

    hostname = get_remote_hostname(remote)
    remote_workspaces = list_remote_workspaces(remote)

    # Filter workspaces by patterns
    if patterns and patterns != [""]:
        remote_workspaces = [ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)]

    # Get sessions for each workspace
    all_sessions = []
    for ws in remote_workspaces:
        remote_sessions = get_remote_session_info(remote, ws)
        if remote_sessions:
            ws_readable = normalize_workspace_name(ws)
            for session in remote_sessions:
                session["workspace_readable"] = ws_readable
            remote_sessions = _filter_sessions_by_date(remote_sessions, since_date, until_date)
            all_sessions.extend(remote_sessions)

    return hostname, all_sessions


def _collect_local_sessions_for_additive(
    patterns: list, since_date, until_date, agent: str = "auto"
) -> tuple:
    """Collect local sessions for additive mode. Returns (label, sessions) or None."""
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    try:
        sessions = collect_sessions_with_dedup(
            patterns,
            since_date=since_date,
            until_date=until_date,
            include_cached=False,
            agent=agent,
        )
        if sessions:
            return (local_label, sessions)
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")
    return None


def _collect_remotes_for_additive(remotes: list, patterns: list, since_date, until_date) -> list:
    """Collect sessions from SSH remotes for additive mode."""
    results = []
    for remote in remotes:
        try:
            hostname, sessions = _get_remote_sessions_for_additive(
                remote, patterns, since_date, until_date
            )
            if hostname and sessions:
                results.append((f"Remote ({hostname})", sessions))
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")
    return results


def _dispatch_lss_additive(args):
    """Handle --local with -r for additive lss (local + remotes)."""
    patterns = getattr(args, "patterns", [""])
    remotes = getattr(args, "remotes", [])
    since_date = getattr(args, "since_date", None)
    until_date = getattr(args, "until_date", None)
    agent = getattr(args, "agent", None) or "auto"

    all_results = []
    local_result = _collect_local_sessions_for_additive(patterns, since_date, until_date, agent)
    if local_result:
        all_results.append(local_result)
    all_results.extend(_collect_remotes_for_additive(remotes, patterns, since_date, until_date))

    if all_results:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for source_label, sessions in all_results:
            for session in sessions:
                print(format_session_line(session, source_label))


def _dispatch_lsw(args):
    """Dispatch lsw (list workspaces) command."""
    patterns = args.pattern if args.pattern else [""]
    local_flag = getattr(args, "local", False)
    remotes = getattr(args, "remotes", None) or []
    agent = getattr(args, "agent", None) or "auto"

    if getattr(args, "all_homes", False):

        class LswAllArgs:
            workspaces_only = True
            since_date = None
            until_date = None

        LswAllArgs.remotes = remotes
        LswAllArgs.patterns = patterns
        LswAllArgs.agent = agent
        cmd_list_all_homes(LswAllArgs())
    elif local_flag and remotes:
        # --local with -r: show both local and remotes (additive)
        class LswAdditiveArgs:
            workspaces_only = True
            since_date = None
            until_date = None

        LswAdditiveArgs.remotes = remotes
        LswAdditiveArgs.patterns = patterns
        LswAdditiveArgs.local_only = True  # Skip Windows/WSL scanning
        LswAdditiveArgs.agent = agent
        _dispatch_lsw_additive(LswAdditiveArgs())
    else:

        class LswArgs:
            remote = _resolve_remote_flag(args)
            workspaces_only = True

        LswArgs.patterns = patterns
        LswArgs.workspace = patterns[0] if patterns and patterns[0] else ""
        LswArgs.agent = agent
        cmd_list(LswArgs())


def _dispatch_lss_to_alias(alias_name: str, args):
    """Handle lss dispatch to alias command."""
    since_date = parse_date_string(args.since) if args.since else None
    until_date = parse_date_string(args.until) if args.until else None
    agent = getattr(args, "agent", None) or "auto"
    cmd_alias_lss(alias_name, since_date=since_date, until_date=until_date, agent=agent)


def _dispatch_lss_local_additive(args, workspace_list: list, remotes: list):
    """Handle lss with --local and -r flags (additive mode)."""
    since_date = parse_date_string(args.since) if args.since else None
    until_date = parse_date_string(args.until) if args.until else None
    patterns, _ = resolve_patterns_for_command(workspace_list)

    class LssAdditiveArgs:
        workspaces_only = False

    LssAdditiveArgs.patterns = patterns
    LssAdditiveArgs.since_date = since_date
    LssAdditiveArgs.until_date = until_date
    LssAdditiveArgs.remotes = remotes
    LssAdditiveArgs.local_only = True
    LssAdditiveArgs.agent = getattr(args, "agent", None) or "auto"
    _dispatch_lss_additive(LssAdditiveArgs())


def _dispatch_lss_all_homes(args, workspace_list: list):
    """Handle lss with --all-homes flag."""
    this_only = getattr(args, "this_only", False)
    since_date = parse_date_string(args.since) if args.since else None
    until_date = parse_date_string(args.until) if args.until else None
    lss_remotes = getattr(args, "remotes", None) or []
    agent = getattr(args, "agent", None) or "auto"

    patterns, alias_for_ws = resolve_patterns_for_command(workspace_list, this_only)
    if alias_for_ws:
        sys.stderr.write(f"Using alias @{alias_for_ws} (use --this for current workspace only)\n")
        cmd_alias_lss(alias_for_ws, since_date=since_date, until_date=until_date, agent=agent)
        return

    class LssAllArgs:
        workspaces_only = False

    LssAllArgs.patterns = patterns
    LssAllArgs.since_date = since_date
    LssAllArgs.until_date = until_date
    LssAllArgs.remotes = lss_remotes
    LssAllArgs.agent = getattr(args, "agent", None) or "auto"
    cmd_list_all_homes(LssAllArgs())


def _dispatch_lss_local(args, workspace_list: list):
    """Handle lss for local sessions only."""
    projects_dir_override = getattr(args, "projects_dir_override", None)

    this_only = getattr(args, "this_only", False)
    all_workspaces_flag = getattr(args, "all_workspaces", False)
    remote_value = _resolve_remote_flag(args)
    needs_remote_scope = bool(
        remote_value or getattr(args, "windows", False) or getattr(args, "wsl", False)
    )
    error_msg = (
        "Not in a Claude Code workspace.\n\n"
        "To list sessions, either:\n"
        "  • Run from within a workspace directory\n"
        "  • Specify a workspace pattern: lss <pattern>\n"
        "  • Use --ah to list from all homes: lss --ah"
    )
    if all_workspaces_flag and not workspace_list:
        patterns = [""]
        alias_for_ws = None
    else:
        require_ws = not needs_remote_scope
        err = error_msg if require_ws else None
        patterns, alias_for_ws = resolve_patterns_for_command(
            workspace_list, this_only, require_workspace=require_ws, error_message=err
        )
    if alias_for_ws:
        sys.stderr.write(f"Using alias @{alias_for_ws} (use --this for current workspace only)\n")
        _dispatch_lss_to_alias(alias_for_ws, args)
        return

    class LssArgs:
        since = args.since
        until = args.until
        remote = remote_value
        workspaces_only = False
        projects_dir = projects_dir_override
        agent = getattr(args, "agent", None) or "auto"

    LssArgs.patterns = patterns
    cmd_list(LssArgs())


def _detect_projects_dir_override(workspace_inputs: list[str]) -> tuple[Path | None, bool]:
    """Return (projects_dir_override, saw_wsl_unc_input)."""
    projects_dir_override = None
    wsl_unc_in_input = False
    for raw in workspace_inputs:
        normalized = raw.replace("\\", "/")
        lower_norm = normalized.lower()
        if lower_norm.startswith("//wsl.localhost/") or lower_norm.startswith("//wsl$/"):
            projects_dir_override = _projects_dir_from_wsl_unc(normalized)
            wsl_unc_in_input = True
            break
        if projects_dir_override is None:
            try:
                path_obj = Path(raw)
                if path_obj.is_absolute():
                    parent = path_obj.parent
                    if parent.exists():
                        projects_dir_override = parent
            except OSError:
                continue
    return projects_dir_override, wsl_unc_in_input


def _ensure_workspace_default_for_remote(args, workspace_list: list[str]) -> list[str]:
    """Default to all workspaces when remote flags are set without explicit patterns."""
    if workspace_list:
        return workspace_list

    if getattr(args, "all_workspaces", False):
        return [""]

    selected_remote = None
    try:
        selected_remote = _resolve_remote_flag(args)
    except SystemExit:
        selected_remote = None

    windows_flag = getattr(args, "windows", False)
    wsl_flag = getattr(args, "wsl", False)

    if selected_remote or windows_flag or wsl_flag:
        # Keep list empty so downstream defaults to current workspace/alias
        return workspace_list

    return workspace_list


def _apply_wsl_unc_override(args, saw_wsl_unc: bool):
    """Force local mode when user passed a WSL UNC path."""
    if saw_wsl_unc and not getattr(args, "wsl", False):
        args.wsl = False
        args.remotes = []


def _dispatch_lss(args):
    """Dispatch lss (list sessions) command."""
    workspace_inputs = args.workspace if args.workspace else []
    projects_dir_override, wsl_unc_in_input = _detect_projects_dir_override(workspace_inputs)
    workspace_list = [_coerce_target_to_workspace_pattern(w) for w in workspace_inputs]
    workspace_list = _ensure_workspace_default_for_remote(args, workspace_list)
    _apply_wsl_unc_override(args, wsl_unc_in_input)
    args.projects_dir_override = projects_dir_override

    local_flag = getattr(args, "local", False)
    remotes = getattr(args, "remotes", None) or []

    # Check for alias (explicit flag or @ prefix)
    alias_name = detect_alias_from_args(args, workspace_list)
    if alias_name:
        _dispatch_lss_to_alias(alias_name, args)
        return

    # Handle --local with -r (additive mode)
    if local_flag and remotes:
        _dispatch_lss_local_additive(args, workspace_list, remotes)
        return

    # Handle --all-homes
    if getattr(args, "all_homes", False):
        _dispatch_lss_all_homes(args, workspace_list)
    else:
        _dispatch_lss_local(args, workspace_list)


def _get_export_output_dir(args) -> str:
    """Get the final output directory from args."""
    if hasattr(args, "output_override") and args.output_override:
        return args.output_override
    return args.output_dir


def _build_export_config(args, output_dir: str, patterns: list, remote=None) -> ExportConfig:
    """Build ExportConfig from args."""
    return ExportConfig(
        output_dir=output_dir,
        patterns=patterns,
        since=args.since,
        until=args.until,
        force=args.force,
        minimal=args.minimal,
        split=args.split,
        flat=args.flat,
        remote=remote,
        lenient=False,
        agent=getattr(args, "agent", None) or "auto",
    )


def _dispatch_export_single_file(args, jsonl_file: str):
    """Handle export of a single JSONL file."""
    final_remotes = _resolve_remote_list(args)
    final_remote = final_remotes[0] if final_remotes else None

    class ConvertArgs:
        output = None
        remote = final_remote

    ConvertArgs.jsonl_file = jsonl_file
    cmd_convert(ConvertArgs())


def _dispatch_export_additive(args, output_dir: str, patterns: list, remotes: list):
    """Handle export with --local and -r flags (additive mode)."""
    # Export from local first
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    sys.stderr.write(f"# {local_label}\n")
    cmd_batch(_build_export_config(args, output_dir, patterns, remote=None))

    # Export from each remote
    for remote in remotes:
        hostname = get_remote_hostname(remote)
        sys.stderr.write(f"\n# Remote ({hostname})\n")
        cmd_batch(_build_export_config(args, output_dir, patterns, remote=remote))


def _dispatch_export_all_homes(args, output_dir: str, patterns: list, remotes: list):
    """Handle export with --all-homes flag."""

    class ExportAllArgs:
        workspace = ""

    ExportAllArgs.patterns = patterns
    ExportAllArgs.output_dir = output_dir
    ExportAllArgs.remotes = remotes
    ExportAllArgs.since = args.since
    ExportAllArgs.until = args.until
    ExportAllArgs.force = args.force
    ExportAllArgs.minimal = args.minimal
    ExportAllArgs.split = args.split
    ExportAllArgs.agent = getattr(args, "agent", None) or "auto"
    cmd_export_all(ExportAllArgs())


def _resolve_export_targets(args, targets: list, output_dir: Path) -> tuple:
    """Resolve export targets, handling workspace detection and aliases.

    Returns:
        Tuple of (resolved_targets, alias_handled) where alias_handled is True if exported via alias
    """
    if targets or args.all_workspaces:
        return targets, False

    this_only = getattr(args, "this_only", False)
    pattern, exists = check_current_workspace_exists()

    if not exists:
        if args.all_homes:
            args.all_workspaces = True
            return [], False
        exit_with_error(
            "Not in a Claude Code workspace.\n\n"
            "To export sessions, either:\n"
            "  • Run from within a workspace directory\n"
            "  • Specify a workspace pattern: export <pattern>\n"
            "  • Use --aw to export all workspaces: export --aw\n"
            "  • Use --ah to export from all homes: export --ah"
        )

    if not this_only:
        alias_for_ws = get_alias_for_workspace(pattern, "local")
        if alias_for_ws:
            sys.stderr.write(
                f"Using alias @{alias_for_ws} (use --this for current workspace only)\n"
            )
            cmd_alias_export(alias_for_ws, output_dir, args)
            return [], True

    return [pattern], False


def _route_export(args, output_dir: Path, final_patterns: list, final_remotes):
    """Route export to appropriate handler based on flags."""
    local_flag = getattr(args, "local", False)
    if local_flag and final_remotes:
        _dispatch_export_additive(args, output_dir, final_patterns, final_remotes)
    elif args.all_homes:
        _dispatch_export_all_homes(args, output_dir, final_patterns, final_remotes)
    else:
        final_remote = final_remotes[0] if final_remotes else None
        if not cmd_batch(_build_export_config(args, output_dir, final_patterns, final_remote)):
            sys.exit(1)


def _dispatch_export(args):
    """Dispatch export command."""
    output_dir = _get_export_output_dir(args)
    targets = args.target if args.target else []
    targets = [_coerce_target_to_workspace_pattern(t) for t in targets]

    alias_name = detect_alias_from_args(args, targets)
    if alias_name:
        cmd_alias_export(alias_name, output_dir, args)
        return

    targets, alias_handled = _resolve_export_targets(args, targets, output_dir)
    if alias_handled:
        return

    if len(targets) == 1 and targets[0].endswith(".jsonl"):
        _dispatch_export_single_file(args, targets[0])
        return

    final_remotes = _resolve_remote_list(args)
    final_patterns = [] if args.all_workspaces else targets
    _route_export(args, output_dir, final_patterns, final_remotes)


def _dispatch_alias(args):
    """Dispatch alias command."""
    alias_cmd = getattr(args, "alias_command", None)

    if alias_cmd == "list" or alias_cmd is None:
        cmd_alias_list(args)
    elif alias_cmd == "show":
        cmd_alias_show(args)
    elif alias_cmd == "create":
        cmd_alias_create(args)
    elif alias_cmd == "delete":
        cmd_alias_delete(args)
    elif alias_cmd == "add":
        cmd_alias_add(args)
    elif alias_cmd == "remove":
        cmd_alias_remove(args)
    elif alias_cmd == "export":
        cmd_alias_config_export(args)
    elif alias_cmd == "import":
        cmd_alias_config_import(args)


def _get_stats_remotes(args) -> list:
    """Get remotes list for stats command, adding WSL/Windows if flagged."""
    remotes = getattr(args, "remotes", None) or []
    if args.wsl:
        wsl_distros = get_wsl_distributions()
        claude_distros = [d for d in wsl_distros if d.get("has_claude")]
        if claude_distros:
            remotes.append(f"wsl://{claude_distros[0]['name']}")
    if args.windows:
        remotes.append("windows")
    return remotes


def _build_sync_args(args, remotes: list):
    """Build SyncArgs class for cmd_stats_sync."""

    class SyncArgs:
        force = args.force
        all_homes = getattr(args, "all_homes", False)
        patterns = args.workspace if args.workspace else [""]

    SyncArgs.remotes = remotes
    return SyncArgs()


def _build_stats_args(args):
    """Build StatsArgs class for cmd_stats."""

    class StatsArgs:
        workspace = args.workspace
        source = args.source
        since = args.since
        until = args.until
        tools = args.tools
        models = args.models
        time = getattr(args, "time", False)
        by_workspace = getattr(args, "by_workspace", False)
        by_day = getattr(args, "by_day", False)
        all_workspaces = getattr(args, "all_workspaces", False)
        top_ws = getattr(args, "top_ws", None)
        this_only = getattr(args, "this_only", False)

    return StatsArgs()


def _dispatch_stats(args):
    """Dispatch stats command."""
    remotes = _get_stats_remotes(args)

    if args.sync:
        cmd_stats_sync(_build_sync_args(args, remotes))
        return

    if args.all_homes:
        cmd_stats_sync(_build_sync_args(args, remotes))
        print()

    cmd_stats(_build_stats_args(args))


def _dir_on_path(directory: Path) -> bool:
    """Return True if directory is on PATH."""
    path_env = os.environ.get("PATH", "")
    if not path_env:
        return False
    try:
        directory = directory.expanduser().resolve()
    except (OSError, RuntimeError):
        directory = directory.expanduser()
    for entry in path_env.split(os.pathsep):
        if not entry:
            continue
        try:
            entry_path = Path(entry).expanduser().resolve()
        except (OSError, RuntimeError):
            entry_path = Path(entry).expanduser()
        if entry_path == directory:
            return True
    return False


def _make_executable(path: Path):
    """Ensure target is executable on POSIX systems."""
    try:
        mode = path.stat().st_mode
        path.chmod(mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
    except (PermissionError, OSError):
        pass


def _install_cli_binary(script_path: Path, bin_dir: str, binary_name: str) -> Path:
    """Install CLI binary to the specified directory."""
    target_dir = Path(bin_dir).expanduser()
    target_dir.mkdir(parents=True, exist_ok=True)
    target_path = target_dir / binary_name
    shutil.copy2(script_path, target_path)
    _make_executable(target_path)
    print(f"Installed CLI to {target_path}")
    if not _dir_on_path(target_dir):
        print(
            f'Note: {target_dir} is not currently on PATH. Add `export PATH="{target_dir}:$PATH"` to your shell profile.'
        )
    return target_path


def _install_skill_files(script_path: Path, skill_dir: str, skill_name: str):
    """Install skill binary and metadata."""
    target_dir = Path(skill_dir).expanduser()
    target_dir.mkdir(parents=True, exist_ok=True)
    skill_binary = target_dir / skill_name
    shutil.copy2(script_path, skill_binary)
    _make_executable(skill_binary)
    skill_metadata = script_path.with_name("SKILL.md")
    if skill_metadata.exists():
        shutil.copy2(skill_metadata, target_dir / "SKILL.md")
    else:
        print(f"Warning: {skill_metadata} not found; skipping SKILL metadata copy.")
    print(f"Installed Claude skill files to {target_dir}")


def _ensure_cleanup_settings():
    """Ensure cleanupPeriodDays is set to a high retention value."""
    settings_path = Path.home() / ".claude" / "settings.json"
    settings_path.parent.mkdir(parents=True, exist_ok=True)
    data = {}
    if settings_path.exists():
        try:
            data = json.loads(settings_path.read_text())
        except json.JSONDecodeError:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            backup_path = settings_path.with_name(f"{settings_path.name}.{timestamp}.bak")
            settings_path.rename(backup_path)
            print(f"Moved invalid settings file to {backup_path}")
            data = {}

    previous_value = data.get("cleanupPeriodDays")
    data["cleanupPeriodDays"] = DEFAULT_CLEANUP_DAYS
    settings_path.write_text(json.dumps(data, indent=2) + "\n")

    if previous_value == DEFAULT_CLEANUP_DAYS:
        print(f"{settings_path} already had cleanupPeriodDays={DEFAULT_CLEANUP_DAYS}")
    else:
        print(f"Updated {settings_path} (cleanupPeriodDays={DEFAULT_CLEANUP_DAYS})")


def _dispatch_install(args):
    """Handle install command."""
    script_path = Path(__file__).resolve()
    did_work = False

    if not getattr(args, "skip_cli", False):
        _install_cli_binary(script_path, args.bin_dir, args.cli_name)
        did_work = True
    else:
        print("Skipped CLI installation (--skip-cli).")

    if not getattr(args, "skip_skill", False):
        _install_skill_files(script_path, args.skill_dir, args.skill_name)
        did_work = True
    else:
        print("Skipped Claude skill installation (--skip-skill).")

    if not getattr(args, "skip_settings", False):
        _ensure_cleanup_settings()
    else:
        print("Skipped settings update (--skip-settings).")

    if not did_work:
        print("No files were installed.")


def _dispatch_command(args):
    """Dispatch parsed arguments to appropriate command handler.

    Args:
        args: Parsed argparse.Namespace object
    """
    if args.command == "lsw":
        _dispatch_lsw(args)
    elif args.command == "lss":
        _dispatch_lss(args)
    elif args.command == "lsh":
        cmd_lsh(args)
    elif args.command == "export":
        _dispatch_export(args)
    elif args.command == "alias":
        _dispatch_alias(args)
    elif args.command == "stats":
        _dispatch_stats(args)
    elif args.command == "reset":
        cmd_reset(args)
    elif args.command == "install":
        _dispatch_install(args)
    elif args.command == "gemini-index":
        cmd_gemini_index(args)


def main():
    """Main entry point."""
    _setup_windows_encoding()
    parser = _create_argument_parser()
    args = parser.parse_args()
    if not hasattr(args, "command") or args.command is None:
        parser.print_help()
        sys.exit(0)

    # Progressively build Gemini hash→path index from current directory
    # This learns the mapping as the user runs agent-history from different directories
    try:
        gemini_update_hash_index_from_cwd()
    except Exception:
        pass  # Non-critical, don't fail if index update fails

    _dispatch_command(args)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.stderr.write("\\nInterrupted\\n")
        sys.exit(130)
    except Exception as e:
        sys.stderr.write(f"\\nError: {e}\\n")
        sys.exit(1)
