#!/usr/bin/env python3
"""
agent-history - Manage and export AI coding assistant conversation sessions

A unified tool to list, filter, and export conversation sessions from Claude Code
and Codex CLI, organized by workspace. Supports both agents with auto-detection.

Author: Built with Claude Code
License: MIT
"""

import argparse
import base64
import errno
import hashlib
import json
import math
import os
import platform
import re
import shlex
import shutil
import sqlite3
import stat
import subprocess
import sys
import tempfile
import threading
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import TimeoutError as FuturesTimeoutError
from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path, PureWindowsPath
from types import SimpleNamespace
from typing import Any, Callable, Optional, TypedDict, Union, cast

__version__ = "1.5.1"

DEFAULT_CLI_NAME = "agent-history"
DEFAULT_SKILL_NAME = "agent-history"
DEFAULT_BIN_DIR = Path.home() / ".local/bin"
DEFAULT_SKILL_DIR = Path.home() / ".claude/skills" / DEFAULT_SKILL_NAME
DEFAULT_CLEANUP_DAYS = 99999

# Agent backend identifiers
AGENT_CLAUDE = "claude"
AGENT_CODEX = "codex"
AGENT_GEMINI = "gemini"

# Codex content types for text extraction
CODEX_TEXT_TYPES = frozenset(["input_text", "output_text"])

# Gemini role display names
_GEMINI_ROLE_DISPLAY_NAMES = {
    "user": "User",
    "assistant": "Model",
    "info": "Info",
    "error": "Error",
    "warning": "Warning",
}

# Codex home directory
CODEX_HOME_DIR = Path.home() / ".codex" / "sessions"

# Gemini home directory (sessions stored in ~/.gemini/tmp/<hash>/chats/)
GEMINI_HOME_DIR = Path.home() / ".gemini" / "tmp"

# ============================================================================
# Named Constants (for readability and maintainability)
# ============================================================================

# Path parsing constants with explicit derivation (NAME-CONST)
MIN_WINDOWS_PATH_LEN = len("C:")  # Minimum for "C:" style paths
MIN_WSL_MNT_PATH_LEN = len("/mnt/c")  # WSL mount prefix length
WSL_UNC_MIN_PARTS = len(["", "wsl.localhost", "Distro"])  # Parts in //wsl.localhost/Distro
WSL_LOCALHOST_SKIP_PARTS = len(["", "", "wsl.localhost", "Distro"])  # Parts to skip
MIN_ENCODED_PATH_LEN = len("C--")  # Minimum for Windows encoded paths

# Splitting constants
MIN_SPLIT_POINTS = 2  # Minimum split points (start + end) for no-split case
MIN_MESSAGES_FOR_SPLIT = 3  # Minimum messages before considering split

# Remote/WSL parsing constants
REMOTE_PARTS_WITH_PATH = 3  # Parts in "remote_hostname_path" after split
SSH_WORKSPACE_PARTS = 3  # Parts in workspace line "encoded|decoded|count"
SSH_SESSION_PARTS = 4  # Parts in session line "file|size|mtime|lines"

# Alias parsing constants
WSL_PREFIX_LEN = 6  # Length of "wsl://"
MNT_ENCODED_PREFIX_LEN = 7  # Length of "-mnt-X-" prefix
WINDOWS_PREFIX_LEN = 8  # Length of "windows:"

# Display constants
MAX_WORKSPACE_NAME_DISPLAY = 28  # Max chars for workspace name in stats display
MAX_SHORT_PART_LEN = 10  # Max length for "short" path component heuristic
RSYNC_PARTIAL_TRANSFER_CODE = 12
EXPORT_PROGRESS_MIN = 10
HASH_DISPLAY_LEN = 8  # Characters to show for truncated hash display
MAX_THOUGHT_LEN = 200  # Max length for thought descriptions before truncating
MAX_TOOL_OUTPUT_LEN = 2000  # Max length for tool output before truncating

# Codex date folder parsing
CODEX_DATE_FOLDER_DEPTH = 4  # Depth of YYYY/MM/DD/file structure

# Time constants
SECONDS_PER_MINUTE = 60

# Database migration versions
DB_VERSION_TIME_TRACKING = 3  # Version that added time tracking
DB_VERSION_AGENT_COLUMN = 4  # Version that added agent column with path-based detection
DB_VERSION_GEMINI_TIMESTAMPS = 5  # Version that fixes Gemini timestamps


# ============================================================================
# Text Truncation Utilities (M4: Extracted helpers)
# ============================================================================


def _truncate_tool_output(output: str, max_len: int = MAX_TOOL_OUTPUT_LEN) -> str:
    """Truncate tool output to max length with indicator.

    Args:
        output: Tool output string
        max_len: Maximum length before truncation

    Returns:
        Original string if within limit, otherwise truncated with indicator
    """
    if len(output) <= max_len:
        return output
    return output[:max_len] + "\n... [truncated]"


def _truncate_thought(thought: str, max_len: int = MAX_THOUGHT_LEN) -> str:
    """Truncate thought description to max length.

    Args:
        thought: Thought description string
        max_len: Maximum length before truncation

    Returns:
        Original string if within limit, otherwise truncated with ellipsis
    """
    if len(thought) <= max_len:
        return thought
    return thought[:max_len] + "..."


# ============================================================================
# TypedDicts for Complex Structures (Type Safety)
# ============================================================================


class SessionMetrics(TypedDict):
    """Session metadata in metrics dict."""

    id: Optional[str]
    cwd: Optional[str]
    cli_version: Optional[str]
    model: Optional[str]
    startTime: Optional[str]
    lastUpdated: Optional[str]


class TokensMetrics(TypedDict):
    """Token counts in metrics dict."""

    input: int
    output: int
    total: int


class TokensSummary(TypedDict, total=False):
    """Token summary for Codex sessions."""

    input_tokens: int
    output_tokens: int
    cache_read_tokens: int
    timestamp: Optional[str]


class MetricsDict(TypedDict, total=False):
    """Full metrics dictionary structure."""

    session: SessionMetrics
    messages: list[dict[str, Any]]
    tool_uses: list[dict[str, Any]]
    tokens: TokensMetrics
    tokens_summary: TokensSummary


class ExportTotals(TypedDict):
    """Totals tracking for export-all operations."""

    sources: list[dict[str, Any]]
    sessions: int
    errors: int


class HashIndexCounts(TypedDict):
    """Counts for hash index operations."""

    added: int
    existing: int
    no_sessions: int
    mappings: list[Any]


# ============================================================================
# JSON Serialization Utilities (L10: Extracted helpers)
# ============================================================================


def pretty_json(obj: Any) -> str:
    """Format object as indented JSON.

    Args:
        obj: Object to serialize

    Returns:
        Pretty-printed JSON string
    """
    return json.dumps(obj, indent=2, ensure_ascii=False)


def save_json(path: Path, data: dict[str, Any]) -> None:
    """Save JSON to file with pretty formatting.

    Args:
        path: Path to save to
        data: Dictionary to serialize
    """
    with open(path, "w", encoding="utf-8") as f:
        f.write(pretty_json(data))


def load_json(path: Path) -> dict[str, Any]:
    """Load JSON from file.

    Args:
        path: Path to load from

    Returns:
        Parsed dictionary
    """
    with open(path, encoding="utf-8") as f:
        return json.load(f)


def codex_get_home_dir() -> Path:
    """Get Codex sessions directory (~/.codex/sessions/).

    Supports CODEX_SESSIONS_DIR environment variable override for testing
    and custom configurations.
    """
    env_override = os.environ.get("CODEX_SESSIONS_DIR")
    if env_override:
        return Path(env_override).expanduser()
    return CODEX_HOME_DIR


def gemini_get_home_dir() -> Path:
    """Get Gemini sessions directory (~/.gemini/tmp/).

    Supports GEMINI_SESSIONS_DIR environment variable override for testing
    and custom configurations.
    """
    env_override = os.environ.get("GEMINI_SESSIONS_DIR")
    if env_override:
        return Path(env_override).expanduser()
    return GEMINI_HOME_DIR


def detect_agent_from_path(path: Path) -> str:
    """Detect agent type from file path.

    Args:
        path: Path to a session file

    Returns:
        AGENT_GEMINI if path contains .gemini,
        AGENT_CODEX if path contains .codex,
        otherwise AGENT_CLAUDE
    """
    path_str = str(path)
    if "/.gemini/" in path_str or "\\.gemini\\" in path_str:
        return AGENT_GEMINI
    if "/.codex/" in path_str or "\\.codex\\" in path_str:
        return AGENT_CODEX
    path_parts = [part.lower() for part in path.parts]
    if any(part.startswith("remote_") and part.endswith("_gemini") for part in path_parts):
        return AGENT_GEMINI
    if any(part.startswith("remote_") and part.endswith("_codex") for part in path_parts):
        return AGENT_CODEX
    if path.suffix.lower() == ".json":
        return AGENT_GEMINI
    return AGENT_CLAUDE


# ============================================================================
# Data Classes
# ============================================================================


@dataclass
class ExportConfig:
    """Configuration for batch export operations.

    This dataclass replaces ad-hoc argument classes used throughout the codebase,
    providing type safety and a single source of truth for export parameters.
    """

    output_dir: str
    patterns: list = field(default_factory=list)
    since: Optional[str] = None
    until: Optional[str] = None
    force: bool = False
    minimal: bool = False
    split: Optional[int] = None
    flat: bool = False
    remote: Optional[str] = None
    lenient: bool = False
    agent: str = "auto"
    jobs: int = 1
    quiet: bool = False

    @property
    def workspace(self) -> str:
        """First pattern as workspace (backward compatibility)."""
        return self.patterns[0] if self.patterns else ""

    @classmethod
    def from_args(cls, args, **overrides) -> "ExportConfig":
        """Create ExportConfig from argparse args with optional overrides.

        Args:
            args: Parsed argparse arguments
            **overrides: Values to override from args

        Returns:
            ExportConfig instance
        """

        def get_field(name: str, default=None):
            """Get field from overrides or args with fallback.

            Precedence: overrides > args > default (M1: explicit precedence)
            """
            if name in overrides:
                return overrides[name]
            return getattr(args, name, default)

        return cls(
            output_dir=get_field("output_dir", "."),
            patterns=get_field("patterns", []),
            since=get_field("since"),
            until=get_field("until"),
            force=get_field("force", False),
            minimal=get_field("minimal", False),
            split=get_field("split"),
            flat=get_field("flat", False),
            remote=get_field("remote"),
            lenient=get_field("lenient", False),
            agent=get_field("agent") or "auto",
            jobs=get_field("jobs", 1),
            quiet=get_field("quiet", False),
        )


@dataclass
class ListCommandArgs:
    """Arguments for list commands (lsw/lss).

    This dataclass replaces ad-hoc LswArgs, LssArgs, LswAllArgs, etc. classes,
    providing a single source of truth for list command parameters.

    Note: since/until can be either strings (YYYY-MM-DD) or datetime objects.
    Use since_date/until_date properties to get datetime objects.
    """

    patterns: list = field(default_factory=list)
    workspaces_only: bool = False
    since: Optional[str] = None  # String format YYYY-MM-DD
    until: Optional[str] = None  # String format YYYY-MM-DD
    remote: Optional[str] = None
    remotes: list = field(default_factory=list)
    local_only: bool = False
    agent: str = "auto"
    projects_dir: Optional[Path] = None
    counts: bool = False
    wsl_counts: bool = False
    no_wsl: bool = False
    no_windows: bool = False

    @property
    def workspace(self) -> str:
        """First pattern as workspace (backward compatibility)."""
        return self.patterns[0] if self.patterns else ""

    @property
    def since_date(self) -> Optional[datetime]:
        """Since as datetime object."""
        if self.since is None:
            return None
        if isinstance(self.since, datetime):
            return self.since
        return parse_date_string(self.since)

    @property
    def until_date(self) -> Optional[datetime]:
        """Until as datetime object."""
        if self.until is None:
            return None
        if isinstance(self.until, datetime):
            return self.until
        return parse_date_string(self.until)


@dataclass
class SyncCommandArgs:
    """Arguments for stats sync command.

    Replaces ad-hoc SyncArgs class for cleaner sync operations.
    """

    patterns: list = field(default_factory=list)
    remotes: list = field(default_factory=list)
    force: bool = False
    all_homes: bool = False
    jobs: int = 1
    no_remote: bool = False
    no_wsl: bool = False
    no_windows: bool = False
    show_progress: bool = False


@dataclass
class StatsCommandArgs:
    """Arguments for stats display command.

    Replaces ad-hoc StatsArgs class for cleaner stats operations.
    """

    workspace: list = field(default_factory=list)
    source: Optional[str] = None
    since: Optional[str] = None
    until: Optional[str] = None
    tools: bool = False
    models: bool = False
    time: bool = False
    by_workspace: bool = False
    by_day: bool = False
    all_workspaces: bool = False
    top_ws: Optional[int] = None
    this_only: bool = False
    agent: str = "auto"


@dataclass
class ConvertCommandArgs:
    """Arguments for single file conversion command.

    Replaces ad-hoc ConvertArgs class.
    """

    jsonl_file: str = ""
    output: Optional[str] = None
    remote: Optional[str] = None
    quiet: bool = False


@dataclass
class ExportAllConfig:
    """Configuration for export-all-homes operations.

    Used by cmd_export_all to export from local, WSL, Windows, and remotes.
    """

    output_dir: str
    patterns: list = field(default_factory=list)
    remotes: list = field(default_factory=list)
    since: Optional[str] = None
    until: Optional[str] = None
    force: bool = False
    minimal: bool = False
    split: Optional[int] = None
    agent: str = "auto"
    jobs: int = 1
    quiet: bool = False
    index: bool = True
    no_wsl: bool = False
    no_windows: bool = False
    no_remote: bool = False

    @property
    def workspace(self) -> str:
        """First pattern as workspace (backward compatibility)."""
        return self.patterns[0] if self.patterns else ""


@dataclass
class SummaryStatsData:
    """Aggregated statistics for display.

    Groups all the data needed by _print_summary_stats to avoid too many parameters.
    """

    session_stats: dict
    token_stats: dict
    tool_stats: dict
    sources: list
    models: list
    top_workspaces: list
    top_limit: Optional[int] = None
    time_stats: Optional[dict] = None
    agent: str = "auto"


# ============================================================================
# Date Parsing
# ============================================================================


def parse_date_string(date_str):
    """
    Parse ISO date string (YYYY-MM-DD) into datetime object.
    Returns datetime object or None if parsing fails.
    """
    if not date_str:
        return None

    try:
        return datetime.strptime(date_str.strip(), "%Y-%m-%d")
    except ValueError:
        return None


def validate_split_lines(value):
    """
    Validate split lines argument for argparse.
    Ensures value is a positive integer.
    """
    try:
        ivalue = int(value)
        if ivalue <= 0:
            raise argparse.ArgumentTypeError(
                f"split value must be a positive integer, got '{value}'"
            )
        return ivalue
    except ValueError as e:
        raise argparse.ArgumentTypeError(f"split value must be an integer, got '{value}'") from e


# ============================================================================
# Input Validation (Security)
# ============================================================================

# Pattern for valid workspace directory names (Claude Code encoded paths)
# Allow: alphanumeric, dashes, underscores, dots, and Unicode characters
# Workspace names always start with dash (Unix) or letter (Windows drive)
# Unicode range \u0080-\uFFFF supports international paths
WORKSPACE_NAME_PATTERN = re.compile(r"^[-a-zA-Z0-9_.\u0080-\uFFFF]+$")

# Cache for Windows home path lookups (avoids repeated slow filesystem operations)
# NO-GLOBAL-MUTABLE: Using class-based cache for testability


class _WindowsHomeCache:
    """Cache for Windows home directory lookups.

    Avoids repeated slow filesystem operations when looking up
    Windows user home directories from WSL.
    """

    def __init__(self) -> None:
        self._cache: dict[str, Optional[Path]] = {}

    def get(self, key: str) -> Optional[Path]:
        """Get cached value for key."""
        return self._cache.get(key)

    def has(self, key: str) -> bool:
        """Check if key is cached."""
        return key in self._cache

    def set(self, key: str, value: Optional[Path]) -> None:
        """Set cached value for key."""
        self._cache[key] = value

    def clear(self) -> None:
        """Clear cache (useful for testing)."""
        self._cache.clear()


# Use holder dict to avoid global statement (PLW0603)
_cache_holder: dict[str, _WindowsHomeCache] = {"instance": _WindowsHomeCache()}


def _get_windows_home_cache() -> _WindowsHomeCache:
    """Get the current Windows home cache instance."""
    return _cache_holder["instance"]


@contextmanager
def windows_home_cache_context(cache: Optional[_WindowsHomeCache] = None):
    """Context manager to temporarily override the Windows home cache.

    This enables testing without affecting global state. When used in tests,
    provides an isolated cache instance that doesn't affect other tests.

    Args:
        cache: Optional cache instance to use. If None, creates a new empty cache.

    Yields:
        The active cache instance.

    Example:
        def test_windows_home_caching():
            cache = _WindowsHomeCache()
            with windows_home_cache_context(cache):
                # Test with isolated cache
                result = get_windows_user_home("alice")
                assert cache.has("alice")
    """
    old_cache = _cache_holder["instance"]
    try:
        _cache_holder["instance"] = cache if cache is not None else _WindowsHomeCache()
        yield _cache_holder["instance"]
    finally:
        _cache_holder["instance"] = old_cache


# Pattern for valid SSH remote host specifications
# Allow: user@hostname, hostname, user@hostname:port, IPv4/IPv6
REMOTE_HOST_PATTERN = re.compile(
    r"^(?:[\w.-]+@)?"  # Optional user@
    r"(?:[\w.-]+|\[[0-9a-fA-F:]+\])"  # Hostname or IPv6 in brackets
    r"(?::\d+)?$"  # Optional :port
)


def validate_workspace_name(workspace_name: str) -> bool:
    """Validate workspace name to prevent command injection and path traversal.

    Workspace names from Claude Code are encoded paths like:
    - '-home-user-project' (Unix)
    - 'C--Users-name-project' (Windows)
    - 'remote_host_home-user-project' (cached remote)
    - 'wsl_Ubuntu_home-user-project' (cached WSL)

    Returns True if valid, False otherwise.
    """
    if not workspace_name:
        return False
    if len(workspace_name) > MAX_WORKSPACE_NAME_LENGTH:
        return False
    # Check for path traversal sequences
    if ".." in workspace_name:
        return False
    return WORKSPACE_NAME_PATTERN.match(workspace_name) is not None


def validate_remote_host(remote_host: Optional[str]) -> bool:
    """Validate remote host specification to prevent command injection.

    Valid formats:
    - hostname
    - user@hostname
    - user@hostname:port
    - IPv4 address
    - user@IPv6 (with brackets)

    Returns True if valid, False otherwise.
    """
    if not remote_host:
        return False
    if len(remote_host) > MAX_REMOTE_HOST_LENGTH:
        return False
    return bool(REMOTE_HOST_PATTERN.match(remote_host))


def sanitize_for_shell(value: str) -> str:
    """Sanitize a value for safe use in shell commands.

    Uses shlex.quote for proper escaping.
    """
    return shlex.quote(value)


def is_safe_path(base_dir: Path, target_path: Path) -> bool:
    """Check if target_path is safely within base_dir (no path traversal).

    This prevents directory traversal attacks where a malicious path
    could escape the intended base directory.

    Args:
        base_dir: The base directory that should contain the path
        target_path: The path to validate

    Returns:
        True if target_path is within base_dir, False otherwise.
    """
    try:
        # Resolve both paths to absolute paths (resolves symlinks and ..)
        base_resolved = base_dir.resolve()
        target_resolved = target_path.resolve()

        # Check if target is within base (or is base itself)
        return target_resolved == base_resolved or str(target_resolved).startswith(
            str(base_resolved) + os.sep
        )
    except (OSError, ValueError):
        return False


# ============================================================================
# Constants
# ============================================================================

# Validation limits
MAX_WORKSPACE_NAME_LENGTH = 1000  # Reasonable limit for workspace names
MAX_REMOTE_HOST_LENGTH = 255  # DNS hostname limit

# Conversation splitting thresholds
SPLIT_MIN_FACTOR = 0.8  # Minimum lines before considering split (80% of target)
SPLIT_MAX_FACTOR = 1.3  # Maximum lines before forcing split (130% of target)
HEADER_LINES_ESTIMATE = 30  # Approximate header size in lines
METADATA_LINES_ESTIMATE = 20  # Approximate metadata section size in lines

# Split point scoring weights
SCORE_USER_MESSAGE_NEXT = 100  # Next message is User (best - starting new topic)
SCORE_TOOL_RESULT = 50  # Current message is tool result (action complete)
SCORE_TIME_GAP_LARGE = 30  # Time gap > 5 minutes
SCORE_TIME_GAP_MEDIUM = 10  # Time gap > 1 minute
SCORE_DISTANCE_PENALTY = 0.05  # Penalty per line away from target

# Time gap thresholds (in seconds)
TIME_GAP_LARGE = 300  # 5 minutes
TIME_GAP_MEDIUM = 60  # 1 minute

# Markdown markers for tool content (NO-MAGIC-STRING)
TOOL_RESULT_MARKER = "**[Tool Result:"
TOOL_USE_MARKER = "**[Tool Use:"

# Pattern matching constants (SIMPLIFY-BOOLEAN)
_MATCH_ALL_PATTERNS = frozenset(["", "*", "all"])

# Regex for tool name extraction (NO-REGEX-PARSE)
_TOOL_NAME_PATTERN = re.compile(r"\*\*\[Tool:\s*([^\]]+)\]\*\*")

# Rsync exit codes (DATA-STRUCTURE: hoisted to module level)
_RSYNC_EXIT_CODES: dict[int, tuple[bool, str]] = {
    0: (True, "Success"),
    1: (False, "Syntax or usage error"),
    2: (False, "Protocol incompatibility"),
    3: (False, "Errors selecting input/output files"),
    5: (False, "Error starting client-server protocol"),
    6: (False, "Daemon unable to append to log file"),
    10: (False, "Error in socket I/O"),
    11: (False, "Error in file I/O"),
    12: (False, "Error in rsync protocol data stream"),
    13: (False, "Errors with program diagnostics"),
    14: (False, "Error in IPC code"),
    20: (False, "Received SIGUSR1 or SIGINT"),
    21: (False, "Error returned by waitpid()"),
    22: (False, "Error allocating memory"),
    23: (True, "Partial transfer due to error"),  # Some files transferred
    24: (True, "Partial transfer - source files vanished"),  # Some files transferred
    25: (False, "Max delete limit reached"),
    30: (False, "Timeout in data send/receive"),
    35: (False, "Timeout waiting for daemon"),
}

# Display limits
MAX_WORKSPACE_CANDIDATES = 20  # Maximum workspaces to show in suggestions
MAX_TOP_WORKSPACES = 20  # Top workspaces to show in stats

# Timeouts (in seconds)
SSH_TIMEOUT = 300  # 5 minutes for SSH operations
SCP_TIMEOUT = 60  # 1 minute for single file copy

# Work period detection (in seconds)
WORK_PERIOD_GAP_THRESHOLD = 30 * 60  # 30 minutes gap starts new work period

# Display formatting
DISPLAY_SEPARATOR_WIDTH = 60  # Width of separator lines

# Cached workspace prefixes
CACHED_REMOTE_PREFIX = "remote_"
CACHED_WSL_PREFIX = "wsl_"
CACHED_WINDOWS_PREFIX = "windows_"
# Tuple for efficient membership testing
CACHED_PREFIXES = (CACHED_REMOTE_PREFIX, CACHED_WSL_PREFIX, CACHED_WINDOWS_PREFIX)

# ============================================================================
# Error Handling Utilities
# ============================================================================


def exit_with_error(message: str, suggestions: Optional[list] = None, exit_code: int = 1):
    """Exit with formatted error message and optional actionable suggestions.

    This centralizes error formatting to provide consistent, helpful error
    messages throughout the CLI. Following Rhodes' principle that errors
    should guide users to solutions.

    Args:
        message: The main error message (without "Error:" prefix)
        suggestions: Optional list of actionable suggestions for the user
        exit_code: Exit code (default: 1)

    Example:
        exit_with_error(
            "Not in a Claude Code workspace",
            suggestions=[
                "Run from within a workspace directory",
                "Specify a workspace pattern: lss <pattern>",
                "Use --aw to list all workspaces"
            ]
        )
    """
    sys.stderr.write(f"Error: {message}\n")
    if suggestions:
        sys.stderr.write("\nTo resolve, try:\n")
        for suggestion in suggestions:
            sys.stderr.write(f"  â€¢ {suggestion}\n")
    sys.exit(exit_code)


def exit_with_date_error(flag_name: str, invalid_value: str):
    """Exit with date format error message.

    Args:
        flag_name: The flag that received invalid date (e.g., "--since")
        invalid_value: The invalid date string provided
    """
    exit_with_error(
        f"Invalid date format for {flag_name}: '{invalid_value}'",
        suggestions=["Use YYYY-MM-DD format (e.g., 2025-11-01)"],
    )


def parse_date_with_validation(date_str: Optional[str], flag_name: str):
    """Parse date string with validation, exit on error.

    Args:
        date_str: Date string to parse (YYYY-MM-DD format)
        flag_name: Flag name for error message (e.g., "--since")

    Returns:
        datetime object or None if date_str is None/empty
    """
    if not date_str:
        return None
    result = parse_date_string(date_str)
    if result is None:
        exit_with_date_error(flag_name, date_str)
    return result


# ============================================================================
# Utility Functions
# ============================================================================


def is_cached_workspace(name: str) -> bool:
    """Check if a workspace/directory name is a cached remote, WSL, or Windows workspace.

    Cached workspaces are created when fetching from remote SSH hosts, WSL
    distributions, or Windows (from WSL). They have prefixes like:
    - 'remote_hostname_...'
    - 'wsl_distro_...'
    - 'windows_username_...'

    Args:
        name: Workspace or directory name to check

    Returns:
        True if this is a cached workspace, False otherwise.
    """
    return any(name.startswith(prefix) for prefix in CACHED_PREFIXES)


def is_native_workspace(name: str) -> bool:
    """Check if a workspace/directory name is a native (non-cached) workspace.

    This is the inverse of is_cached_workspace().

    Args:
        name: Workspace or directory name to check

    Returns:
        True if this is a native workspace, False if cached.
    """
    return not is_cached_workspace(name)


def matches_any_pattern(workspace_name: str, patterns: list[str]) -> bool:
    """Check if workspace name matches any pattern in the list.

    A pattern matches if it is a substring of the workspace name.
    Empty patterns ('', '*', 'all') match all workspaces.

    Args:
        workspace_name: Workspace name to check
        patterns: List of patterns to match against

    Returns:
        True if workspace matches any pattern, False otherwise.
    """
    if not patterns:
        return True

    # Check for match-all patterns using frozenset (SIMPLIFY-BOOLEAN)
    if any(p in _MATCH_ALL_PATTERNS for p in patterns):
        return True

    # Check for substring matches
    return any(pattern and pattern in workspace_name for pattern in patterns)


def parse_and_validate_dates(since_str: Optional[str], until_str: Optional[str]) -> tuple:
    """Parse and validate date filter arguments.

    Validates that date strings are in YYYY-MM-DD format and that
    the since date comes before the until date (if both provided).

    Args:
        since_str: Start date string (YYYY-MM-DD) or None
        until_str: End date string (YYYY-MM-DD) or None

    Returns:
        Tuple of (since_date, until_date) as datetime objects or None.
        Exits with error if validation fails.
    """
    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    if since_str and since_date is None:
        exit_with_date_error("--since", since_str)

    if until_str and until_date is None:
        exit_with_date_error("--until", until_str)

    if since_date and until_date and since_date > until_date:
        exit_with_error("--since date must be before --until date")

    # Warn about future dates (they're valid but likely unintentional)
    today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    if since_date and since_date > today:
        sys.stderr.write(
            f"Warning: --since date '{since_str}' is in the future; no results expected\n"
        )
    if until_date and until_date > today:
        sys.stderr.write(f"Warning: --until date '{until_str}' is in the future\n")

    return since_date, until_date


def is_date_in_range(
    dt: Optional[datetime],
    since_date: Optional[datetime],
    until_date: Optional[datetime],
) -> bool:
    """Check if datetime is within date range (inclusive).

    Args:
        dt: datetime object to check
        since_date: Start date filter (datetime or None)
        until_date: End date filter (datetime or None)

    Returns:
        True if dt is within range, False otherwise.
    """
    if dt is None:
        return True
    check_date = dt.date() if hasattr(dt, "date") else dt
    if since_date:
        since = since_date.date() if hasattr(since_date, "date") else since_date
        if check_date < since:
            return False
    if until_date:
        until = until_date.date() if hasattr(until_date, "date") else until_date
        if check_date > until:
            return False
    return True


# ============================================================================
# Shared Session Filter Logic (H2: DRY, H6: Early Returns, H8: Flat Conditionals)
# ============================================================================


def _matches_workspace_pattern(
    workspace: str,
    pattern: str,
    get_readable: Optional[Callable[[str], str]] = None,
) -> bool:
    """Check if workspace matches pattern with optional readable name lookup.

    This is a shared helper that eliminates duplicate filter logic between
    Codex and Gemini backends.

    Args:
        workspace: Encoded workspace identifier
        pattern: Pattern to match (substring or Claude-style encoded)
        get_readable: Optional function to get human-readable workspace name

    Returns:
        True if pattern matches workspace
    """
    # No pattern filter - match all
    if not pattern:
        return True

    readable = get_readable(workspace) if get_readable else None

    # Prefer matching against readable paths first (avoids false positives due to
    # path encoding collisions like "claude/history" -> "...-claude-history").
    if readable and pattern in readable:
        return True

    # Match encoded IDs only when user appears to provide an encoded pattern.
    looks_encoded = (
        pattern.startswith("-")
        or pattern.startswith("home-")
        or pattern.startswith("mnt-")
        or bool(re.match(r"^[A-Za-z]--", pattern))
    )
    if looks_encoded and pattern in workspace:
        return True

    # If we don't have a readable representation, fall back to matching the raw identifier.
    if not readable and pattern in workspace:
        return True

    # Claude-style encoded pattern match (home-user-project -> /home/user/project).
    # Only apply this heuristic when the pattern looks like a path encoding, otherwise
    # it can create false positives for ordinary hyphenated names (e.g. "claude-history").
    if not looks_encoded:
        return False

    normalized_pattern = pattern[1:] if pattern.startswith("-") else pattern
    pattern_as_path = "/" + normalized_pattern.replace("-", "/")
    return pattern_as_path in (readable or workspace)


def _session_matches_filters(
    workspace: str,
    modified: datetime,
    pattern: str,
    since_date: Optional[datetime],
    until_date: Optional[datetime],
    get_readable: Optional[Callable[[str], str]] = None,
) -> bool:
    """Check if session matches all filters.

    Shared implementation for Codex and Gemini backends, eliminating
    duplicate filter logic.

    Args:
        workspace: Workspace identifier
        modified: Session modification datetime
        pattern: Workspace pattern to match
        since_date: Start date filter (inclusive)
        until_date: End date filter (inclusive)
        get_readable: Optional function to get human-readable workspace name

    Returns:
        True if session matches all filters
    """
    if not _matches_workspace_pattern(workspace, pattern, get_readable):
        return False
    return is_date_in_range(modified, since_date, until_date)


def get_patterns_from_args(args) -> list:
    """Extract workspace patterns from command arguments.

    Handles both new-style 'patterns' (lsw) and legacy 'workspace' (lss) attributes.
    Ensures we always return a flat list of strings and default to [""] (match all)
    when no explicit patterns are provided.

    Args:
        args: Parsed argument object

    Returns:
        List[str] of patterns ("" matches all workspaces)
    """
    # New-style 'patterns' takes precedence if present
    patterns = getattr(args, "patterns", None)
    if patterns is not None:
        return patterns if patterns else [""]

    # lss uses 'workspace' (nargs="*") which is a list
    work = getattr(args, "workspace", None)
    if work is None:
        return [""]
    if isinstance(work, list):
        return work if work else [""]
    # Fallback: single string value
    return [work]


def detect_alias_from_args(args, workspace_list: Optional[list] = None) -> Optional[str]:
    """Detect alias name from args or @ prefix in workspace list.

    Args:
        args: Parsed arguments with optional 'alias' attribute
        workspace_list: List of workspace patterns (may have @prefix)

    Returns:
        Alias name if found, None otherwise
    """
    alias_flag = getattr(args, "alias", None)
    if alias_flag:
        return alias_flag
    if workspace_list and workspace_list[0].startswith("@"):
        return workspace_list[0][1:]
    return None


def _find_git_root(start: Path) -> Optional[Path]:
    """Find nearest ancestor directory with a .git entry (directory or file)."""
    try:
        start_resolved = start.resolve()
    except OSError:
        start_resolved = start
    for parent in [start_resolved, *start_resolved.parents]:
        if (parent / ".git").exists():
            return parent
    return None


def infer_non_claude_patterns_from_cwd() -> list[str]:
    """Infer useful workspace patterns for Codex/Gemini when none are provided.

    Most users run the CLI from some subdirectory of a repo; using only
    `cwd.name` breaks in that case (e.g. project/docs -> "docs").
    """
    cwd = Path.cwd()
    root = _find_git_root(cwd) or cwd
    patterns = [cwd.name]
    # Add repo root name as a loose fallback when running from subdirectories.
    if root.name and root.name not in patterns:
        patterns.append(root.name)
    return patterns


def _is_wsl_mnt_path(path_str: str) -> bool:
    """Return True if path looks like a WSL /mnt/<drive>/ path."""
    if not path_str.startswith("/mnt/") or len(path_str) <= MIN_WSL_MNT_PATH_LEN:
        return False
    if len(path_str) <= MIN_WSL_MNT_PATH_LEN + 1:
        return False
    drive_letter = path_str[5]
    return drive_letter.isalpha() and path_str[6] == "/"


def _infer_this_only_patterns_for_source(
    source_hint: Optional[str], agent: str
) -> Optional[list[str]]:
    """Infer patterns for --this when targeting a different source."""
    if agent not in ("auto", "claude"):
        return None
    if source_hint not in ("windows", "wsl"):
        return None
    patterns = infer_non_claude_patterns_from_cwd()
    cwd_str = str(Path.cwd())
    if source_hint == "windows" and _is_wsl_mnt_path(cwd_str) and cwd_str not in patterns:
        patterns.insert(0, cwd_str)
    return patterns


def resolve_patterns_for_command(
    workspace_list: list,
    this_only: bool = False,
    require_workspace: bool = False,
    error_message: Optional[str] = None,
    agent: str = "auto",
    source_hint: Optional[str] = None,
) -> tuple:
    """Resolve workspace patterns, optionally checking for alias membership.

    Args:
        workspace_list: Explicit workspace patterns from args
        this_only: If True, skip alias checking
        require_workspace: If True, exit with error if not in workspace
        error_message: Custom error message for require_workspace
        agent: Agent type (auto, claude, codex, gemini). Aliases only apply to Claude.

    Returns:
        Tuple of (patterns, alias_name) where alias_name is set if
        current workspace belongs to an alias and this_only is False
    """
    if workspace_list:
        return workspace_list, None

    # For non-Claude agents, use CWD path as pattern (matches against workspace_readable)
    if agent in ("gemini", "codex"):
        return infer_non_claude_patterns_from_cwd(), None

    if this_only:
        source_patterns = _infer_this_only_patterns_for_source(source_hint, agent)
        if source_patterns:
            return source_patterns, None

    pattern, exists = check_current_workspace_exists()
    if not exists:
        if require_workspace:
            msg = error_message or "Not in a Claude Code workspace."
            exit_with_error(msg)
        return [""], None

    # Aliases are Claude-specific; skip for other agents
    if not this_only and agent in ("auto", "claude"):
        alias_for_ws = get_alias_for_workspace(pattern, "local")
        if alias_for_ws:
            return [pattern], alias_for_ws

    return [pattern], None


def _iter_sessions_deduped(
    patterns: list,
    since_date,
    until_date,
    projects_dir: Optional[Path],
    quiet: bool,
    skip_message_count: bool,
    use_cached_counts: bool,
    agent: str,
):
    """Generate deduplicated sessions from patterns."""
    seen_files = set()

    for pattern in patterns if patterns else [""]:
        pattern_sessions = get_unified_sessions(
            agent=agent,
            pattern=pattern,
            since_date=since_date,
            until_date=until_date,
            skip_message_count=skip_message_count,
            use_cached_counts=use_cached_counts,
            projects_dir=projects_dir,
            quiet=quiet,
        )
        for session in pattern_sessions:
            file_key = str(session["file"])
            if file_key not in seen_files:
                seen_files.add(file_key)
                yield session


def collect_sessions_with_dedup(  # noqa: PLR0913
    patterns: list,
    since_date=None,
    until_date=None,
    projects_dir: Optional[Path] = None,
    quiet: bool = True,
    include_cached: bool = False,
    skip_message_count: bool = False,
    use_cached_counts: bool = False,
    agent: str = "auto",
) -> list:
    """Collect sessions from multiple patterns with automatic deduplication.

    This is a common operation used by cmd_list, cmd_batch, and cmd_export_all.
    Sessions are deduplicated by file path.

    Args:
        patterns: List of workspace patterns to search
        since_date: Optional start date filter
        until_date: Optional end date filter
        projects_dir: Optional projects directory (for WSL/Windows access)
        quiet: If True, suppress "not found" warnings
        include_cached: If True, include cached remote/WSL workspaces
        skip_message_count: If True, skip counting messages (faster for slow filesystems)
        agent: Agent backend to use - 'auto', 'claude', or 'codex'

    Returns:
        List of session dictionaries, deduplicated by file path.
    """
    sessions = _iter_sessions_deduped(
        patterns,
        since_date,
        until_date,
        projects_dir,
        quiet,
        skip_message_count,
        use_cached_counts,
        agent,
    )

    if include_cached:
        return list(sessions)

    # Filter out cached workspaces (only for Claude backend)
    # M2: Extracted predicate for readability
    def _should_include_session(session: dict[str, Any]) -> bool:
        """Check if session should be included (Codex/Gemini or native workspace)."""
        if session.get("agent") in (AGENT_CODEX, AGENT_GEMINI):
            return True
        return is_native_workspace(session["workspace"])

    return [s for s in sessions if _should_include_session(s)]


def _format_message_count(session: dict) -> str:
    """Format message count for output, honoring skipped counts."""
    if session.get("message_count_skipped"):
        return "?"
    return str(session.get("message_count", 0))


def format_session_line(session: dict, source_label: str) -> str:
    """Format a session as a tab-separated output line.

    Args:
        session: Session dictionary with workspace_readable, filename, etc.
        source_label: Source label (e.g., 'Local', 'WSL (Ubuntu)', 'Remote (host)')

    Returns:
        Tab-separated string for output.
    """
    agent = session.get("agent", AGENT_CLAUDE)
    return (
        f"{agent}\t{source_label}\t{session['workspace_readable']}\t{session['filename']}\t"
        f"{_format_message_count(session)}\t{session['modified'].strftime('%Y-%m-%d')}"
    )


def print_sessions_output(sessions: list, source_label: str, workspaces_only: bool):
    """Print sessions in appropriate format (workspaces-only or full details).

    Args:
        sessions: List of session dictionaries
        source_label: Source label for output
        workspaces_only: If True, print only unique workspace names
    """

    # M3: Extracted Windows path filtering for testability
    def _should_skip_windows_workspace(workspace: str) -> bool:
        """Check if Windows workspace should be skipped (non-existent path)."""
        if sys.platform != "win32" or not source_label.startswith("Windows"):
            return False
        path_str = workspace.replace(" [missing]", "")
        try:
            return not Path(path_str).exists()
        except OSError:
            return True  # Skip on error

    if workspaces_only:
        seen = set()
        ordered = []
        for s in sessions:
            # Use workspace_readable for dedup - it's consistent across agents
            key = s.get("workspace_readable") or s.get("workspace")
            if key in seen:
                continue
            seen.add(key)
            ordered.append(s["workspace_readable"])
        for ws in sorted(ordered):
            if _should_skip_windows_workspace(ws):
                continue
            print(ws)
    else:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for session in sessions:
            print(format_session_line(session, source_label))


# ============================================================================
# External Command Paths (Security) - NO-GLOBAL-MUTABLE fix
# ============================================================================


class _CommandPathCache:
    """Cache for external command path resolution.

    Caches results of shutil.which() lookups to avoid repeated
    filesystem searches for the same commands.

    This class wraps the cache to make it testable and clearable,
    following Rhodes' NO-GLOBAL-MUTABLE principle.
    """

    def __init__(self) -> None:
        self._paths: dict[str, str] = {}

    def get_path(self, cmd: str) -> str:
        """Get absolute path for command, with caching.

        Args:
            cmd: Command name (e.g., 'ssh', 'rsync')

        Returns:
            Absolute path or original command name if not found.
        """
        if cmd not in self._paths:
            path = shutil.which(cmd)
            self._paths[cmd] = path if path else cmd
        return self._paths[cmd]

    def clear(self) -> None:
        """Clear cache (useful for testing)."""
        self._paths.clear()


# Module instance (can be replaced in tests)
_command_path_cache = _CommandPathCache()


def get_command_path(cmd: str) -> str:
    """Get absolute path for an external command.

    Uses shutil.which() to find the command in PATH, then caches the result.
    Falls back to the command name if not found (lets subprocess handle the error).

    Args:
        cmd: Command name (e.g., 'ssh', 'rsync', 'wsl')

    Returns:
        Absolute path to the command, or the original name if not found.
    """
    return _command_path_cache.get_path(cmd)


# ============================================================================
# JSONL Parsing and Markdown Conversion
# ============================================================================


def decode_content(encoded_str: str) -> str:
    """Decode base64-encoded content from Claude API responses.

    Some content in .jsonl files (particularly large tool outputs) is stored
    base64-encoded to preserve binary data and special characters.

    Args:
        encoded_str: Base64-encoded string from API response

    Returns:
        Decoded UTF-8 string, or error message if decoding fails.
        Returns error message rather than raising to allow partial
        conversation recovery when some content is corrupted.
    """
    try:
        return base64.b64decode(encoded_str).decode("utf-8")
    except (ValueError, UnicodeDecodeError) as e:
        # ValueError: invalid base64, UnicodeDecodeError: not valid UTF-8
        return f"[Error decoding content: {e}]"


def _iter_json_objects(line: str) -> Optional[list]:
    """Parse one or more JSON objects from a line.

    Returns a list of objects, or None if parsing fails.
    """
    decoder = json.JSONDecoder()
    idx = 0
    end = len(line)
    objects = []
    while idx < end:
        while idx < end and line[idx].isspace():
            idx += 1
        if idx >= end:
            break
        try:
            obj, next_idx = decoder.raw_decode(line, idx)
        except json.JSONDecodeError:
            return None
        objects.append(obj)
        idx = next_idx
    return objects


def _format_tool_use_block(block: dict) -> list:
    """Format a tool_use block as markdown lines."""
    tool_name = block.get("name", "unknown")
    tool_id = block.get("id", "")
    tool_input = block.get("input", {})

    lines = [f"\n**[Tool Use: {tool_name}]**"]
    if tool_id:
        lines.append(f"Tool ID: `{tool_id}`")
    lines.extend(["\nInput:", "```json", pretty_json(tool_input), "```\n"])
    return lines


def _extract_text_from_result_item(item) -> str:
    """Extract text from a single result item.

    Tool result content can be either dict items with 'text' key
    or primitive values that need string conversion.

    Args:
        item: A result item from tool_result content list

    Returns:
        Text content as string
    """
    if isinstance(item, dict):
        return item.get("text", "")
    return str(item)


def _format_tool_result_block(block: dict) -> list:
    """Format a tool_result block as markdown lines."""
    tool_use_id = block.get("tool_use_id", "")
    is_error = block.get("is_error", False)
    result_content = block.get("content", "")

    # Handle both string and list content in tool results
    if isinstance(result_content, list):
        result_text = "\n".join(_extract_text_from_result_item(item) for item in result_content)
    else:
        result_text = result_content

    status = "ERROR" if is_error else "Success"
    lines = [f"\n**[Tool Result: {status}]**"]
    if tool_use_id:
        lines.append(f"Tool Use ID: `{tool_use_id}`")
    lines.extend(["\n```", result_text, "```\n"])
    return lines


def extract_content(message_obj: dict) -> str:
    """Extract text content from message object, preserving all information.

    Claude API messages contain different content structures:
    - User messages: Simple string content
    - Assistant messages: Array of content blocks (text, tool_use, tool_result)

    Args:
        message_obj: Message dictionary from JSONL entry

    Returns:
        Markdown-formatted string of all message content.
    """
    if "content" not in message_obj:
        return "[No content]"

    content = message_obj["content"]

    # User messages have simple string content
    if isinstance(content, str):
        return content

    # Assistant messages have array of content blocks
    if not isinstance(content, list):
        return "[No content]"

    content_parts = []
    for block in content:
        block_type = block.get("type")
        if block_type == "text":
            content_parts.append(block.get("text", ""))
        elif block_type == "tool_use":
            content_parts.extend(_format_tool_use_block(block))
        elif block_type == "tool_result":
            content_parts.extend(_format_tool_result_block(block))

    return "\n".join(content_parts) if content_parts else "[No content]"


def get_first_timestamp(jsonl_file: Path) -> Optional[str]:
    """Extract the first message timestamp from a .jsonl file.

    Scans the file line by line looking for the first user or assistant
    message with a timestamp. Returns None if file cannot be read or
    contains no valid messages.

    Args:
        jsonl_file: Path to JSONL file

    Returns:
        ISO 8601 timestamp string or None if not found or file cannot be read.
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    entry = json.loads(line)
                    entry_type = entry.get("type")
                    if entry_type in ("user", "assistant"):
                        timestamp = entry.get("timestamp", "")
                        if timestamp:
                            return timestamp
                except json.JSONDecodeError as e:
                    # Skip malformed lines with debug info (NO-BARE-EXCEPT)
                    if os.environ.get("DEBUG"):
                        sys.stderr.write(
                            f"Warning: Malformed JSON at {jsonl_file}:{line_num}: {e}\n"
                        )
                    continue
    except OSError as e:
        # Log I/O errors in debug mode (NO-BARE-EXCEPT)
        if os.environ.get("DEBUG"):
            sys.stderr.write(f"Warning: Cannot read {jsonl_file}: {e}\n")
    return None


def estimate_message_lines(msg_content: str, has_metadata: bool) -> int:
    """Estimate number of lines a message will take in markdown."""
    lines = 0
    lines += 1  # Message header (## Message N)
    lines += 1  # Empty line
    lines += 1  # Timestamp
    lines += 1  # Empty line
    lines += len(msg_content.split("\n"))  # Content
    lines += 1  # Empty line
    if has_metadata:
        lines += METADATA_LINES_ESTIMATE
        lines += 1  # Empty line
    lines += 1  # Separator (---)
    lines += 1  # Empty line
    return lines


def is_tool_result_message(msg_content: str) -> bool:
    """Check if message content contains a tool result.

    Args:
        msg_content: Message content string (formatted as markdown)

    Returns:
        True if content contains a tool result block.
    """
    return TOOL_RESULT_MARKER in msg_content


def extract_tool_name_from_content(content: str) -> str:
    """Extract tool name from formatted tool call content.

    NO-REGEX-PARSE: Uses compiled regex instead of multiple string splits.

    Args:
        content: Formatted content string like "**[Tool: Bash]**..."

    Returns:
        Tool name or "unknown" if not found
    """
    match = _TOOL_NAME_PATTERN.search(content)
    return match.group(1).strip() if match else "unknown"


def _parse_timestamp_safe(timestamp_str: str) -> Optional[datetime]:
    """Parse ISO 8601 timestamp, handling Z suffix.

    Args:
        timestamp_str: ISO 8601 timestamp string (may end with 'Z')

    Returns:
        datetime object or None on error
    """
    if not timestamp_str:
        return None
    try:
        return datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return None


def _truncate_hash(hash_string: str, max_len: int = HASH_DISPLAY_LEN) -> str:
    """Truncate hash to display length if needed.

    Args:
        hash_string: The hash string to truncate
        max_len: Maximum length (default: HASH_DISPLAY_LEN)

    Returns:
        Truncated hash if longer than max_len, otherwise original
    """
    if len(hash_string) > max_len:
        return hash_string[:max_len]
    return hash_string


def calculate_time_gap(msg1: dict, msg2: dict) -> float:
    """Calculate time gap in seconds between two messages.

    Used by the conversation splitting algorithm to identify natural
    break points - longer gaps suggest topic changes or session breaks.

    Args:
        msg1: First message dictionary (must have 'timestamp' key)
        msg2: Second message dictionary (must have 'timestamp' key)

    Returns:
        Absolute time difference in seconds between messages.
        Returns 0 if timestamps are missing or malformed, which
        causes the split algorithm to rely on other signals.
    """
    ts1 = _parse_timestamp_safe(msg1.get("timestamp", ""))
    ts2 = _parse_timestamp_safe(msg2.get("timestamp", ""))

    if ts1 is None or ts2 is None:
        return 0

    return abs((ts2 - ts1).total_seconds())


def _calculate_split_score(
    messages: list, index: int, msg: dict, current_lines: int, target_lines: int
) -> float:
    """Calculate score for splitting after message at index."""
    score = 0

    # Priority 1: Next message is User (best - starting new topic)
    if index + 1 < len(messages) and messages[index + 1]["role"] == "user":
        score += SCORE_USER_MESSAGE_NEXT

    # Priority 2: Current message is tool result (action complete)
    if is_tool_result_message(msg["content"]):
        score += SCORE_TOOL_RESULT

    # Priority 3: Time gap before next message
    if index + 1 < len(messages):
        time_gap = calculate_time_gap(msg, messages[index + 1])
        if time_gap > TIME_GAP_LARGE:
            score += SCORE_TIME_GAP_LARGE
        elif time_gap > TIME_GAP_MEDIUM:
            score += SCORE_TIME_GAP_MEDIUM

    # Slight preference for being closer to target
    score -= abs(current_lines - target_lines) * SCORE_DISTANCE_PENALTY
    return score


def find_best_split_point(messages: list, target_lines: int, minimal: bool) -> Optional[int]:
    """Find the optimal message index to split a conversation."""
    min_lines = int(target_lines * SPLIT_MIN_FACTOR)
    max_lines = int(target_lines * SPLIT_MAX_FACTOR)

    current_lines = HEADER_LINES_ESTIMATE
    best_split = None
    best_score = -1

    for i, msg in enumerate(messages):
        current_lines += estimate_message_lines(msg["content"], not minimal)

        if current_lines > max_lines:
            return i if i > 0 else 1

        if current_lines >= min_lines:
            score = _calculate_split_score(messages, i, msg, current_lines, target_lines)
            if score > best_score:
                best_score = score
                best_split = i + 1

    return best_split


def _build_message_dict(
    entry: dict, message_obj: dict, role: str, content: str, timestamp: str
) -> dict:
    """Build a message dictionary from entry and message object.

    Args:
        entry: The JSONL entry dict with metadata
        message_obj: The message object from the entry
        role: Message role ("user" or "assistant")
        content: Extracted message content
        timestamp: ISO 8601 timestamp string

    Returns:
        Message dict with all preserved metadata
    """
    return {
        "role": role,
        "content": content,
        "timestamp": timestamp,
        "uuid": entry.get("uuid", ""),
        "parentUuid": entry.get("parentUuid"),
        "sessionId": entry.get("sessionId", ""),
        "agentId": entry.get("agentId"),
        "requestId": entry.get("requestId"),
        "cwd": entry.get("cwd", ""),
        "version": entry.get("version", ""),
        "gitBranch": entry.get("gitBranch"),
        "isSidechain": entry.get("isSidechain"),
        "userType": entry.get("userType"),
        "model": message_obj.get("model"),
        "usage": message_obj.get("usage"),
        "stop_reason": message_obj.get("stop_reason"),
        "stop_sequence": message_obj.get("stop_sequence"),
    }


def read_jsonl_messages(jsonl_file: Path, quiet: bool = False):
    """Read and parse all messages from a Claude Code JSONL session file.

    Parses each line of the JSONL file, extracting user and assistant
    messages along with their metadata. Invalid JSON lines are skipped
    with a warning.

    Args:
        jsonl_file: Path to the .jsonl session file

    Returns:
        List of message dicts containing:
        - role: "user" or "assistant"
        - content: Extracted and formatted message content
        - timestamp: ISO 8601 timestamp string
        - uuid, parentUuid, sessionId, agentId: Message identifiers
        - cwd, version, gitBranch: Context information
        - model, usage, stop_reason: Assistant message metadata
    """
    messages = []

    def handle_entry(entry: dict) -> None:
        entry_type = entry.get("type")
        if entry_type in ("user", "assistant"):
            message_obj = entry.get("message", {})
            role = message_obj.get("role", entry_type)
            timestamp = entry.get("timestamp", "")
            content = extract_content(message_obj)

            messages.append(_build_message_dict(entry, message_obj, role, content, timestamp))

    with open(jsonl_file, encoding="utf-8") as f:
        for line in f:
            try:
                handle_entry(json.loads(line))
            except json.JSONDecodeError as e:
                parsed = _iter_json_objects(line)
                if not parsed:
                    stripped = line.lstrip()
                    if stripped.startswith("{") or stripped.startswith("["):
                        if not quiet:
                            print(
                                f"Warning: Couldn't parse line in {jsonl_file.name}: {e}",
                                file=sys.stderr,
                            )
                    continue
                for entry in parsed:
                    if isinstance(entry, dict):
                        handle_entry(entry)

    return messages


def _generate_message_section(
    msg: dict, msg_num: int, is_agent: bool, minimal: bool, uuid_to_index: dict
) -> list:
    """Generate markdown section for a single message."""
    lines = []

    # Determine message label
    if is_agent and msg_num == 1 and msg["role"] == "user":
        role_label = "Task Prompt (from Parent Claude)"
    else:
        role_label = msg["role"].title()

    # Add HTML anchor (skip in minimal mode)
    if not minimal and msg.get("uuid"):
        lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
        lines.append("")

    lines.append(f"## Message {msg_num} - {role_label}")
    lines.append("")
    lines.append(f"*{msg['timestamp']}*")
    lines.append("")
    lines.append(msg["content"])
    lines.append("")

    # Add metadata section (skip in minimal mode) - uses consolidated function (H5)
    if not minimal:
        lines.extend(_generate_message_metadata(msg, uuid_to_index))

    lines.append("---")
    lines.append("")
    return lines


def _generate_markdown_file_header(
    jsonl_file: Path, messages: list, is_agent: bool, display_file: Optional[str] = None
) -> list:
    """Generate the file header section of markdown."""
    lines = ["# Claude Conversation (Agent)" if is_agent else "# Claude Conversation", ""]
    lines.append(f"**File:** {display_file or jsonl_file.name}")
    lines.append(f"**Messages:** {len(messages)}")
    if messages:
        lines.append(f"**First message:** {messages[0]['timestamp']}")
        lines.append(f"**Last message:** {messages[-1]['timestamp']}")
    return lines


def _generate_agent_conversation_notice(parent_session_id: str, agent_id: str) -> list:
    """Generate the agent conversation notice section."""
    lines = [
        "",
        "> **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.",
        ">",
        "> - Messages labeled 'User' represent task instructions from the parent Claude session",
        "> - Messages labeled 'Assistant' are responses from this agent",
    ]
    if parent_session_id:
        lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
    if agent_id:
        lines.append(f"> - **Agent ID:** `{agent_id}`")
    return lines


def _get_agent_info(messages: list) -> tuple:
    """Extract agent info from messages. Returns (is_agent, parent_session_id, agent_id)."""
    is_agent = any(msg.get("isSidechain") for msg in messages)
    if is_agent and messages:
        return is_agent, messages[0].get("sessionId"), messages[0].get("agentId")
    return False, None, None


def parse_jsonl_to_markdown(
    jsonl_file: Path,
    minimal: bool = False,
    messages: Optional[list] = None,
    display_file: Optional[str] = None,
) -> str:
    """Convert a Claude Code JSONL session file to readable Markdown."""
    if messages is None:
        messages = read_jsonl_messages(jsonl_file)

    is_agent, parent_session_id, agent_id = _get_agent_info(messages)

    md_lines = _generate_markdown_file_header(jsonl_file, messages, is_agent, display_file)
    if is_agent:
        md_lines.extend(_generate_agent_conversation_notice(parent_session_id, agent_id))
    md_lines.extend(["", "---", ""])

    uuid_to_index = {msg["uuid"]: i for i, msg in enumerate(messages, 1) if msg.get("uuid")}
    for i, msg in enumerate(messages, 1):
        md_lines.extend(_generate_message_section(msg, i, is_agent, minimal, uuid_to_index))

    return "\n".join(md_lines)


def generate_markdown_parts(
    messages, jsonl_file: Path, minimal: bool, split_lines: int, display_file: Optional[str] = None
):
    """Generate multiple markdown parts from messages, split at smart break points.

    Returns list of (part_num, total_parts, markdown_content, start_msg, end_msg) tuples.
    """
    if not split_lines or len(messages) == 0:
        return None

    # Find all split points
    split_points = [0]  # Start with message 0
    remaining_messages = messages

    while True:
        # Find next split point in remaining messages
        split_idx = find_best_split_point(remaining_messages, split_lines, minimal)

        if split_idx is None or split_idx >= len(remaining_messages):
            # No more splits needed
            break

        # Add this split point (adjusted for global message index)
        global_idx = split_points[-1] + split_idx
        split_points.append(global_idx)
        remaining_messages = messages[global_idx:]

    # Add end point
    split_points.append(len(messages))

    # If only one part, no splitting needed
    if len(split_points) <= MIN_SPLIT_POINTS:
        return None

    total_parts = len(split_points) - 1
    parts = []

    for part_num in range(total_parts):
        start_idx = split_points[part_num]
        end_idx = split_points[part_num + 1]
        part_messages = messages[start_idx:end_idx]

        # Generate markdown for this part
        part_md = generate_markdown_for_messages(
            part_messages,
            jsonl_file,
            minimal,
            part_num=part_num + 1,
            total_parts=total_parts,
            start_msg_num=start_idx + 1,
            end_msg_num=end_idx,
            display_file=display_file,
        )

        parts.append((part_num + 1, total_parts, part_md, start_idx + 1, end_idx))

    return parts


def _generate_markdown_header(
    jsonl_file,
    part_messages,
    is_agent,
    part_num,
    total_parts,
    start_msg_num,
    end_msg_num,
    display_file: Optional[str] = None,
):
    """Generate markdown header section."""
    md_lines = []

    # Title
    title_base = "Claude Conversation (Agent)" if is_agent else "Claude Conversation"
    if part_num and total_parts:
        md_lines.append(f"# {title_base} - Part {part_num} of {total_parts}")
    else:
        md_lines.append(f"# {title_base}")

    md_lines.append("")
    md_lines.append(f"**File:** {display_file or jsonl_file.name}")

    if part_num and total_parts:
        md_lines.append(f"**Part:** {part_num} of {total_parts}")
        md_lines.append(
            f"**Messages in this part:** {len(part_messages)} (#{start_msg_num}-#{end_msg_num})"
        )
    else:
        md_lines.append(f"**Messages:** {len(part_messages)}")

    if part_messages:
        md_lines.append(f"**First message:** {part_messages[0]['timestamp']}")
        md_lines.append(f"**Last message:** {part_messages[-1]['timestamp']}")

    return md_lines


def _add_metadata_field(
    lines: list[str],
    msg: dict,
    key: str,
    label: str,
    format_str: str = "- **{label}:** `{value}`",
) -> None:
    """Add an optional metadata field to lines.

    NO-NESTED-FUNCTION: Extracted from _generate_message_metadata.

    Args:
        lines: List to append formatted field to
        msg: Message dictionary
        key: Key to extract from message
        label: Display label for the field
        format_str: Format string with {label} and {value} placeholders
    """
    value = msg.get(key)
    if value is not None:
        lines.append(format_str.format(label=label, value=value))


def _generate_message_metadata(msg: dict, uuid_to_index: dict) -> list[str]:
    """Generate metadata section for a message.

    Args:
        msg: Message dictionary
        uuid_to_index: Mapping of UUIDs to message numbers

    Returns:
        List of markdown lines for metadata section
    """
    md_lines = ["### Metadata", ""]

    _add_metadata_field(md_lines, msg, "uuid", "UUID")

    # Parent UUID with link
    parent_uuid = msg.get("parentUuid")
    if parent_uuid:
        if parent_uuid in uuid_to_index:
            parent_msg_num = uuid_to_index[parent_uuid]
            md_lines.append(
                f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*"
            )
        else:
            md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")

    _add_metadata_field(md_lines, msg, "sessionId", "Session ID")
    _add_metadata_field(md_lines, msg, "agentId", "Agent ID")
    _add_metadata_field(md_lines, msg, "requestId", "Request ID")
    _add_metadata_field(md_lines, msg, "cwd", "Working Directory")
    _add_metadata_field(md_lines, msg, "gitBranch", "Git Branch")
    _add_metadata_field(md_lines, msg, "version", "Version")
    _add_metadata_field(md_lines, msg, "userType", "User Type")

    if msg.get("isSidechain") is not None:
        md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

    _add_metadata_field(md_lines, msg, "model", "Model")
    _add_metadata_field(md_lines, msg, "stop_reason", "Stop Reason")
    _add_metadata_field(md_lines, msg, "stop_sequence", "Stop Sequence")

    # Usage stats
    usage = msg.get("usage")
    if usage:
        md_lines.append("- **Usage:**")
        md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
        md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
        if usage.get("cache_creation_input_tokens"):
            md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
        if usage.get("cache_read_input_tokens"):
            md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

    md_lines.append("")
    return md_lines


def _get_role_label(msg: dict, is_agent: bool, index: int, start_msg_num: int) -> str:
    """Get display label for message role."""
    if is_agent and index == start_msg_num and msg["role"] == "user":
        return "Task Prompt (from Parent Claude)"
    return msg["role"].title()


def _generate_message_block(
    msg: dict, index: int, is_agent: bool, start_msg_num: int, minimal: bool, uuid_to_index: dict
) -> list:
    """Generate markdown lines for a single message."""
    lines = []
    role_label = _get_role_label(msg, is_agent, index, start_msg_num)

    if not minimal and msg.get("uuid"):
        lines.extend([f'<a name="msg-{msg["uuid"]}"></a>', ""])

    lines.extend(
        [f"## Message {index} - {role_label}", "", f"*{msg['timestamp']}*", "", msg["content"], ""]
    )

    if not minimal:
        lines.extend(_generate_message_metadata(msg, uuid_to_index))

    lines.extend(["---", ""])
    return lines


def _extract_agent_metadata(messages: list) -> tuple:
    """Extract agent metadata from messages.

    Args:
        messages: List of message dicts

    Returns:
        Tuple of (is_agent, parent_session_id, agent_id)
    """
    if not messages:
        return False, None, None

    is_agent = any(msg.get("isSidechain") for msg in messages)
    if not is_agent:
        return False, None, None

    first_msg = messages[0]
    return True, first_msg.get("sessionId"), first_msg.get("agentId")


def generate_markdown_for_messages(
    part_messages,
    jsonl_file,
    minimal,
    part_num=None,
    total_parts=None,
    start_msg_num=1,
    end_msg_num=None,
    display_file: Optional[str] = None,
):
    """Generate markdown for a subset of messages (used for splitting)."""
    is_agent, parent_session_id, agent_id = _extract_agent_metadata(part_messages)

    md_lines = _generate_markdown_header(
        jsonl_file,
        part_messages,
        is_agent,
        part_num,
        total_parts,
        start_msg_num,
        end_msg_num,
        display_file=display_file,
    )

    if is_agent:
        md_lines.extend(_generate_agent_conversation_notice(parent_session_id, agent_id))

    md_lines.extend(["", "---", ""])

    # Build UUID to message index map
    uuid_to_index = {
        msg["uuid"]: i for i, msg in enumerate(part_messages, start_msg_num) if msg.get("uuid")
    }

    # Generate messages
    for i, msg in enumerate(part_messages, start_msg_num):
        md_lines.extend(
            _generate_message_block(msg, i, is_agent, start_msg_num, minimal, uuid_to_index)
        )

    return "\n".join(md_lines)


# ============================================================================
# Codex Backend - JSONL Parsing
# ============================================================================


def codex_extract_content(payload: dict) -> str:
    """Extract text content from Codex message payload.

    Codex messages have content as either a string or an array of objects
    with type "input_text" (user) or "output_text" (assistant).

    Args:
        payload: The message payload dict containing content field

    Returns:
        Extracted text content as a string
    """
    content = payload.get("content", [])
    if isinstance(content, str):
        return content
    parts = []
    for item in content:
        if isinstance(item, dict) and item.get("type") in CODEX_TEXT_TYPES:
            parts.append(item.get("text", ""))
    return "\n".join(parts)


def codex_format_function_call(payload: dict) -> str:
    """Format a Codex function_call payload as markdown.

    Args:
        payload: The function_call payload dict

    Returns:
        Formatted markdown string for the tool call
    """
    name = payload.get("name", "unknown")
    args = payload.get("arguments", "{}")
    call_id = payload.get("call_id", "")
    return f"**[Tool: {name}]**\nCall ID: `{call_id}`\n```json\n{args}\n```"


def codex_format_function_result(payload: dict) -> str:
    """Format a Codex function_call_output payload as markdown.

    Args:
        payload: The function_call_output payload dict

    Returns:
        Formatted markdown string for the tool result
    """
    call_id = payload.get("call_id", "")
    output = payload.get("output", "")
    return f"**[Tool Result]**\nCall ID: `{call_id}`\n```\n{output}\n```"


def codex_read_jsonl_messages(jsonl_file: Path) -> tuple:
    """Read messages from Codex rollout JSONL file.

    Parses the Codex JSONL format which uses a {timestamp, type, payload}
    envelope structure. Extracts session metadata and all messages including
    function calls and results.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Tuple of (messages_list, session_meta_dict or None)
        Messages contain: role, content, timestamp, and optionally
        is_tool_call or is_tool_result flags
    """
    messages = []
    session_meta = None

    with open(jsonl_file, encoding="utf-8") as f:
        for line in f:
            try:
                entry = json.loads(line)
                entry_type = entry.get("type")
                timestamp = entry.get("timestamp", "")
                payload = entry.get("payload", {})

                if entry_type == "session_meta":
                    session_meta = payload
                elif entry_type == "response_item":
                    payload_type = payload.get("type")
                    if payload_type == "message":
                        messages.append(
                            {
                                "role": payload.get("role"),
                                "content": codex_extract_content(payload),
                                "timestamp": timestamp,
                            }
                        )
                    elif payload_type in ("function_call", "custom_tool_call"):
                        messages.append(
                            {
                                "role": "assistant",
                                "content": codex_format_function_call(payload),
                                "timestamp": timestamp,
                                "is_tool_call": True,
                            }
                        )
                    elif payload_type in ("function_call_output", "custom_tool_call_output"):
                        messages.append(
                            {
                                "role": "tool",
                                "content": codex_format_function_result(payload),
                                "timestamp": timestamp,
                                "is_tool_result": True,
                            }
                        )
            except json.JSONDecodeError:
                continue

    return messages, session_meta


def codex_get_first_timestamp(jsonl_file: Path) -> Optional[str]:
    """Get timestamp from Codex session's session_meta line.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        ISO 8601 timestamp string or None if not found
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            first_line = f.readline()
            entry = json.loads(first_line)
            if entry.get("type") == "session_meta":
                return entry.get("timestamp", "")
    except (OSError, json.JSONDecodeError):
        pass
    return None


def codex_parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False) -> str:
    """Convert Codex rollout JSONL to markdown format.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file
        minimal: If True, omit metadata sections

    Returns:
        Markdown formatted string of the conversation
    """
    messages, session_meta = codex_read_jsonl_messages(jsonl_file)

    md_lines = ["# Codex Conversation", ""]

    if session_meta and not minimal:
        md_lines.extend(
            [
                "## Session Metadata",
                "",
                f"- **Session ID:** `{session_meta.get('id', 'unknown')}`",
                f"- **Working Directory:** `{session_meta.get('cwd', 'unknown')}`",
                f"- **CLI Version:** `{session_meta.get('cli_version', 'unknown')}`",
                f"- **Source:** `{session_meta.get('source', 'unknown')}`",
                "",
            ]
        )

    md_lines.extend(["---", ""])

    for i, msg in enumerate(messages, 1):
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        timestamp = msg.get("timestamp", "")

        if role == "user":
            md_lines.append(f"## User (Message {i})")
        elif role == "assistant":
            if msg.get("is_tool_call"):
                md_lines.append(f"## Tool Call (Message {i})")
            else:
                md_lines.append(f"## Assistant (Message {i})")
        elif role == "tool":
            md_lines.append(f"## Tool Result (Message {i})")
        else:
            md_lines.append(f"## {role.title()} (Message {i})")

        if timestamp and not minimal:
            md_lines.append(f"*{timestamp}*")

        md_lines.extend(["", content, "", "---", ""])

    return "\n".join(md_lines)


def codex_extract_metrics_from_jsonl(jsonl_file: Path) -> MetricsDict:  # noqa: C901
    """Extract metrics from Codex JSONL file for stats database.

    Mirror of extract_metrics_from_jsonl() for Codex format.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Dict with session, messages, and tool_uses data
    """
    messages, session_meta = codex_read_jsonl_messages(jsonl_file)

    session: SessionMetrics = {
        "id": session_meta.get("id") if session_meta else None,
        "cwd": session_meta.get("cwd") if session_meta else None,
        "cli_version": session_meta.get("cli_version") if session_meta else None,
        "model": None,
        "startTime": None,
        "lastUpdated": None,
    }
    metrics: MetricsDict = {
        "session": session,
        "messages": [],
        "tool_uses": [],
    }

    # Extract model and token usage (latest total_token_usage) from event stream.
    last_token_usage = None
    last_token_timestamp = None
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                except json.JSONDecodeError:
                    continue

                entry_type = entry.get("type")
                payload = entry.get("payload", {})

                if entry_type == "turn_context" and not metrics["session"]["model"]:
                    metrics["session"]["model"] = payload.get("model")
                    continue

                if entry_type == "event_msg" and payload.get("type") == "token_count":
                    info = payload.get("info") or {}
                    total_usage = info.get("total_token_usage")
                    if total_usage:
                        last_token_usage = total_usage
                        last_token_timestamp = entry.get("timestamp")
    except OSError:
        pass

    if last_token_usage:
        metrics["tokens_summary"] = {
            "input_tokens": last_token_usage.get("input_tokens", 0),
            "output_tokens": last_token_usage.get("output_tokens", 0)
            + last_token_usage.get("reasoning_output_tokens", 0),
            "cache_read_tokens": last_token_usage.get("cached_input_tokens", 0),
            "timestamp": last_token_timestamp,
        }

    for msg in messages:
        if msg.get("is_tool_call"):
            # Extract tool name using regex (NO-REGEX-PARSE fix)
            content = msg.get("content", "")
            tool_name = extract_tool_name_from_content(content)
            metrics["tool_uses"].append(
                {
                    "name": tool_name,
                    "timestamp": msg.get("timestamp"),
                }
            )
        elif not msg.get("is_tool_result"):
            metrics["messages"].append(
                {
                    "role": msg.get("role"),
                    "timestamp": msg.get("timestamp"),
                }
            )

    return metrics


# ============================================================================
# Codex Backend - Session Scanning
# ============================================================================


def codex_get_workspace_from_session(jsonl_file: Path) -> str:
    """Extract workspace (cwd) from Codex session's session_meta.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Workspace path from session_meta.cwd (e.g., '/home/user/project') or 'unknown'
    """
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            first_line = f.readline()
            entry = json.loads(first_line)
            if entry.get("type") == "session_meta":
                cwd = entry.get("payload", {}).get("cwd", "")
                if cwd:
                    return cwd
    except (OSError, json.JSONDecodeError):
        pass
    return "unknown"


def codex_get_workspace_readable(workspace: str) -> str:
    """Convert a Codex workspace identifier to a readable path string."""
    if not workspace:
        return ""
    # Raw paths are already readable; fall back to decoding if an encoded workspace leaks in.
    if workspace.startswith("-") or workspace.startswith("home-") or workspace.startswith("mnt-"):
        return normalize_workspace_name(workspace, verify_local=False)
    return workspace


def codex_count_messages(jsonl_file: Path) -> int:
    """Count user/assistant messages in a Codex session.

    Args:
        jsonl_file: Path to the Codex rollout .jsonl file

    Returns:
        Number of user and assistant messages (excluding tool calls/results)
    """
    count = 0
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    if entry.get("type") == "response_item":
                        payload = entry.get("payload", {})
                        if payload.get("type") == "message":
                            count += 1
                except json.JSONDecodeError:
                    continue
    except OSError:
        pass
    return count


# ============================================================================
# Codex Session Index - Incremental workspace mapping
# ============================================================================

CODEX_INDEX_VERSION = 3  # Bumped to rebuild index with raw cwd paths (not encoded)


def codex_get_index_file() -> Path:
    """Get path to Codex session index file (~/.agent-history/codex_index.json)."""
    return get_config_dir() / "codex_index.json"


def codex_load_index() -> dict[str, Any]:
    """Load Codex session index from file.

    Returns:
        Index dict with keys: version, last_scan_date, sessions
        sessions maps session file path (str) to encoded workspace name.
        Returns empty index if file doesn't exist or is invalid.
    """
    index_file = codex_get_index_file()
    default_index: dict[str, Any] = {
        "version": CODEX_INDEX_VERSION,
        "last_scan_date": None,
        "sessions": {},
    }

    if not index_file.exists():
        return default_index

    try:
        with open(index_file, encoding="utf-8") as f:
            data = json.load(f)

            # Version mismatch requires rebuild
            if data.get("version") != CODEX_INDEX_VERSION:
                if os.environ.get("DEBUG"):
                    sys.stderr.write(
                        f"Codex index version mismatch: "
                        f"expected {CODEX_INDEX_VERSION}, got {data.get('version')}\n"
                    )
                return default_index

            return data

    except OSError as e:
        if os.environ.get("DEBUG"):
            sys.stderr.write(f"Cannot read Codex index {index_file}: {e}\n")
    except json.JSONDecodeError as e:
        if os.environ.get("DEBUG"):
            sys.stderr.write(f"Invalid JSON in Codex index {index_file}: {e}\n")

    return default_index


def codex_save_index(index: dict) -> None:
    """Save Codex session index to file."""
    index_file = codex_get_index_file()
    try:
        index_file.parent.mkdir(parents=True, exist_ok=True)
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index, f, indent=2)
    except OSError as e:
        if os.environ.get("DEBUG"):
            sys.stderr.write(f"Cannot write Codex index {index_file}: {e}\n")


def _codex_parse_date_folder(folder_path: Path) -> str:
    """Parse YYYY/MM/DD folder structure to date string.

    Args:
        folder_path: Path like .../2025/12/15/rollout-xxx.jsonl

    Returns:
        Date string like "2025-12-15" or empty string if invalid
    """
    try:
        parts = folder_path.parts
        # Find YYYY/MM/DD pattern in path (last 4 parts before filename)
        if len(parts) >= CODEX_DATE_FOLDER_DEPTH:
            year, month, day = parts[-4], parts[-3], parts[-2]
            if year.isdigit() and month.isdigit() and day.isdigit():
                return f"{year}-{month}-{day}"
    except (ValueError, IndexError):
        pass
    return ""


def _iter_numeric_subdirs(parent: Path):
    """Iterate sorted numeric subdirectories of a parent directory."""
    for child in sorted(parent.iterdir()):
        if child.is_dir() and child.name.isdigit():
            yield child


def _is_date_before_cutoff(year: int, month: int, day: int, cutoff) -> bool:
    """Check if date is before cutoff date.

    M5: Extracted for testability.

    Args:
        year: Year component
        month: Month component
        day: Day component
        cutoff: Cutoff datetime or date (or None)

    Returns:
        True if the date is before cutoff
    """
    if not cutoff:
        return False
    from datetime import date as date_type

    folder_date = date_type(year, month, day)
    since = cutoff.date() if hasattr(cutoff, "date") else cutoff
    return folder_date < since


def _iter_day_folders(month_dir: Path, year: int, month: int, since_dt):
    """Generate day folders within a month, filtering by since_dt."""
    for day_dir in _iter_numeric_subdirs(month_dir):
        day = int(day_dir.name)
        if not _is_date_before_cutoff(year, month, day, since_dt):
            yield day_dir


def _iter_month_folders(year_dir: Path, year: int, since_dt):
    """Generate month folders within a year, filtering by since_dt."""
    for month_dir in _iter_numeric_subdirs(year_dir):
        month = int(month_dir.name)
        if since_dt and year == since_dt.year and month < since_dt.month:
            continue
        yield from _iter_day_folders(month_dir, year, month, since_dt)


def _iter_date_folders(sessions_dir: Path, since_dt):
    """Generate date folder paths, filtering by since_dt."""
    for year_dir in _iter_numeric_subdirs(sessions_dir):
        year = int(year_dir.name)
        if since_dt and year < since_dt.year:
            continue
        yield from _iter_month_folders(year_dir, year, since_dt)


def _codex_date_folders_since(sessions_dir: Path, since_date: Optional[str]) -> list:
    """Get list of date folders on or after since_date.

    Args:
        sessions_dir: Base sessions directory (~/.codex/sessions/)
        since_date: Date string "YYYY-MM-DD" to start from (inclusive)

    Returns:
        List of Path objects for YYYY/MM/DD folders to scan
    """
    if not sessions_dir.exists():
        return []

    since_dt = datetime.strptime(since_date, "%Y-%m-%d") if since_date else None
    return list(_iter_date_folders(sessions_dir, since_dt))


def _remove_stale_entries(sessions_map: dict) -> int:
    """Remove entries for files that no longer exist.

    Args:
        sessions_map: Dict mapping file paths to workspace names

    Returns:
        Number of stale entries removed
    """
    stale_keys = [k for k in sessions_map if not Path(k).exists()]
    for k in stale_keys:
        del sessions_map[k]
    return len(stale_keys)


def _scan_folders_for_sessions(
    folders: list[Path],
    existing_sessions: dict[str, str],
) -> dict[str, str]:
    """Scan folders and add new sessions to the map.

    Args:
        folders: List of date folders to scan
        existing_sessions: Current session->workspace mapping

    Returns:
        Updated session mapping (modifies in place and returns for chaining)
    """
    for day_dir in folders:
        for jsonl_file in day_dir.glob("rollout-*.jsonl"):
            file_key = str(jsonl_file)
            if file_key not in existing_sessions:
                workspace = codex_get_workspace_from_session(jsonl_file)
                existing_sessions[file_key] = workspace
    return existing_sessions


def codex_ensure_index_updated(sessions_dir: Optional[Path] = None) -> dict[str, str]:
    """Ensure Codex session index is up-to-date.

    Performs incremental indexing: only scans date folders since last update.

    Args:
        sessions_dir: Override sessions directory (for testing)

    Returns:
        Dict mapping session file paths (str) to encoded workspace names
    """
    sessions_dir = sessions_dir or codex_get_home_dir()

    if not sessions_dir.exists():
        return {}

    index = codex_load_index()
    sessions_map = index.get("sessions", {})

    # Clean up deleted files
    _remove_stale_entries(sessions_map)

    # Incremental scan from last scan date (or full scan if first run)
    folders = _codex_date_folders_since(sessions_dir, index.get("last_scan_date"))
    _scan_folders_for_sessions(folders, sessions_map)

    # Save updated index
    index["sessions"] = sessions_map
    index["last_scan_date"] = datetime.now().strftime("%Y-%m-%d")
    codex_save_index(index)

    return sessions_map


def _codex_build_session_dict(
    jsonl_file: Path,
    workspace: str,
    modified: datetime,
    skip_message_count: bool,
    use_cached_counts: bool = False,
) -> dict:
    """Build a session dictionary for a Codex session file."""
    message_count = 0
    if not skip_message_count:
        cached = None
        if use_cached_counts:
            cached = _get_cached_message_count(jsonl_file, modified.timestamp())
        message_count = cached if cached is not None else codex_count_messages(jsonl_file)
    return {
        "agent": AGENT_CODEX,
        "workspace": workspace,
        "workspace_readable": codex_get_workspace_readable(workspace),
        "file": jsonl_file,
        "filename": jsonl_file.name,
        "message_count": message_count,
        "message_count_skipped": skip_message_count,
        "modified": modified,
        "source": "local",
    }


def _codex_session_matches_filters(
    workspace: str,
    modified: datetime,
    pattern: str,
    since_date: Optional[datetime],
    until_date: Optional[datetime],
) -> bool:
    """Check if a Codex session matches the given filters.

    Uses shared _session_matches_filters for DRY implementation.
    """
    return _session_matches_filters(
        workspace,
        modified,
        pattern,
        since_date,
        until_date,
        get_readable=codex_get_workspace_readable,
    )


def codex_scan_sessions(
    pattern: str = "",
    since_date=None,
    until_date=None,
    sessions_dir: Optional[Path] = None,
    skip_message_count: bool = False,
    use_cached_counts: bool = False,
) -> list:
    """Scan ~/.codex/sessions/YYYY/MM/DD/ for rollout-*.jsonl files.

    Uses incremental indexing for efficient workspace lookups. The index maps
    session files to workspaces and is updated incrementally based on date folders.

    Args:
        pattern: Substring pattern to filter workspaces (empty matches all)
        since_date: Only include sessions modified on or after this date
        until_date: Only include sessions modified on or before this date
        sessions_dir: Override sessions directory (for testing)
        skip_message_count: If True, skip counting messages (set to 0)

    Returns:
        List of session dicts sorted by modified time (newest first)
    """
    if sessions_dir is None:
        sessions_dir = codex_get_home_dir()

    if not sessions_dir.exists():
        return []

    # Get workspace mapping from incremental index
    sessions_map = codex_ensure_index_updated(sessions_dir)

    sessions = []
    # Walk through YYYY/MM/DD structure using glob
    for jsonl_file in sessions_dir.glob("*/*/*/rollout-*.jsonl"):
        file_key = str(jsonl_file)
        # Look up workspace from index (fallback to file read if not in index or empty)
        workspace = sessions_map.get(file_key)
        if not workspace:  # None or empty string
            workspace = codex_get_workspace_from_session(jsonl_file)
            # Update index with recomputed workspace if we had to fall back
            if workspace and file_key in sessions_map:
                sessions_map[file_key] = workspace

        modified = datetime.fromtimestamp(jsonl_file.stat().st_mtime)

        if _codex_session_matches_filters(workspace, modified, pattern, since_date, until_date):
            sessions.append(
                _codex_build_session_dict(
                    jsonl_file,
                    workspace,
                    modified,
                    skip_message_count,
                    use_cached_counts=use_cached_counts,
                )
            )

    return sorted(sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Gemini Backend - JSON Parsing
# ============================================================================


def _extract_gemini_content_part(part) -> str:
    """Extract text from a single Gemini content part."""
    if isinstance(part, str):
        return part
    if not isinstance(part, dict):
        return ""

    if "text" in part:
        return part["text"]
    if "inlineData" in part:
        mime = part["inlineData"].get("mimeType", "unknown")
        return f"[Inline data: {mime}]"
    if "executableCode" in part:
        code = part["executableCode"]
        lang = code.get("language", "")
        return f"```{lang}\n{code.get('code', '')}\n```"
    if "codeExecutionResult" in part:
        result = part["codeExecutionResult"]
        return f"**Output:**\n```\n{result.get('output', '')}\n```"
    return ""


def _extract_gemini_content(content) -> str:
    """Extract text content from Gemini message content field."""
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        parts = [_extract_gemini_content_part(p) for p in content]
        return "\n".join(p for p in parts if p)
    return ""


def _build_gemini_message(msg: dict, content: str) -> Optional[dict]:
    """Build a normalized message dict from Gemini message data."""
    msg_type = msg.get("type", "")
    timestamp = msg.get("timestamp", "")

    if msg_type == "user":
        return {"role": "user", "content": content, "timestamp": timestamp}
    if msg_type == "gemini":
        return {
            "role": "assistant",
            "content": content,
            "timestamp": timestamp,
            "thoughts": msg.get("thoughts", []),
            "tokens": msg.get("tokens"),
            "model": msg.get("model"),
            "tool_calls": msg.get("toolCalls", []),
        }
    if msg_type in ("info", "error", "warning"):
        return {"role": msg_type, "content": content, "timestamp": timestamp}
    return None


def gemini_read_json_messages(json_file: Path) -> tuple:
    """Read messages from Gemini CLI JSON session file.

    Gemini stores sessions as single JSON files (not JSONL) with structure:
    {sessionId, projectHash, startTime, lastUpdated, messages: [...]}

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Tuple of (messages_list, session_meta_dict or None)
        Messages contain: role, content, timestamp, and optionally
        is_tool_call, thoughts, tokens, model fields
    """
    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
    except (OSError, json.JSONDecodeError):
        return [], None

    session_meta = {
        "sessionId": data.get("sessionId"),
        "projectHash": data.get("projectHash"),
        "startTime": data.get("startTime"),
        "lastUpdated": data.get("lastUpdated"),
        "summary": data.get("summary"),
    }

    messages = []
    for msg in data.get("messages", []):
        content = _extract_gemini_content(msg.get("content", ""))
        normalized = _build_gemini_message(msg, content)
        if normalized:
            messages.append(normalized)

    return messages, session_meta


def gemini_format_tool_call(tool_call: dict) -> str:
    """Format a Gemini tool call as markdown.

    Args:
        tool_call: Tool call dict with id, name, args, result, status, etc.

    Returns:
        Formatted markdown string for the tool call
    """
    name = tool_call.get("displayName") or tool_call.get("name", "unknown")
    args = tool_call.get("args", {})
    status = tool_call.get("status", "")
    result = tool_call.get("result", [])

    lines = [f"**[Tool: {name}]** ({status})"]

    if args:
        try:
            args_str = pretty_json(args)
        except (TypeError, ValueError):
            args_str = str(args)
        lines.append(f"```json\n{args_str}\n```")

    # Extract result output if present
    if result:
        for r in result:
            if isinstance(r, dict):
                func_resp = r.get("functionResponse", {})
                output = func_resp.get("response", {}).get("output", "")
                if output:
                    # Use helper for consistent truncation (M4)
                    output = _truncate_tool_output(output)
                    lines.append(f"**Result:**\n```\n{output}\n```")

    return "\n".join(lines)


def gemini_format_thoughts(thoughts: list) -> str:
    """Format Gemini reasoning thoughts as markdown.

    Args:
        thoughts: List of thought dicts with subject, description, timestamp
                  (or strings for legacy/alternative formats)

    Returns:
        Formatted markdown string for the thoughts
    """
    if not thoughts:
        return ""

    lines = ["", "**Reasoning:**"]
    for thought in thoughts:
        # Handle both dict format and potential string format
        if isinstance(thought, str):
            if thought.strip():
                text = _truncate_thought(thought)
                lines.append(f"- {text}")
        elif isinstance(thought, dict):
            subject = thought.get("subject", "")
            description = thought.get("description", "")
            desc_text = _truncate_thought(description)
            if subject:
                lines.append(f"- **{subject}**: {desc_text}")
            elif description:
                lines.append(f"- {desc_text}")
        # Skip other types silently

    return "\n".join(lines)


def gemini_get_first_timestamp(json_file: Path) -> Optional[str]:
    """Get startTime from Gemini session.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        ISO 8601 timestamp string or None if not found
    """
    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
            return data.get("startTime", "")
    except (OSError, json.JSONDecodeError):
        pass
    return None


def _gemini_format_session_metadata(session_meta: dict) -> list:
    """Format Gemini session metadata as markdown lines."""
    project_hash = session_meta.get("projectHash", "unknown")
    short_hash = _truncate_hash(project_hash)
    lines = [
        "## Session Metadata",
        "",
        f"- **Session ID:** `{session_meta.get('sessionId', 'unknown')}`",
        f"- **Project Hash:** `{short_hash}...`",
        f"- **Start Time:** `{session_meta.get('startTime', 'unknown')}`",
        f"- **Last Updated:** `{session_meta.get('lastUpdated', 'unknown')}`",
        "",
    ]
    if session_meta.get("summary"):
        lines.extend(["### Summary", "", session_meta["summary"], ""])
    return lines


def _gemini_get_role_header(role: str, msg_num: int) -> str:
    """Get the markdown header for a message role."""
    name = _GEMINI_ROLE_DISPLAY_NAMES.get(role, role.title())
    return f"## {name} (Message {msg_num})"


def _gemini_format_message(msg: dict, msg_num: int, minimal: bool) -> list:
    """Format a single Gemini message as markdown lines."""
    lines = [_gemini_get_role_header(msg.get("role", "unknown"), msg_num)]

    if not minimal:
        if msg.get("timestamp"):
            lines.append(f"*{msg['timestamp']}*")
        if msg.get("model"):
            lines.append(f"*Model: {msg['model']}*")

    lines.append("")

    if msg.get("thoughts") and not minimal:
        lines.extend([gemini_format_thoughts(msg["thoughts"]), ""])

    if msg.get("content"):
        lines.extend([msg["content"], ""])

    for tc in msg.get("tool_calls", []):
        lines.extend([gemini_format_tool_call(tc), ""])

    if msg.get("tokens") and not minimal:
        tokens = msg["tokens"]
        lines.extend(
            [
                f"*Tokens: {tokens.get('total', 0)} total "
                f"(in: {tokens.get('input', 0)}, out: {tokens.get('output', 0)}, "
                f"cached: {tokens.get('cached', 0)})*",
                "",
            ]
        )

    lines.extend(["---", ""])
    return lines


def gemini_parse_json_to_markdown(json_file: Path, minimal: bool = False) -> str:
    """Convert Gemini session JSON to markdown format.

    Args:
        json_file: Path to the Gemini session .json file
        minimal: If True, omit metadata sections

    Returns:
        Markdown formatted string of the conversation
    """
    messages, session_meta = gemini_read_json_messages(json_file)
    md_lines = ["# Gemini Conversation", ""]

    if session_meta and not minimal:
        md_lines.extend(_gemini_format_session_metadata(session_meta))

    md_lines.extend(["---", ""])

    for i, msg in enumerate(messages, 1):
        md_lines.extend(_gemini_format_message(msg, i, minimal))

    return "\n".join(md_lines)


def gemini_extract_metrics_from_json(json_file: Path) -> MetricsDict:
    """Extract metrics from Gemini JSON file for stats database.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Dict with session, messages, and tool_uses data
    """
    messages, session_meta = gemini_read_json_messages(json_file)

    metrics: MetricsDict = {
        "session": {
            "id": session_meta.get("sessionId") if session_meta else None,
            "cwd": session_meta.get("projectHash")
            if session_meta
            else None,  # Use hash as workspace
            "cli_version": None,  # Gemini doesn't store CLI version in session
            "model": None,
            "startTime": session_meta.get("startTime") if session_meta else None,
            "lastUpdated": session_meta.get("lastUpdated") if session_meta else None,
        },
        "messages": [],
        "tool_uses": [],
        "tokens": {
            "input": 0,
            "output": 0,
            "total": 0,
        },
    }

    for msg in messages:
        role = msg.get("role")
        timestamp = msg.get("timestamp")

        # Extract model from first assistant message
        if role == "assistant" and not metrics["session"]["model"]:
            metrics["session"]["model"] = msg.get("model")

        # Accumulate token usage
        tokens = msg.get("tokens")
        if tokens:
            metrics["tokens"]["input"] += tokens.get("input", 0)
            metrics["tokens"]["output"] += tokens.get("output", 0)
            metrics["tokens"]["total"] += tokens.get("total", 0)

        # Track tool calls
        for tc in msg.get("tool_calls", []):
            status = tc.get("status", "")
            metrics["tool_uses"].append(
                {
                    "name": tc.get("name", "unknown"),
                    "timestamp": tc.get("timestamp") or timestamp,
                    "is_error": status.lower() in ("error", "failed", "failure")
                    if status
                    else False,
                }
            )

        # Track messages (exclude system messages)
        if role in ("user", "assistant"):
            msg_data = {
                "role": role,
                "timestamp": timestamp,
                "model": msg.get("model"),
            }
            # Include per-message token usage for stats
            if tokens:
                msg_data["input_tokens"] = tokens.get("input", 0)
                msg_data["output_tokens"] = tokens.get("output", 0)
            metrics["messages"].append(msg_data)

    return metrics


# ============================================================================
# Gemini Hash Index - Progressive hash-to-path mapping
# ============================================================================

GEMINI_HASH_INDEX_VERSION = 1


def gemini_get_hash_index_file() -> Path:
    """Get path to Gemini hash index file (~/.agent-history/gemini_hash_index.json)."""
    return get_config_dir() / "gemini_hash_index.json"


def gemini_load_hash_index() -> dict:
    """Load Gemini hash-to-path index from file.

    Returns:
        Index dict with keys: version, hashes
        hashes maps project hash (str) to absolute path (str)
    """
    index_file = gemini_get_hash_index_file()
    if index_file.exists():
        try:
            with open(index_file, encoding="utf-8") as f:
                data = json.load(f)
                if data.get("version") == GEMINI_HASH_INDEX_VERSION:
                    return data
        except (OSError, json.JSONDecodeError):
            pass
    return {"version": GEMINI_HASH_INDEX_VERSION, "hashes": {}}


def gemini_save_hash_index(index: dict) -> None:
    """Save Gemini hash-to-path index to file."""
    index_file = gemini_get_hash_index_file()
    index_file.parent.mkdir(parents=True, exist_ok=True)
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(index, f, indent=2)


def gemini_compute_project_hash(path: Path) -> str:
    """Compute SHA-256 hash of a path, matching Gemini CLI's approach.

    Gemini CLI uses SHA-256 of the absolute path string as the project identifier.

    Args:
        path: Path to hash (will be resolved to absolute)

    Returns:
        SHA-256 hex digest of the absolute path string
    """
    abs_path = str(path.resolve())
    return hashlib.sha256(abs_path.encode("utf-8")).hexdigest()


def gemini_update_hash_index_from_cwd() -> dict:
    """Update Gemini hash index based on current working directory.

    Called on each agent-history run to progressively build hashâ†’path mapping.
    Checks if the current directory's hash exists in Gemini's session storage,
    and if so, records the mapping.

    Returns:
        Updated hash index dict
    """
    index = gemini_load_hash_index()
    cwd = Path.cwd()
    cwd_hash = gemini_compute_project_hash(cwd)

    # Check if this hash exists in Gemini's session storage
    gemini_dir = gemini_get_home_dir()
    hash_dir = gemini_dir / cwd_hash / "chats"

    if hash_dir.exists() and any(hash_dir.glob("session-*.json")):
        # Found Gemini sessions for this directory - record the mapping
        current_path = str(cwd.resolve())
        existing_path = index["hashes"].get(cwd_hash)

        if existing_path != current_path:
            index["hashes"][cwd_hash] = current_path
            gemini_save_hash_index(index)

    return index


def gemini_get_path_for_hash(project_hash: str) -> str | None:
    """Look up the real path for a Gemini project hash.

    Args:
        project_hash: SHA-256 hash of the project path

    Returns:
        Absolute path string if known, None otherwise
    """
    index = gemini_load_hash_index()
    return index["hashes"].get(project_hash)


def gemini_get_workspace_readable(workspace: str) -> str:
    """Get human-readable workspace name for a Gemini workspace identifier.

    Handles three cases:
    - Real path (starts with / or drive letter): return as-is
    - SHA-256 hash: looks up in hash index, falls back to truncated hash
    - Encoded path (like -home-user-project): normalizes to readable path

    Args:
        workspace: Real path, encoded workspace path, or SHA-256 hash

    Returns:
        Readable workspace name (path or [hash:xxxxxxxx])
    """
    # If it's already a real path, return it directly
    # Check for Unix paths (/) or Windows paths (drive letter like C:\)
    if workspace.startswith("/") or (len(workspace) > 1 and workspace[1] == ":"):
        return workspace

    # Try to look up as a hash in the index
    real_path = gemini_get_path_for_hash(workspace)
    if real_path:
        # Return the path directly - it's already readable
        return real_path

    # Fall back to truncated hash display
    if len(workspace) > HASH_DISPLAY_LEN:
        return f"[hash:{workspace[:HASH_DISPLAY_LEN]}]"
    return workspace


def _has_gemini_sessions(project_hash: str, gemini_dir: Path) -> bool:
    """Check if Gemini has sessions for this project hash.

    Args:
        project_hash: SHA-256 hash of project path
        gemini_dir: Gemini home directory

    Returns:
        True if sessions exist for this hash
    """
    hash_dir = gemini_dir / project_hash / "chats"
    return hash_dir.exists() and any(hash_dir.glob("session-*.json"))


def _build_path_mapping(
    project_str: str,
    project_hash: str,
    status: str,
    path_missing: bool,
) -> dict[str, Any]:
    """Build a path mapping result dict.

    Args:
        project_str: Resolved path string
        project_hash: SHA-256 hash of project path
        status: Status string ('added', 'existing', 'no_sessions')
        path_missing: Whether the path doesn't exist

    Returns:
        Mapping dict for the result
    """
    return {
        "path": project_str,
        "hash": project_hash[:HASH_DISPLAY_LEN],
        "status": status,
        "path_missing": path_missing,
    }


def _try_add_path_to_index(
    path: Path,
    index: dict[str, Any],
    gemini_dir: Path,
) -> tuple[str, dict[str, Any]]:
    """Try to add a single path to the index.

    Args:
        path: Path to add
        index: Current index dictionary
        gemini_dir: Gemini home directory

    Returns:
        Tuple of (status, mapping_dict)
    """
    resolved = path.resolve()
    project_hash = gemini_compute_project_hash(resolved)
    project_str = str(resolved)
    path_missing = not resolved.exists()

    # No sessions for this hash
    if not _has_gemini_sessions(project_hash, gemini_dir):
        return "no_sessions", _build_path_mapping(
            project_str, project_hash, "no_sessions", path_missing
        )

    # Already in index with same path
    existing = index["hashes"].get(project_hash)
    if existing == project_str:
        return "existing", _build_path_mapping(project_str, project_hash, "existing", path_missing)

    # Add to index
    index["hashes"][project_hash] = project_str
    return "added", _build_path_mapping(project_str, project_hash, "added", path_missing)


def gemini_add_paths_to_index(paths: list[Path]) -> HashIndexCounts:
    """Add explicit paths to the Gemini hashâ†’path index.

    For each path, computes its SHA-256 hash and checks if Gemini has
    sessions for that hash. If sessions exist, adds the mapping to the index.

    Args:
        paths: List of Path objects to add to the index

    Returns:
        Dict with 'added', 'existing', 'no_sessions' counts and 'mappings' list
    """
    if not paths:
        return {"added": 0, "existing": 0, "no_sessions": 0, "mappings": []}

    index = gemini_load_hash_index()
    gemini_dir = gemini_get_home_dir()

    # Process each path
    results = [_try_add_path_to_index(p, index, gemini_dir) for p in paths]

    # Aggregate results
    counts: HashIndexCounts = {
        "added": sum(1 for status, _ in results if status == "added"),
        "existing": sum(1 for status, _ in results if status == "existing"),
        "no_sessions": sum(1 for status, _ in results if status == "no_sessions"),
        "mappings": [mapping for _, mapping in results],
    }

    # Save if we added anything
    if counts["added"] > 0:
        gemini_save_hash_index(index)

    return counts


# ============================================================================
# Gemini Backend - Session Scanning
# ============================================================================


def gemini_get_workspace_from_session(json_file: Path) -> str:
    """Extract workspace identifier from Gemini session.

    Uses the hash index to get the real path if known, otherwise returns
    the hash. This ensures consistent workspace naming across scan/export/stats.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Encoded path (if hashâ†’path known) or project hash, or 'unknown'
    """
    # The file is in ~/.gemini/tmp/<project_hash>/chats/<session>.json
    # So we need to go up two levels to get the project hash
    try:
        chats_dir = json_file.parent  # chats/
        project_dir = chats_dir.parent  # <project_hash>/
        project_hash = project_dir.name

        # Try to get the real path from hash index
        real_path = gemini_get_path_for_hash(project_hash)
        if real_path:
            # Return real path directly - encoding would mangle hyphens in folder names
            return real_path
        return project_hash
    except (AttributeError, IndexError):
        pass
    return "unknown"


def gemini_count_messages(json_file: Path) -> int:
    """Count user/assistant messages in a Gemini session.

    Args:
        json_file: Path to the Gemini session .json file

    Returns:
        Number of user and gemini messages
    """
    count = 0
    try:
        with open(json_file, encoding="utf-8") as f:
            data = json.load(f)
            for msg in data.get("messages", []):
                if msg.get("type") in ("user", "gemini"):
                    count += 1
    except (OSError, json.JSONDecodeError):
        pass
    return count


def _gemini_build_session_dict(
    json_file: Path,
    workspace: str,
    modified: datetime,
    skip_message_count: bool,
    use_cached_counts: bool = False,
) -> dict:
    """Build a session dictionary for a Gemini session file."""
    # Use hash index to get readable workspace name if known
    workspace_readable = gemini_get_workspace_readable(workspace)
    message_count = 0
    if not skip_message_count:
        cached = None
        if use_cached_counts:
            cached = _get_cached_message_count(json_file, modified.timestamp())
        message_count = cached if cached is not None else gemini_count_messages(json_file)
    return {
        "agent": AGENT_GEMINI,
        "workspace": workspace,
        "workspace_readable": workspace_readable,
        "file": json_file,
        "filename": json_file.name,
        "message_count": message_count,
        "message_count_skipped": skip_message_count,
        "modified": modified,
        "source": "local",
    }


def _gemini_session_matches_filters(
    workspace: str,
    modified: datetime,
    pattern: str,
    since_date: Optional[datetime],
    until_date: Optional[datetime],
) -> bool:
    """Check if a Gemini session matches the given filters.

    Uses shared _session_matches_filters with gemini_get_workspace_readable
    for hash-to-path lookups.
    """
    return _session_matches_filters(
        workspace,
        modified,
        pattern,
        since_date,
        until_date,
        get_readable=gemini_get_workspace_readable,
    )


def gemini_scan_sessions(
    pattern: str = "",
    since_date=None,
    until_date=None,
    sessions_dir: Optional[Path] = None,
    skip_message_count: bool = False,
    use_cached_counts: bool = False,
) -> list:
    """Scan ~/.gemini/tmp/*/chats/ for session-*.json files.

    Args:
        pattern: Substring pattern to filter workspaces (empty matches all)
        since_date: Only include sessions modified on or after this date
        until_date: Only include sessions modified on or before this date
        sessions_dir: Override sessions directory (for testing)
        skip_message_count: If True, skip counting messages (set to 0)

    Returns:
        List of session dicts sorted by modified time (newest first)
    """
    if sessions_dir is None:
        sessions_dir = gemini_get_home_dir()

    if not sessions_dir.exists():
        return []

    sessions = []
    # Gemini stores sessions in ~/.gemini/tmp/<project_hash>/chats/session-*.json
    for json_file in sessions_dir.glob("*/chats/session-*.json"):
        workspace = gemini_get_workspace_from_session(json_file)
        modified = datetime.fromtimestamp(json_file.stat().st_mtime)

        if _gemini_session_matches_filters(workspace, modified, pattern, since_date, until_date):
            sessions.append(
                _gemini_build_session_dict(
                    json_file,
                    workspace,
                    modified,
                    skip_message_count,
                    use_cached_counts=use_cached_counts,
                )
            )

    return sorted(sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Unified Backend Dispatch
# ============================================================================


def _get_claude_projects_path() -> Path:
    """Get Claude projects directory path without validation.

    Used for detection/existence checks where we don't want to exit on error.

    Returns:
        Path to Claude projects directory (may not exist)
    """
    env_override = os.environ.get("CLAUDE_PROJECTS_DIR")
    if env_override:
        return Path(env_override).expanduser()
    return Path.home() / ".claude" / "projects"


def get_active_backends(agent: str) -> list:
    """Return list of active backends based on agent flag.

    Args:
        agent: Agent selection - 'auto', 'claude', 'codex', or 'gemini'

    Returns:
        List of backend identifiers (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI) that exist
    """
    claude_projects = _get_claude_projects_path()

    if agent == AGENT_CLAUDE:
        return [AGENT_CLAUDE] if claude_projects.exists() else []
    elif agent == AGENT_CODEX:
        return [AGENT_CODEX] if codex_get_home_dir().exists() else []
    elif agent == AGENT_GEMINI:
        return [AGENT_GEMINI] if gemini_get_home_dir().exists() else []
    else:  # auto
        backends = []
        if claude_projects.exists():
            backends.append(AGENT_CLAUDE)
        if codex_get_home_dir().exists():
            backends.append(AGENT_CODEX)
        if gemini_get_home_dir().exists():
            backends.append(AGENT_GEMINI)
        return backends


def _scan_backend(
    backend: str,
    pattern: str,
    since_date,
    until_date,
    skip_message_count: bool,
    use_cached_counts: bool,
    **kwargs,
) -> list[dict[str, Any]]:
    """Scan sessions for a single backend.

    M6: Consolidated agent dispatch for cleaner code and easier extension.

    Args:
        backend: Backend identifier (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI)
        pattern: Workspace pattern to filter
        since_date: Start date filter
        until_date: End date filter
        skip_message_count: Skip counting messages
        **kwargs: Additional arguments for Claude backend

    Returns:
        List of session dicts with agent field set
    """
    try:
        if backend == AGENT_CLAUDE:
            sessions = get_workspace_sessions(
                pattern,
                since_date=since_date,
                until_date=until_date,
                skip_message_count=skip_message_count,
                use_cached_counts=use_cached_counts,
                **kwargs,
            )
        elif backend == AGENT_CODEX:
            sessions = codex_scan_sessions(
                pattern=pattern,
                since_date=since_date,
                until_date=until_date,
                skip_message_count=skip_message_count,
                use_cached_counts=use_cached_counts,
            )
        elif backend == AGENT_GEMINI:
            sessions = gemini_scan_sessions(
                pattern=pattern,
                since_date=since_date,
                until_date=until_date,
                skip_message_count=skip_message_count,
                use_cached_counts=use_cached_counts,
            )
        else:
            return []

        # Ensure agent field is set
        for s in sessions:
            if "agent" not in s:
                s["agent"] = backend
        return sessions
    except SystemExit:
        # Claude backend may exit if projects dir not found
        return []


def get_unified_sessions(
    agent: str = "auto",
    pattern: str = "",
    since_date=None,
    until_date=None,
    skip_message_count: bool = False,
    use_cached_counts: bool = False,
    **kwargs,
) -> list:
    """Get sessions from specified agent backend(s).

    Args:
        agent: Agent selection - 'auto', 'claude', 'codex', or 'gemini'
        pattern: Workspace pattern to filter by
        since_date: Filter to sessions modified on or after this date
        until_date: Filter to sessions modified on or before this date
        skip_message_count: If True, skip counting messages
        **kwargs: Additional arguments passed to backend functions

    Returns:
        List of session dicts from all active backends, sorted by modified (newest first)
    """
    all_sessions = []
    backends = get_active_backends(agent)

    # M6: Use consolidated backend dispatch
    for backend in backends:
        sessions = _scan_backend(
            backend,
            pattern=pattern,
            since_date=since_date,
            until_date=until_date,
            skip_message_count=skip_message_count,
            use_cached_counts=use_cached_counts,
            **kwargs,
        )
        all_sessions.extend(sessions)

    return sorted(all_sessions, key=lambda s: s["modified"], reverse=True)


# ============================================================================
# Workspace Scanning
# ============================================================================


def get_claude_projects_dir():
    """Get the Claude projects directory, with error handling.

    Exits with error if directory doesn't exist.
    """
    projects_dir = _get_claude_projects_path()

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found at {projects_dir}\n")
        sys.exit(1)

    return projects_dir


def path_to_encoded_workspace(path: str) -> str:
    """Convert an absolute path to Claude's encoded workspace directory name.

    Args:
        path: Absolute path (e.g., '/home/user/my-project' or 'C:\\Users\\alice\\projects')

    Returns:
        Encoded workspace name (e.g., '-home-user-my-project' or 'C--Users-alice-projects')
    """
    # Remove trailing slash/backslash if present
    path = path.rstrip("/").rstrip("\\")

    # Handle native Windows paths (C:\... or C:/...)
    # Check for drive letter followed by colon (e.g., C:, D:)
    if len(path) >= MIN_WINDOWS_PATH_LEN and path[1] == ":":
        return _convert_windows_path_to_encoded(path)

    # Handle WSL-mounted Windows paths like /mnt/c/Users/me/project
    if path.startswith("/mnt/") and len(path) > MIN_WSL_MNT_PATH_LEN:
        drive_letter = path[5]
        sep = path[MIN_WSL_MNT_PATH_LEN]
        if drive_letter.isalpha() and sep in ("/", "\\"):
            remainder = path[MIN_WSL_MNT_PATH_LEN + 1 :].replace("\\", "/").strip("/")
            normalized = remainder.replace("/", "-") if remainder else ""
            return f"{drive_letter.upper()}--{normalized}"

    # Replace / with - and add leading -
    if path.startswith("/"):
        return "-" + path[1:].replace("/", "-")
    else:
        return "-" + path.replace("/", "-")


def _convert_windows_path_to_encoded(path: str) -> str:
    """Convert Windows absolute path (C:\\... or C:/...) to encoded format."""
    drive = path[0].upper()
    rest = path[2:].lstrip("/\\").replace("\\", "/").replace("/", "-")
    return f"{drive}--{rest}"


def _get_workspaces_from_dir(projects_dir: Optional[Path]) -> list:
    """Get native workspace names from a projects directory."""
    if not projects_dir:
        return []
    return [d.name for d in projects_dir.iterdir() if d.is_dir() and is_native_workspace(d.name)]


def _get_workspaces_for_source(source_key: str, args=None) -> list:
    """Get list of workspace names for a given source.

    Args:
        source_key: Source identifier ('local', 'remote:host', 'wsl:distro', 'windows[:user]')
        args: Arguments object for remote access

    Returns:
        List of workspace names for the source
    """
    if source_key.startswith("wsl:"):
        return _get_workspaces_from_dir(get_wsl_projects_dir(source_key[4:]))

    if source_key.startswith("windows"):
        user = source_key[8:] if source_key.startswith("windows:") else None
        return _get_workspaces_from_dir(get_windows_projects_dir(user))

    if source_key.startswith("remote:"):
        hostname = source_key[7:]
        remote_host = getattr(args, "remote", hostname) if args else hostname
        batch_ws = get_remote_workspaces_batch(remote_host)
        return [ws["encoded"] for ws in batch_ws] if batch_ws else []

    # Local
    return _get_workspaces_from_dir(get_claude_projects_dir())


def resolve_workspace_input(workspace_input: str, source_key: str = "local", args=None) -> list:
    """Resolve a workspace input (pattern, path, or encoded name) to actual encoded workspace names.

    Args:
        workspace_input: Can be:
            - Encoded name: '-home-user-project'
            - Absolute path: '/home/user/project'
            - Pattern: 'project' or 'my-project'
        source_key: Source to search in ('local', 'remote:host', 'wsl:distro', 'windows')
        args: Arguments object for remote access

    Returns:
        List of matching encoded workspace names
    """
    # Case 1: Already an encoded workspace name (starts with -)
    if workspace_input.startswith("-"):
        return [workspace_input]

    # Case 2: Absolute path - convert to encoded name
    if workspace_input.startswith("/"):
        return [path_to_encoded_workspace(workspace_input)]

    # Case 3: Windows absolute path (C:\... or C:/...)
    if len(workspace_input) > MIN_WINDOWS_PATH_LEN and workspace_input[1] == ":":
        return [_convert_windows_path_to_encoded(workspace_input)]

    # Case 4: Pattern - search for matching workspaces
    try:
        workspaces = _get_workspaces_for_source(source_key, args)
    except (OSError, PermissionError):
        return []

    return [ws for ws in workspaces if workspace_input in ws]


def _looks_like_windows_drive(path: str) -> bool:
    """Return True if the string resembles a Windows absolute path (C:/...)."""
    return len(path) > MIN_WINDOWS_PATH_LEN and path[1] == ":"


def _collapse_slashes(path: str) -> str:
    """Collapse duplicate slashes for consistent comparisons."""
    return re.sub(r"/{2,}", "/", path)


def _strip_wsl_unc_prefix(path: str) -> str:
    """Convert //wsl.localhost/Distro/... into /Distro/... form."""
    lowered = path.lower()
    if lowered.startswith("//wsl.localhost/") or lowered.startswith("//wsl$/"):
        parts = [p for p in path.split("/") if p]
        if len(parts) >= WSL_UNC_MIN_PARTS:
            return "/" + "/".join(parts[2:])
    return path


def _extract_workspace_from_projects_path(parts: list[str]) -> tuple[str | None, list[str]]:
    """Return workspace dir when path includes .claude/projects/<workspace>."""
    if ".claude" not in parts:
        return None, parts
    try:
        idx = parts.index(".claude")
    except ValueError:
        return None, parts
    if idx + 2 < len(parts) and parts[idx + 1] == "projects":
        workspace_dir = parts[idx + 2]
        remainder = parts[idx + 2 :]
        return workspace_dir, remainder
    return None, parts


def _projects_dir_from_wsl_unc(path: str) -> Path:
    """Best-effort guess of ~/.claude/projects for a WSL UNC input."""
    if is_running_in_wsl():
        local_path = _strip_wsl_unc_prefix(path)
        parts = [p for p in local_path.split("/") if p]
        if len(parts) > 1 and parts[0] == "home":
            return Path(f"/home/{parts[1]}/.claude/projects")
        return Path(local_path).parent
    parts = [p for p in path.split("/") if p]
    distro = parts[1] if len(parts) > 1 else None
    try:
        home_idx = parts.index("home")
        user = parts[home_idx + 1] if len(parts) > home_idx + 1 else None
    except ValueError:
        user = None
    if distro and user:
        return Path(f"//wsl.localhost/{distro}/home/{user}/.claude/projects")
    if distro:
        return Path(f"//wsl.localhost/{distro}/home/.claude/projects")
    return Path(path).parent


def _is_already_encoded_or_special(target: str) -> bool:
    """Check if target is already encoded, an alias, or a jsonl file."""
    if not target:
        return True
    if target.startswith("-") or target.startswith("@"):
        return True
    if target.lower().endswith(".jsonl"):
        return True
    return False


def _try_extract_workspace_from_absolute_path(expanded: str, parts: list) -> str | None:
    """Try to extract workspace pattern from an absolute path ending in '-...'."""
    try:
        if Path(expanded).is_absolute() and parts and parts[-1].startswith("-"):
            return parts[-1]
    except OSError:
        pass
    return None


def _coerce_target_to_workspace_pattern(target: str) -> str:
    """Convert absolute filesystem paths into Claude workspace patterns."""
    if _is_already_encoded_or_special(target):
        return target

    expanded = os.path.expanduser(target).strip()
    if not expanded:
        return target

    normalized = _collapse_slashes(_strip_wsl_unc_prefix(expanded.replace("\\", "/")))
    parts = [p for p in normalized.split("/") if p]

    # Direct absolute path that already points to a workspace dir
    workspace_from_path = _try_extract_workspace_from_absolute_path(expanded, parts)
    if workspace_from_path:
        return workspace_from_path

    workspace_dir, remainder = _extract_workspace_from_projects_path(parts)
    if workspace_dir:
        if workspace_dir.startswith("-"):
            return workspace_dir
        normalized = "/" + "/".join(remainder)
    elif normalized.startswith("/wsl.localhost/") or normalized.startswith("/wsl$/"):
        split_parts = normalized.split("/")
        if len(split_parts) >= WSL_LOCALHOST_SKIP_PARTS:
            normalized = "/" + "/".join(split_parts[3:])

    if normalized.startswith("/"):
        return path_to_encoded_workspace(normalized)

    if _looks_like_windows_drive(normalized):
        return _convert_windows_path_to_encoded(expanded)

    return target


def _is_windows_encoded_path(name: str) -> bool:
    """Check if name is a Windows-style encoded path (e.g., 'C--path-to-dir')."""
    # Must be at least 3 chars, start with drive letter (A-Z), followed by "--"
    return len(name) >= MIN_ENCODED_PATH_LEN and name[0].isalpha() and name[1:3] == "--"


def _build_current_path(base_path: Path, segments: list[str]) -> Path:
    """Build current path from base and accumulated segments.

    Args:
        base_path: Base path
        segments: List of path segments

    Returns:
        Combined path
    """
    return base_path / "/".join(segments) if segments else base_path


def _find_longest_matching_segment(
    parts: list[str],
    start_idx: int,
    current_path: Path,
    unc_mode: bool,
) -> tuple[Optional[str], int]:
    """Find longest matching segment starting at start_idx.

    Args:
        parts: List of dash-split path components
        start_idx: Starting index in parts
        current_path: Current filesystem path
        unc_mode: Whether in UNC path mode (skip is_dir check)

    Returns:
        Tuple of (matched_segment, parts_consumed) or (None, 0) if no match
    """
    for end_idx in range(len(parts), start_idx, -1):
        candidate = "-".join(parts[start_idx:end_idx])
        candidate_path = current_path / candidate

        try:
            exists = candidate_path.exists()
            is_dir = candidate_path.is_dir() if not unc_mode else True

            if exists and is_dir:
                return candidate, end_idx - start_idx
        except OSError:
            continue

    return None, 0


def _resolve_path_segments(parts: list[str], base_path: Path) -> list[str]:
    """Resolve dash-separated parts to actual filesystem path segments.

    Tries progressively longer combinations of parts to handle directory
    names that contain dashes (e.g., 'my-project' encoded as 'my-project').

    Args:
        parts: List of dash-split path components
        base_path: Base path to verify against filesystem

    Returns:
        List of resolved path segments
    """
    path_segments: list[str] = []
    i = 0
    unc_mode = _is_wsl_unc_path(base_path)

    while i < len(parts):
        current_path = _build_current_path(base_path, path_segments)

        match, consumed = _find_longest_matching_segment(parts, i, current_path, unc_mode)

        if match:
            path_segments.append(match)
            i += consumed
        else:
            # No match - use single part as-is
            path_segments.append(parts[i])
            i += 1

    return path_segments


def _generate_merge_segments(parts: list, mask: int) -> list:
    """Generate segment list from parts based on merge mask."""
    segments = []
    current = parts[0]
    for i in range(1, len(parts)):
        if mask & (1 << (i - 1)):
            current += "-" + parts[i]
        else:
            segments.append(current)
            current = parts[i]
    segments.append(current)
    return segments


def _find_deepest_existing_index(segments: list, base_path: Path) -> int:
    """Find the deepest index where the path exists under base_path."""
    deepest = -1
    probe = base_path
    for idx, seg in enumerate(segments):
        probe = probe.joinpath(seg)
        try:
            if probe.exists():
                deepest = idx
            else:
                break
        except OSError:
            break
    return deepest


def _get_fallback_from_base(base_path: Path, parts: list) -> tuple:
    """Return first child dir as fallback, or original parts if nothing found."""
    try:
        existing = [
            child.name
            for child in base_path.iterdir()
            if child.is_dir() and not child.name.startswith(".")
        ]
        if existing:
            return [existing[0]], False
    except OSError:
        pass
    return parts, False


def _resolve_existing_wsl_path(parts: list, base_path: Path) -> tuple:
    """Try all segment merges to find the deepest existing path under base_path.

    This is used for UNC WSL paths on Windows where directory names may contain
    dashes that were split into separate parts. Returns the best-matching
    segment list (trimmed to the deepest existing prefix). If nothing exists,
    returns the original parts and a False flag.
    """
    if not base_path or not parts:
        return parts, False

    best_segments = None
    best_depth = -1
    best_full_segments = None
    n = len(parts)

    for mask in range(1 << (n - 1)):
        segments = _generate_merge_segments(parts, mask)
        deepest = _find_deepest_existing_index(segments, base_path)

        # Decide whether to update best result
        should_update = deepest > best_depth
        if best_segments is not None and deepest == best_depth:
            should_update = len(segments) < len(best_segments)

        if should_update:
            best_depth = deepest
            best_segments = segments[: deepest + 1] if deepest >= 0 else None
            if deepest == len(segments) - 1:
                best_full_segments = segments

        # Track best full match
        if deepest == len(segments) - 1:
            if best_full_segments is None or len(segments) < len(best_full_segments):
                best_full_segments = segments

    if best_full_segments is not None:
        return best_full_segments, True
    if best_segments is not None:
        return best_segments, False

    return _get_fallback_from_base(base_path, parts)


def _normalize_windows_path(workspace_dir_name: str, verify_local: bool) -> str:
    """Normalize a Windows-style encoded path (e.g., 'C--Users-alice-projects').

    Args:
        workspace_dir_name: Encoded name starting with drive letter and '--'
        verify_local: If True, verify against /mnt/<drive>/ filesystem

    Returns:
        Decoded path (e.g., 'C:\\Users\\alice\\projects' on Windows, '/mnt/c/...' on WSL)
    """
    drive_letter = workspace_dir_name[0].upper()
    rest = workspace_dir_name[3:]  # Skip 'C--'
    parts = rest.split("-")

    # On Windows, return a native drive path
    if sys.platform == "win32":
        drive_root = Path(f"{drive_letter}:\\")
        if verify_local:
            resolved = _resolve_path_segments(parts, drive_root)
            return str(drive_root.joinpath(*resolved))
        else:
            # Without verification, just join with backslashes
            return f"{drive_letter}:\\" + "\\".join(parts)

    # On WSL, prefer /mnt/<drive> if available
    if verify_local:
        mnt_base = Path(f"/mnt/{drive_letter.lower()}")
        if mnt_base.exists():
            path_segments = _resolve_path_segments(parts, mnt_base)
            if path_segments:
                # Return a WSL-usable path: /mnt/<drive>/<segments>
                return "/mnt/" + drive_letter.lower() + "/" + "/".join(path_segments)

    # Fallback: POSIX-style drive path
    return f"/{drive_letter}/" + rest.replace("-", "/")


def _format_unc_path(base_path: Path, readable_path: str) -> str:
    """Format a UNC path string from a base and POSIX-readable path."""
    base_str = str(base_path)
    if base_str.startswith("//"):
        base_str = "\\" + base_str.lstrip("/")
    if not base_str.startswith("\\\\"):
        base_str = "\\\\" + base_str.lstrip("\\")
    base_str = base_str.replace("/", "\\").rstrip("\\")
    suffix = readable_path.lstrip("/")
    if suffix:
        suffix = suffix.replace("/", "\\")
        return f"{base_str}\\{suffix}"
    return base_str


def _is_wsl_unc_path(path: Path | None) -> bool:
    if not path:
        return False
    base_str = str(path)
    # Normalize extended UNC (\\?\UNC\wsl.localhost\...) to standard UNC
    if base_str.startswith("\\\\?\\UNC\\"):
        base_str = "\\\\" + base_str[8:]
    return (
        base_str.startswith("\\\\wsl.localhost\\")
        or base_str.startswith("\\\\wsl$\\")
        or base_str.startswith("//wsl.localhost/")
        or base_str.startswith("//wsl$/")
    )


def _normalize_unix_path(encoded: str, verify_local: bool, base_path: Optional[Path] = None) -> str:
    """Normalize a Unix-style encoded path (e.g., 'home-user-my-project').

    Args:
        encoded: Encoded path without leading dash
        verify_local: If True, verify against filesystem
        base_path: Base path for verification (e.g., //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., '/home/user/my-project')
    """
    parts = encoded.split("-")

    if base_path and (_is_wsl_unc_path(base_path) or sys.platform == "win32"):
        resolved_segments, found_full = _resolve_existing_wsl_path(parts, Path(base_path))
        segments = list(resolved_segments) if resolved_segments else list(parts)
        if not found_full and len(segments) < len(parts):
            segments.append("-".join(parts[len(segments) :]))
        readable_path = "/" + "/".join(segments)
        marker = "" if found_full else " [missing]"
        base_str = str(base_path)
        if base_str.startswith("/") and not base_str.startswith("//"):
            return readable_path + marker
        return _format_unc_path(base_path, readable_path) + marker

    if not verify_local:
        return "/" + encoded.replace("-", "/")

    effective_base = base_path if base_path else Path("/")
    path_segments = _resolve_path_segments(parts, effective_base)

    # If resolution found no real merges (pure fallback), keep hyphens in the last segment
    if path_segments == parts:
        path_segments = (
            [*parts[:2], "-".join(parts[2:])] if len(parts) > MIN_WINDOWS_PATH_LEN else parts
        )

    path_segments, partial_marker = _apply_windows_base_resolution(parts, base_path, path_segments)

    readable_path = "/" + "/".join(path_segments)
    return readable_path + partial_marker


def _apply_windows_base_resolution(
    parts: list[str], base_path: Optional[Path], current_segments: list[str]
):
    """Handle Windows base paths when normalizing Unix-style encodings."""
    if not base_path or sys.platform != "win32":
        return current_segments, ""

    resolved_segments, found_full = _resolve_existing_wsl_path(parts, Path(base_path))
    if not resolved_segments:
        return current_segments, ""

    combined_segments = list(resolved_segments)
    if not found_full and len(resolved_segments) < len(parts):
        combined_segments.append("-".join(parts[len(resolved_segments) :]))

    final_path = Path(base_path).joinpath(*combined_segments)
    try:
        full_exists = len(resolved_segments) == len(parts) and final_path.exists()
    except OSError:
        full_exists = False

    if full_exists:
        return combined_segments, ""
    return combined_segments, " [missing]"


def normalize_workspace_name(
    workspace_dir_name: str, verify_local: bool = True, base_path: Optional[Path] = None
) -> str:
    """Convert workspace directory name to readable path.

    Claude Code encodes workspace paths by replacing '/' with '-'. This function
    reverses that encoding, optionally verifying against the filesystem to
    correctly handle directory names that contain dashes.

    Args:
        workspace_dir_name: Encoded workspace name (e.g., '-home-user-my-project')
        verify_local: If True, verify against local filesystem to handle dashes correctly
        base_path: Base path to prepend when verifying (for WSL: //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., '/home/user/my-project')
    """
    # Handle Windows-style paths (e.g., 'C--Users-alice-projects-myapp')
    if _is_windows_encoded_path(workspace_dir_name):
        return _normalize_windows_path(workspace_dir_name, verify_local)

    # Remove leading dash for Unix paths
    encoded = workspace_dir_name[1:] if workspace_dir_name.startswith("-") else workspace_dir_name

    return _normalize_unix_path(encoded, verify_local, base_path)


def get_current_workspace_pattern():
    """
    Detect the current workspace based on the current working directory.
    Returns a pattern that can be used to match the workspace directory.
    """
    cwd = Path.cwd()
    cwd_str = str(cwd)

    # Handle Windows paths (C:\path\to\project -> C--path-to-project)
    if sys.platform == "win32" and len(cwd_str) >= MIN_WINDOWS_PATH_LEN and cwd_str[1] == ":":
        # Extract drive letter and path
        drive = cwd_str[0]
        path_part = cwd_str[2:].lstrip("\\").lstrip("/")
        # Convert backslashes and forward slashes to dashes
        path_part = path_part.replace("\\", "-").replace("/", "-")
        workspace_pattern = f"{drive}--{path_part}"
    else:
        # Unix/Linux paths (/home/user/projects/myapp -> home-user-projects-myapp)
        workspace_pattern = cwd_str.lstrip("/").replace("/", "-")

    return workspace_pattern


def check_current_workspace_exists():
    """
    Check if the current directory corresponds to a known Claude Code workspace.
    Returns (pattern, exists) tuple.
    """
    pattern = get_current_workspace_pattern()
    projects_dir = get_claude_projects_dir()

    if not pattern:
        return pattern, False
    if not projects_dir or not projects_dir.exists():
        return pattern, False
    if sys.platform == "win32" and re.match(r"^[A-Za-z]--$", pattern):
        return pattern, False

    # Check if any workspace directory matches the pattern
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue
        # Skip cached remote/wsl directories
        dirname = workspace_dir.name
        if is_cached_workspace(dirname):
            continue
        if pattern in dirname:
            return pattern, True

    return pattern, False


def _detect_wsl_base_path(projects_dir: Path) -> Optional[Path]:
    """Detect WSL base path from projects directory, or None if not WSL.

    Handles both str and bytes forms for compatibility with tests that supply
    dummy path objects.
    """
    import os as _os

    p = _os.fspath(projects_dir)
    if isinstance(p, bytes):
        p = p.decode(errors="ignore")

    is_wsl_unc = any(
        p.startswith(prefix)
        for prefix in ("\\\\wsl.localhost\\", "//wsl.localhost/", "\\\\wsl$\\", "//wsl$/")
    )
    if not is_wsl_unc:
        return None

    # Use the UNC anchor (e.g., \\wsl.localhost\\Ubuntu) as the base path
    anchor = PureWindowsPath(p).anchor
    return Path(anchor.rstrip("\\/")) if anchor else None


def _should_skip_workspace(dir_name: str, include_cached: bool) -> bool:
    """Check if a workspace directory should be skipped."""
    if not include_cached:
        if is_cached_workspace(dir_name):
            return True
        if dir_name.startswith("-remote-") or dir_name.startswith("--wsl-"):
            return True
    return False


def _get_cached_message_count(jsonl_file: Path, current_mtime: float) -> Optional[int]:
    """Return cached message count from metrics DB if mtime matches."""
    db_path = get_metrics_db_path()
    if not db_path.exists():
        return None
    try:
        conn = sqlite3.connect(str(db_path), timeout=1.0)
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT message_count, file_mtime FROM sessions WHERE file_path = ?",
            (str(jsonl_file),),
        ).fetchone()
        conn.close()
    except sqlite3.Error:
        return None
    if not row:
        return None
    file_mtime = row["file_mtime"]
    if file_mtime is None or file_mtime < current_mtime:
        return None
    return row["message_count"]


def _count_file_messages(
    jsonl_file: Path, skip_count: bool, use_cached_counts: bool = False
) -> int:
    """Count valid messages in a JSONL file.

    Only counts lines that are valid JSON with type 'user' or 'assistant'
    (or have 'role' field for older formats).
    """
    if skip_count:
        return 0
    try:
        current_mtime = jsonl_file.stat().st_mtime
    except OSError:
        current_mtime = None
    if use_cached_counts and current_mtime is not None:
        cached = _get_cached_message_count(jsonl_file, current_mtime)
        if cached is not None:
            return cached
    count = 0
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                stripped = line.strip()
                if not stripped:
                    continue
                try:
                    data = json.loads(stripped)
                    # Count only user/assistant messages
                    msg_type = data.get("type") or data.get("role")
                    if msg_type in ("user", "assistant", "model"):
                        count += 1
                except json.JSONDecodeError:
                    continue
    except OSError:
        pass
    return count


def _get_session_from_file(
    jsonl_file: Path,
    workspace_dir: Path,
    readable_name: str,
    skip_message_count: bool,
    use_cached_counts: bool = False,
) -> dict:
    """Build session dict from a JSONL file."""
    stat = jsonl_file.stat()
    message_count = _count_file_messages(
        jsonl_file, skip_message_count, use_cached_counts=use_cached_counts
    )
    return {
        "workspace": workspace_dir.name,
        "workspace_readable": readable_name,
        "file": jsonl_file,
        "filename": jsonl_file.name,
        "size_kb": stat.st_size / 1024,
        "modified": datetime.fromtimestamp(stat.st_mtime),
        "message_count": message_count,
        "message_count_skipped": skip_message_count,
    }


def _is_session_in_date_range(
    session: dict[str, Any],
    since_date: Optional[datetime],
    until_date: Optional[datetime],
) -> bool:
    """Check if session modification date is within the specified range."""
    return is_date_in_range(session.get("modified"), since_date, until_date)


def _is_valid_workspace_dir(
    workspace_dir: Path,
    projects_dir: Path,
    include_cached: bool,
) -> bool:
    """Check if workspace directory is valid for scanning.

    Centralizes validation logic for workspace directories, making the main
    scanning loop cleaner and the validation independently testable.

    Args:
        workspace_dir: Workspace directory to validate
        projects_dir: Parent projects directory
        include_cached: Whether to include cached remote/WSL workspaces

    Returns:
        True if workspace is valid for scanning
    """
    if not workspace_dir.is_dir():
        return False

    dir_name = workspace_dir.name

    if not validate_workspace_name(dir_name):
        return False

    if not is_safe_path(projects_dir, workspace_dir):
        return False

    if _should_skip_workspace(dir_name, include_cached):
        return False

    return True


def _normalize_pattern(pattern: str) -> str:
    """Normalize pattern for cross-platform matching.

    Converts path separators to dashes for Claude-style encoded matching.
    """
    return pattern.replace("\\", "-").replace("/", "-")


def _get_pattern_tail(pattern: str) -> str:
    """Get last component of pattern for partial matching."""
    return pattern.replace("\\", "/").split("/")[-1]


def _workspace_matches_pattern(dir_name: str, workspace_pattern: str, match_all: bool) -> bool:
    """Check if workspace matches the pattern.

    Args:
        dir_name: Workspace directory name (encoded path)
        workspace_pattern: Pattern to match
        match_all: If True, match all workspaces

    Returns:
        True if workspace matches pattern
    """
    if match_all:
        return True

    # Direct match
    if workspace_pattern in dir_name:
        return True

    # Normalized match (handle path separators)
    normalized = _normalize_pattern(workspace_pattern)
    if normalized in dir_name:
        return True

    # Tail match (just the last component)
    tail = _get_pattern_tail(workspace_pattern)
    return bool(tail and tail in dir_name)


def get_workspace_sessions(
    workspace_pattern: str,
    quiet: bool = False,
    since_date=None,
    until_date=None,
    include_cached: bool = False,
    projects_dir: Optional[Path] = None,
    skip_message_count: bool = False,
    use_cached_counts: bool = False,
):
    """Find all Claude Code sessions in workspaces matching the pattern.

    Args:
        workspace_pattern: Substring to match workspace names. Use "", "*", or "all" to match all.
        quiet: Suppress progress output if True
        since_date: Filter to sessions modified on or after this date
        until_date: Filter to sessions modified on or before this date
        include_cached: If True, include remote_* and wsl_* cached workspaces
        projects_dir: Explicit projects directory path (default: auto-detect)
        skip_message_count: If True, skip counting messages (faster for slow filesystems)

    Returns:
        List of session dicts with workspace, file, and metadata info
    """
    if projects_dir is None:
        projects_dir = get_claude_projects_dir()

    match_all = workspace_pattern in ("", "*", "all")
    wsl_base = _detect_wsl_base_path(projects_dir)
    sessions = []

    for workspace_dir in projects_dir.iterdir():
        # Use centralized validation (H10)
        if not _is_valid_workspace_dir(workspace_dir, projects_dir, include_cached):
            continue

        dir_name = workspace_dir.name
        if not _workspace_matches_pattern(dir_name, workspace_pattern, match_all):
            continue

        readable_name = normalize_workspace_name(dir_name, base_path=wsl_base)

        for jsonl_file in workspace_dir.glob("*.jsonl"):
            session = _get_session_from_file(
                jsonl_file,
                workspace_dir,
                readable_name,
                skip_message_count,
                use_cached_counts=use_cached_counts,
            )
            if _is_session_in_date_range(session, since_date, until_date):
                sessions.append(session)

    sessions.sort(key=lambda s: s["modified"])
    return sessions


# ============================================================================
# WSL Access
# ============================================================================


def is_running_in_wsl() -> bool:
    """Detect if we're running inside WSL."""
    try:
        with open("/proc/version") as f:
            content = f.read().lower()
            return "microsoft" in content or "wsl" in content
    except OSError:
        return False


def _find_user_home_on_drives(username: str) -> Optional[Path]:
    """Find Windows user home by username across all drives."""
    mnt = Path("/mnt")
    if not mnt.exists():
        return None
    for drive in sorted(mnt.iterdir()):
        if drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha():
            user_path = drive / "Users" / username
            if user_path.exists() and (user_path / ".claude" / "projects").exists():
                return user_path
    return None


def _get_userprofile_via_cmd() -> Optional[Path]:
    """Get Windows home via cmd.exe USERPROFILE variable."""
    try:
        result = subprocess.run(
            [get_command_path("cmd.exe"), "/c", "echo %USERPROFILE%"],
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        win_path = result.stdout.strip()
        if not win_path or win_path == "%USERPROFILE%":
            return None

        result = subprocess.run(
            [get_command_path("wslpath"), win_path],
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        wsl_path = Path(result.stdout.strip())
        if wsl_path.exists() and (wsl_path / ".claude" / "projects").exists():
            return wsl_path
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass
    return None


def _find_claude_user_in_drive(drive: Path) -> Optional[Path]:
    """Find first user with Claude installed in a drive."""
    users_dir = drive / "Users"
    if not users_dir.exists():
        return None
    for user_dir in users_dir.iterdir():
        if user_dir.is_dir() and not user_dir.is_symlink():
            if (user_dir / ".claude" / "projects").exists():
                return user_dir
    return None


def _scan_drives_for_claude_user() -> Optional[Path]:
    """Scan all drives for any user with Claude installed."""
    mnt = Path("/mnt")
    if not mnt.exists():
        return None
    for drive in sorted(mnt.iterdir()):
        if not (drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha()):
            continue
        user_home = _find_claude_user_in_drive(drive)
        if user_home:
            return user_home
    return None


def get_windows_home_from_wsl(username: Optional[str] = None) -> Optional[Path]:
    """
    Get Windows user home directory from WSL.

    Args:
        username: Optional Windows username. If None, uses USERPROFILE.

    Returns:
        Path to Windows home directory, or None if not found.
    """
    cache_key = username or "_default_"
    cache = _get_windows_home_cache()

    # Check cache first to avoid slow filesystem operations
    if cache.has(cache_key):
        return cache.get(cache_key)

    if username:
        result = _find_user_home_on_drives(username)
        cache.set(cache_key, result)
        return result

    # Primary approach: use Windows USERPROFILE
    home = _get_userprofile_via_cmd()
    if home:
        cache.set(cache_key, home)
        return home

    # Fallback: scan all drives
    result = _scan_drives_for_claude_user()
    cache.set(cache_key, result)
    return result


def _is_valid_windows_drive(drive: Path) -> bool:
    """Check if path is a valid single-letter Windows drive mount."""
    return drive.is_dir() and len(drive.name) == 1 and drive.name.isalpha()


def _scan_users_in_drive(drive: Path, results: list):
    """Scan a drive for Windows users with Claude installed."""
    users_dir = drive / "Users"
    if not users_dir.exists():
        return

    for user_dir in users_dir.iterdir():
        if not user_dir.is_dir() or user_dir.is_symlink():
            continue
        claude_dir = user_dir / ".claude" / "projects"
        if claude_dir.exists():
            workspace_count = len([d for d in claude_dir.iterdir() if d.is_dir()])
            results.append(
                {
                    "username": user_dir.name,
                    "drive": drive.name,
                    "path": user_dir,
                    "claude_dir": claude_dir,
                    "workspace_count": workspace_count,
                }
            )


def get_windows_users_with_claude():
    """Get list of all Windows users with Claude Code installed.

    If CLAUDE_SKIP_WINDOWS_SCAN=1 is set, skip Windows scanning entirely (for fast tests).
    """
    if os.environ.get("CLAUDE_SKIP_WINDOWS_SCAN") == "1":
        return []

    results = []
    mnt = Path("/mnt")

    if not mnt.exists():
        return results

    for drive in sorted(mnt.iterdir()):
        if _is_valid_windows_drive(drive):
            _scan_users_in_drive(drive, results)

    return results


def is_wsl_remote(remote_spec: str) -> bool:
    """Check if remote spec is a WSL distribution (wsl://DistroName)."""
    return remote_spec.startswith("wsl://")


def is_windows_remote(remote_spec: str) -> bool:
    """Check if remote spec is Windows from WSL (windows or windows://username)."""
    return remote_spec == "windows" or remote_spec.startswith("windows://")


def get_source_tag(remote_spec: Optional[str] = None) -> str:
    """
    Generate source tag for filename/directory prefixes.

    Args:
        remote_spec: Remote specification (None for local, 'wsl://Ubuntu', 'windows', 'user@host')

    Returns:
        Source tag string:
        - '' for local (no tag)
        - 'wsl_{distro}_' for WSL
        - 'windows_' for Windows from WSL
        - 'remote_{hostname}_' for SSH remotes
    """
    if not remote_spec:
        # Local - no tag
        return ""

    if is_wsl_remote(remote_spec):
        # WSL: wsl_ubuntu_
        distro = remote_spec[6:]  # Remove 'wsl://' prefix
        return f"wsl_{distro.lower()}_"

    if is_windows_remote(remote_spec):
        # Windows from WSL: windows_ or windows_username_
        if remote_spec.startswith("windows://"):
            username = remote_spec[10:]  # Remove 'windows://' prefix
            return f"windows_{username.lower()}_"
        else:
            return "windows_"

    # SSH Remote: remote_hostname_
    # Extract hostname from user@hostname or hostname
    if "@" in remote_spec:
        hostname = remote_spec.split("@")[1]
    else:
        hostname = remote_spec

    # Take first part if FQDN (e.g., dev.example.com -> dev)
    hostname = hostname.split(".")[0].lower()

    return f"remote_{hostname}_"


def get_workspace_name_from_path(workspace_dir_name: str) -> str:
    """
    Extract clean workspace name from directory name.
    Removes source tags and normalizes path.

    Examples:
        'C--Users-alice-projects-myapp' -> 'myapp'
        'remote_server01_home-bob-projects-mylib' -> 'mylib'
        'wsl_ubuntu_home-user-projects-auth' -> 'auth'
    """
    # Remove source tags if present
    if workspace_dir_name.startswith(CACHED_REMOTE_PREFIX):
        # remote_hostname_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= REMOTE_PARTS_WITH_PATH:
            workspace_dir_name = parts[2]
    elif workspace_dir_name.startswith(CACHED_WSL_PREFIX):
        # wsl_distro_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= REMOTE_PARTS_WITH_PATH:
            workspace_dir_name = parts[2]

    # Get the last path component (workspace name)
    # home-alice-projects-myapp -> myapp
    # C--Users-alice-projects-myapp -> myapp
    path_parts = workspace_dir_name.split("-")

    # Find the workspace name (usually last component or last few)
    # Simple heuristic: take last part, or last 2 if hyphenated (e.g., claude-history)
    if len(path_parts) >= MIN_WINDOWS_PATH_LEN:
        # Check if last 2 parts form a common pattern
        last_two = "-".join(path_parts[-2:])
        # If the second-to-last part is short (likely part of name like "claude-history")
        if len(path_parts[-2]) <= MAX_SHORT_PART_LEN:
            return last_two

    return path_parts[-1] if path_parts else workspace_dir_name


def _get_wsl_distro_names() -> list:
    """Get list of WSL distribution names.

    Notes:
        On Windows, `wsl --list --quiet` typically returns UTF-16 with a BOM and
        may include stray null terminators. We decode with 'utf-16' (to consume
        the BOM correctly), fall back to UTF-8 if needed, and strip any BOM or
        nulls per line before returning clean distro names.
    """
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "--list", "--quiet"],
            check=False,
            capture_output=True,
            timeout=5,
        )
        if result.returncode != 0:
            return []

        raw = result.stdout
        # Prefer utf-16 (consumes BOM). Fallback to utf-8 if decode fails.
        try:
            text = raw.decode("utf-16")
        except UnicodeError:
            text = raw.decode("utf-8", errors="ignore")

        names = []
        for line in text.splitlines():
            # Clean BOM on first line, nulls, and any incidental markers
            cleaned = line.lstrip("\ufeff").replace("\x00", "").strip()
            # Newer WSL may show a leading '* ' for default in some modes; be tolerant
            if cleaned.startswith("* "):
                cleaned = cleaned[2:].strip()
            if cleaned:
                names.append(cleaned)
        return names
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def _get_wsl_usernames_from_unc(distro_name: str) -> list:
    """Discover WSL usernames via UNC paths when wsl.exe is unavailable."""
    candidates = [
        Path(f"//wsl.localhost/{distro_name}/home"),
        Path(f"//wsl$/{distro_name}/home"),
    ]
    for base in candidates:
        try:
            if not _path_exists_with_timeout(base, timeout=5.0):
                continue
            return [p.name for p in base.iterdir() if p.is_dir() and not p.name.startswith(".")]
        except (OSError, PermissionError):
            continue
    return []


def _build_wsl_distro_info(distro_name: str, usernames: list) -> Optional[dict]:
    """Build WSL distro info from one or more candidate usernames."""
    for username in usernames:
        claude_path = _locate_wsl_projects_dir(distro_name, username)
        codex_path = _locate_wsl_agent_dir(distro_name, username, AGENT_CODEX)
        gemini_path = _locate_wsl_agent_dir(distro_name, username, AGENT_GEMINI)
        has_claude = claude_path is not None
        has_codex = codex_path is not None
        has_gemini = gemini_path is not None
        if has_claude or has_codex or has_gemini:
            return {
                "name": distro_name,
                "username": username,
                "has_claude": has_claude,
                "has_codex": has_codex,
                "has_gemini": has_gemini,
                "path": str(claude_path) if claude_path else None,
                "codex_path": str(codex_path) if codex_path else None,
                "gemini_path": str(gemini_path) if gemini_path else None,
            }

    if not usernames:
        return None

    username = usernames[0]
    return {
        "name": distro_name,
        "username": username,
        "has_claude": False,
        "has_codex": False,
        "has_gemini": False,
        "path": None,
        "codex_path": None,
        "gemini_path": None,
    }


def _get_wsl_distro_info(distro_name: str) -> Optional[dict]:
    """Get info for a single WSL distribution. Returns None on failure."""
    try:
        user_result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "whoami"],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if user_result.returncode != 0:
            usernames = _get_wsl_usernames_from_unc(distro_name)
            return _build_wsl_distro_info(distro_name, usernames)

        username = user_result.stdout.strip()
        return _build_wsl_distro_info(distro_name, [username])
    except (subprocess.TimeoutExpired, FileNotFoundError):
        usernames = _get_wsl_usernames_from_unc(distro_name)
        return _build_wsl_distro_info(distro_name, usernames) if usernames else None


def get_wsl_distributions() -> list:
    """Get list of available WSL distributions.

    Returns:
        List of dicts with distro info: {'name': str, 'username': str, 'has_claude': bool, ...}

    Test override:
        If environment variables are set:
          - CLAUDE_WSL_TEST_DISTRO: name of a synthetic distro
          - CLAUDE_WSL_PROJECTS_DIR: path to a projects dir for that distro
        then return a single entry using those values. This enables real
        filesystem E2E tests without mocking wsl.exe.

        If CLAUDE_SKIP_WSL_SCAN=1 is set, skip WSL scanning entirely (for fast tests).
    """
    # Skip WSL scanning if requested (for fast tests)
    if os.environ.get("CLAUDE_SKIP_WSL_SCAN") == "1":
        return []

    # Test override
    test_distro = os.environ.get("CLAUDE_WSL_TEST_DISTRO")
    test_projects = os.environ.get("CLAUDE_WSL_PROJECTS_DIR")
    if test_distro and test_projects:
        p = Path(test_projects)
        if p.exists():
            return [
                {
                    "name": test_distro,
                    "username": "test",
                    "has_claude": True,
                    "has_codex": False,
                    "has_gemini": False,
                    "path": str(p),
                    "codex_path": None,
                    "gemini_path": None,
                }
            ]

    if platform.system() != "Windows":
        return []

    distributions = []
    for distro_name in _get_wsl_distro_names():
        if not distro_name:
            continue
        info = _get_wsl_distro_info(distro_name)
        if info:
            distributions.append(info)
    return distributions


def _wsl_distro_has_agent(distro: dict, agent: str) -> bool:
    """Check whether a WSL distro has data for the requested agent."""
    if agent == AGENT_CLAUDE:
        return bool(distro.get("has_claude"))
    if agent == AGENT_CODEX:
        return bool(distro.get("has_codex"))
    if agent == AGENT_GEMINI:
        return bool(distro.get("has_gemini"))
    return bool(distro.get("has_claude") or distro.get("has_codex") or distro.get("has_gemini"))


def _filter_wsl_distros_for_agent(wsl_distros: list, agent: Optional[str]) -> list:
    """Filter WSL distros by agent presence (auto includes any)."""
    agent_key = agent or "auto"
    return [d for d in wsl_distros if _wsl_distro_has_agent(d, agent_key)]


def _select_wsl_distro_for_agent(wsl_distros: list, agent: Optional[str]) -> Optional[dict]:
    """Pick the first WSL distro that matches the requested agent."""
    agent_key = agent or "auto"
    if agent_key == "auto":
        for flag in ("has_claude", "has_codex", "has_gemini"):
            for distro in wsl_distros:
                if distro.get(flag):
                    return distro
        return None
    for distro in wsl_distros:
        if _wsl_distro_has_agent(distro, agent_key):
            return distro
    return None


def _get_wsl_candidate_paths(distro_name: str, username: str) -> list:
    """Return candidate UNC paths for a WSL distro."""
    return [
        Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects"),
        Path(f"//wsl$/{distro_name}/home/{username}/.claude/projects"),
    ]


def _path_exists_with_timeout(path: Path, timeout: float = 5.0) -> bool:
    """Check if a path exists with a timeout (for UNC paths that may block).

    Args:
        path: Path to check
        timeout: Maximum time to wait in seconds

    Returns:
        True if path exists, False if doesn't exist or timeout reached.
    """

    def check_exists() -> bool:
        return path.exists()

    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(check_exists)
            return future.result(timeout=timeout)
    except (FuturesTimeoutError, OSError):
        return False


def _locate_wsl_projects_dir(distro_name: str, username: str):
    """Find the first accessible UNC path for a WSL distro."""
    for candidate in _get_wsl_candidate_paths(distro_name, username):
        try:
            # Use timeout to prevent blocking on unreachable UNC paths
            if _path_exists_with_timeout(candidate, timeout=5.0):
                return candidate
        except OSError:
            continue
    return None


def _get_wsl_home_path(distro_name: str) -> Optional[str]:
    """Get the $HOME path inside a specific WSL distro.

    Returns the Linux path (e.g., '/home/user'). Returns None on failure.
    """
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "sh", "-lc", 'printf %s "$HOME"'],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode != 0:
            return None
        home = result.stdout.strip()
        # basic sanity: must look like an absolute path
        return home if home.startswith("/") else None
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return None


def get_wsl_projects_dir(distro_name: str) -> Optional[Path]:
    """Get Claude projects directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .claude/projects in WSL, accessible from Windows.
        Returns None if the distribution is inaccessible or Claude is not installed.
    """
    # Test override (DRY-EXTRACT-WSL: same pattern as other WSL functions)
    override = os.environ.get("CLAUDE_WSL_PROJECTS_DIR")
    if override and Path(override).exists():
        return Path(override)

    # Get username using common helper
    username = _get_wsl_username(distro_name)
    if username:
        return _locate_wsl_projects_dir(distro_name, username)
    return None


def get_windows_projects_dir(username: Optional[str] = None):
    """Get Claude projects directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .claude/projects in Windows, accessible from WSL.
        Returns None if not running in WSL, user not found, or Claude not installed.
    """
    # Test override
    override = os.environ.get("CLAUDE_WINDOWS_PROJECTS_DIR")
    if override and Path(override).exists():
        return Path(override)

    if not is_running_in_wsl():
        return None

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        return None

    projects_dir = windows_home / ".claude" / "projects"

    if not projects_dir.exists():
        return None

    return projects_dir


# ============================================================================
# Multi-Agent WSL/Windows Path Accessors
# ============================================================================


def _get_wsl_gemini_candidate_paths(distro_name: str, username: str) -> list:
    """Return candidate UNC paths for Gemini sessions in a WSL distro."""
    return [
        Path(f"//wsl.localhost/{distro_name}/home/{username}/.gemini/tmp"),
        Path(f"//wsl$/{distro_name}/home/{username}/.gemini/tmp"),
    ]


def _get_wsl_codex_candidate_paths(distro_name: str, username: str) -> list:
    """Return candidate UNC paths for Codex sessions in a WSL distro."""
    return [
        Path(f"//wsl.localhost/{distro_name}/home/{username}/.codex/sessions"),
        Path(f"//wsl$/{distro_name}/home/{username}/.codex/sessions"),
    ]


def _locate_wsl_agent_dir(distro_name: str, username: str, agent: str) -> Optional[Path]:
    """Find the first accessible UNC path for an agent in a WSL distro."""
    if agent == AGENT_CLAUDE:
        candidates = _get_wsl_candidate_paths(distro_name, username)
    elif agent == AGENT_GEMINI:
        candidates = _get_wsl_gemini_candidate_paths(distro_name, username)
    elif agent == AGENT_CODEX:
        candidates = _get_wsl_codex_candidate_paths(distro_name, username)
    else:
        return None

    for candidate in candidates:
        try:
            if _path_exists_with_timeout(candidate, timeout=5.0):
                return candidate
        except OSError:
            continue
    return None


def _get_wsl_username(distro_name: str) -> Optional[str]:
    """Get username from a WSL distribution.

    DRY-EXTRACT-WSL: Common helper to avoid duplicating subprocess call.

    Args:
        distro_name: WSL distribution name

    Returns:
        Username string or None on failure
    """
    try:
        result = subprocess.run(
            [get_command_path("wsl"), "-d", distro_name, "whoami"],
            check=False,
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            username = result.stdout.strip()
            return username if username else None
        usernames = _get_wsl_usernames_from_unc(distro_name)
        return usernames[0] if usernames else None
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass
    return None


def _get_wsl_agent_sessions_dir(distro_name: str, agent: str, env_var: str) -> Optional[Path]:
    """Get session directory for an agent from WSL distribution.

    DRY-EXTRACT-WSL: Common implementation for all agent WSL accessors.

    Args:
        distro_name: WSL distribution name
        agent: Agent type (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI)
        env_var: Environment variable name for test override

    Returns:
        Path to agent's sessions directory, or None if not found.
    """
    # Test override
    override = os.environ.get(env_var)
    if override and Path(override).exists():
        return Path(override)

    # Get username and locate directory
    username = _get_wsl_username(distro_name)
    if username:
        return _locate_wsl_agent_dir(distro_name, username, agent)
    return None


def gemini_get_wsl_sessions_dir(distro_name: str) -> Optional[Path]:
    """Get Gemini sessions directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .gemini/tmp in WSL, accessible from Windows.
        Returns None if the distribution is inaccessible or Gemini is not installed.
    """
    return _get_wsl_agent_sessions_dir(distro_name, AGENT_GEMINI, "GEMINI_WSL_SESSIONS_DIR")


def codex_get_wsl_sessions_dir(distro_name: str) -> Optional[Path]:
    """Get Codex sessions directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .codex/sessions in WSL, accessible from Windows.
        Returns None if the distribution is inaccessible or Codex is not installed.
    """
    return _get_wsl_agent_sessions_dir(distro_name, AGENT_CODEX, "CODEX_WSL_SESSIONS_DIR")


def gemini_get_windows_sessions_dir(username: Optional[str] = None):
    """Get Gemini sessions directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .gemini/tmp in Windows, accessible from WSL.
        Returns None if not running in WSL, user not found, or Gemini not installed.
    """
    # Test override
    override = os.environ.get("GEMINI_WINDOWS_SESSIONS_DIR")
    if override and Path(override).exists():
        return Path(override)

    if not is_running_in_wsl():
        return None

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        return None

    sessions_dir = windows_home / ".gemini" / "tmp"

    if not sessions_dir.exists():
        return None

    return sessions_dir


def codex_get_windows_sessions_dir(username: Optional[str] = None):
    """Get Codex sessions directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .codex/sessions in Windows, accessible from WSL.
        Returns None if not running in WSL, user not found, or Codex not installed.
    """
    # Test override
    override = os.environ.get("CODEX_WINDOWS_SESSIONS_DIR")
    if override and Path(override).exists():
        return Path(override)

    if not is_running_in_wsl():
        return None

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        return None

    sessions_dir = windows_home / ".codex" / "sessions"

    if not sessions_dir.exists():
        return None

    return sessions_dir


def get_agent_wsl_dir(distro_name: str, agent: str):
    """Get session directory for any agent from WSL distribution.

    Args:
        distro_name: WSL distribution name
        agent: Agent type (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI)

    Returns:
        Path to agent's sessions directory, or None if not found.
    """
    if agent == AGENT_CLAUDE:
        return get_wsl_projects_dir(distro_name)
    elif agent == AGENT_CODEX:
        return codex_get_wsl_sessions_dir(distro_name)
    elif agent == AGENT_GEMINI:
        return gemini_get_wsl_sessions_dir(distro_name)
    return None


def get_agent_windows_dir(username: Optional[str], agent: str):
    """Get session directory for any agent from Windows.

    Args:
        username: Windows username (or None for auto-detect)
        agent: Agent type (AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI)

    Returns:
        Path to agent's sessions directory, or None if not found.

    If CLAUDE_SKIP_WINDOWS_SCAN=1 is set, skip Windows scanning entirely (for fast tests).
    """
    if os.environ.get("CLAUDE_SKIP_WINDOWS_SCAN") == "1":
        return None

    if agent == AGENT_CLAUDE:
        return get_windows_projects_dir(username)
    elif agent == AGENT_CODEX:
        return codex_get_windows_sessions_dir(username)
    elif agent == AGENT_GEMINI:
        return gemini_get_windows_sessions_dir(username)
    return None


# ============================================================================
# Remote Fetching
# ============================================================================


def parse_remote_host(remote_spec: Optional[str]) -> tuple:
    """Parse remote host specification.

    Args:
        remote_spec: Remote host in format 'user@hostname' or 'hostname'

    Returns:
        Tuple of (user, hostname, full_spec) or (None, hostname, hostname)
    """
    if not remote_spec:
        return (None, "", "")
    if "@" in remote_spec:
        user, hostname = remote_spec.split("@", 1)
        return (user, hostname, remote_spec)
    else:
        # Just hostname, SSH will use current user
        return (None, remote_spec, remote_spec)


def check_ssh_connection(remote_host: Optional[str]) -> bool:
    """Check if passwordless SSH connection is possible.

    Args:
        remote_host: Full remote spec (user@hostname or hostname)

    Returns:
        True if connection successful, False otherwise
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return False

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Try SSH with BatchMode (no password prompts) and short timeout
        result = subprocess.run(
            [
                get_command_path("ssh"),
                "-o",
                "BatchMode=yes",
                "-o",
                "ConnectTimeout=5",
                remote_host,
                "echo ok",
            ],
            check=False,
            capture_output=True,
            text=True,
            timeout=10,
        )
        return result.returncode == 0 and result.stdout.strip() == "ok"
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def get_remote_hostname(remote_host: Optional[str]) -> str:
    """Extract hostname from remote spec for use in directory prefix.

    Args:
        remote_host: Remote spec (user@hostname or hostname)

    Returns:
        Hostname portion only
    """
    _, hostname, _ = parse_remote_host(remote_host)
    # Clean hostname for use in directory names (remove dots, etc.)
    return hostname.replace(".", "-")


def get_remote_workspaces_batch(remote_host: Optional[str]) -> list:
    """Get all workspace info from remote in one batch operation.

    Creates and runs a script on remote to gather all info efficiently.

    Returns:
        List of dicts with workspace info including decoded paths
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    # Create a shell script that will run on remote
    script = """#!/bin/bash
cd ~/.claude/projects/ 2>/dev/null || exit 1

for dir in -*/ ; do
    dir=${dir%/}  # Remove trailing slash
    [ -d "$dir" ] || continue

    # Decode the workspace name by testing paths
    encoded="${dir#-}"  # Remove leading dash

    # Try to find the actual path
    IFS='-' read -ra PARTS <<< "$encoded"
    best_path=""

    # Build path by testing combinations
    current=""
    for part in "${PARTS[@]}"; do
        if [ -z "$current" ]; then
            current="$part"
        else
            # Try with dash
            test_with_dash="$current-$part"
            # Try as separate segment
            test_separate="$current/$part"

            if [ -d "/$test_with_dash" ]; then
                current="$test_with_dash"
            elif [ -d "/$current/$part" ]; then
                current="$current/$part"
            else
                current="$test_separate"
            fi
        fi
    done

    # Get session count
    session_count=$(find "$dir" -maxdepth 1 -name "*.jsonl" 2>/dev/null | wc -l)

    # Output: encoded_name|decoded_path|session_count
    echo "$dir|/$current|$session_count"
done
"""

    try:
        # Run the script on remote via SSH
        # Ensure Unix line endings (important on Windows)
        # Replace both \r\n and standalone \r with \n
        script_unix = script.replace("\r\n", "\n").replace("\r", "\n")
        script_bytes = script_unix.encode("utf-8")

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, "bash -s"],
            check=False,
            input=script_bytes,
            capture_output=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        stdout_text = result.stdout.decode("utf-8", errors="replace")
        # Parse results
        workspaces = []
        for line in stdout_text.strip().split("\n"):
            if not line:
                continue
            parts = line.split("|")
            if len(parts) >= SSH_WORKSPACE_PARTS:
                workspaces.append(
                    {
                        "encoded": parts[0],
                        "decoded": parts[1],
                        "session_count": int(parts[2]) if parts[2].isdigit() else 0,
                    }
                )

        return workspaces

    except (subprocess.TimeoutExpired, subprocess.SubprocessError, OSError):
        # SSH command failed, timed out, or couldn't execute
        return []


def normalize_remote_workspace_name(remote_host: Optional[str], workspace_dir_name: str) -> str:
    """Convert remote workspace directory name to readable path by verifying via SSH.

    Args:
        remote_host: Remote host specification
        workspace_dir_name: Encoded workspace name

    Returns:
        Decoded path
    """
    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        # Fall back to simple replacement without remote verification
        encoded = (
            workspace_dir_name[1:] if workspace_dir_name.startswith("-") else workspace_dir_name
        )
        return "/" + encoded.replace("-", "/")
    if not validate_workspace_name(workspace_dir_name):
        return workspace_dir_name  # Return as-is if invalid

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    # Remove leading dash
    if workspace_dir_name.startswith("-"):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # Generate possible decodings by trying different dash positions
    parts = encoded.split("-")

    # Generate all reasonable path combinations (limit to avoid exponential explosion)
    # We'll try keeping dashes together in common patterns
    candidates = []

    # Strategy: Build path greedily, trying longer segments first
    def generate_paths(parts, start_idx, current_path):
        if start_idx >= len(parts):
            candidates.append("/".join(current_path))
            return

        # Try combining 1 to 3 parts (limit to keep it reasonable)
        for length in range(1, min(4, len(parts) - start_idx + 1)):
            segment = "-".join(parts[start_idx : start_idx + length])
            generate_paths(parts, start_idx + length, [*current_path, segment])

    generate_paths(parts, 0, [])

    # Limit candidates to reasonable number
    candidates = candidates[:MAX_WORKSPACE_CANDIDATES]

    # Test all candidates in ONE SSH call
    test_commands = " || ".join([f'test -e "/{path}" && echo "/{path}"' for path in candidates])

    try:
        result = subprocess.run(
            [get_command_path("ssh"), remote_host, test_commands],
            check=False,
            capture_output=True,
            text=True,
            timeout=10,
        )

        if result.stdout.strip():
            # Return the first (longest) match
            return result.stdout.strip().split("\n")[0]
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass

    # Fallback to simple replacement
    return "/" + encoded.replace("-", "/")


def list_remote_workspaces(remote_host: Optional[str]) -> list:
    """List workspace directories on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of workspace directory names (e.g., ['-home-user-project', ...])
        Excludes remote caches (remote_* and wsl_*) to prevent circular fetching
    """
    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # List directories in remote ~/.claude/projects/ (simple and fast)
        result = subprocess.run(
            [get_command_path("ssh"), remote_host, 'ls -1 ~/.claude/projects/ | grep "^-"'],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        # Parse output - one directory name per line
        all_workspaces = [
            line.strip() for line in result.stdout.strip().split("\n") if line.strip()
        ]

        # Filter out remote caches to prevent circular fetching:
        # - remote_* = SSH remote caches
        # - wsl_* = WSL caches
        # These are already fetched data and shouldn't be re-fetched
        workspaces = [ws for ws in all_workspaces if is_native_workspace(ws)]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_remote_session_info(remote_host: Optional[str], remote_workspace: str) -> list:
    """Get session file information from remote workspace without downloading.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count
    """
    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []
    if not validate_workspace_name(remote_workspace):
        sys.stderr.write(f"Error: Invalid workspace name: {remote_workspace}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Get file stats from remote using find and stat
        # Output format: filename|size_bytes|mtime_epoch|line_count
        # Use sanitize_for_shell to safely quote the workspace name
        safe_workspace = sanitize_for_shell(remote_workspace)
        cmd = f"""cd ~/.claude/projects/{safe_workspace} && \
                  for f in *.jsonl; do \
                      [ -f "$f" ] || continue; \
                      size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null); \
                      mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null); \
                      lines=$(wc -l < "$f"); \
                      echo "$f|$size|$mtime|$lines"; \
                  done"""

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split("\n"):
            if not line or "|" not in line:
                continue

            parts = line.split("|")
            if len(parts) != SSH_SESSION_PARTS:
                continue

            filename, size_bytes, mtime_epoch, line_count = parts

            try:
                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)

                sessions.append(
                    {
                        "filename": filename,
                        "size_kb": size_kb,
                        "modified": modified,
                        "message_count": message_count,
                    }
                )
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def _validate_fetch_inputs(remote_host: Optional[str], remote_workspace: str) -> Optional[dict]:
    """Validate inputs for fetch_workspace_files. Returns error dict or None."""
    if not validate_remote_host(remote_host):
        return {
            "success": False,
            "files_copied": 0,
            "bytes": 0,
            "error": f"Invalid remote host: {remote_host}",
        }
    if not validate_workspace_name(remote_workspace):
        return {
            "success": False,
            "files_copied": 0,
            "bytes": 0,
            "error": f"Invalid workspace name: {remote_workspace}",
        }
    return None


def _escape_rsync_remote_path(path: str) -> str:
    """Escape special characters in rsync remote paths.

    Rsync interprets spaces and some special characters in remote paths.
    This escapes them with backslashes for proper handling.
    """
    # Escape spaces and other special characters that rsync interprets
    # Note: for local paths in subprocess list args, this isn't needed
    # But for remote paths (host:path), these chars need escaping
    special_chars = " '\"!#$&()*;<>?[]\\^`{|}~"
    result = []
    for char in path:
        if char in special_chars:
            result.append(f"\\{char}")
        else:
            result.append(char)
    escaped = "".join(result)
    # Leading dashes are treated as rsync options unless prefixed.
    if escaped.startswith("-"):
        escaped = f"./{escaped}"
    return escaped


def _convert_to_rsync_path(local_dir: Path) -> str:
    """Convert local path to rsync-compatible format."""
    local_path_posix = str(local_dir).replace("\\", "/")

    if ":" in local_path_posix and not local_path_posix.startswith("/"):
        drive_match = re.match(r"([A-Za-z]):(.*)", local_path_posix)
        if drive_match:
            drive_letter = drive_match.group(1).lower()
            path_part = drive_match.group(2)
            return f"/cygdrive/{drive_letter}{path_part}/"
    return local_path_posix + "/"


def _ensure_trailing_slash(path: str) -> str:
    """Ensure a trailing slash for rsync destination paths."""
    return path if path.endswith("/") else f"{path}/"


def _get_wsl_path_for_windows_path(path: Path) -> Optional[str]:
    """Convert a Windows path to a WSL path using wslpath."""
    wsl_cmd = get_command_path("wsl")
    try:
        win_path = str(path).replace("\\", "/")
        result = subprocess.run(
            [wsl_cmd, "wslpath", "-a", win_path],
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        wsl_path = result.stdout.strip()
        return wsl_path if wsl_path else None
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        return None


def _run_rsync_with_wsl_fallback(
    rsync_cmd: list[str], local_dir: Path, timeout: int
) -> subprocess.CompletedProcess:
    """Run rsync, preferring WSL rsync on Windows and falling back as needed."""
    if os.name == "nt":
        wsl_path = _get_wsl_path_for_windows_path(local_dir)
        if wsl_path:
            remote_path = rsync_cmd[-2]
            wsl_cmd = [
                get_command_path("wsl"),
                "rsync",
                *rsync_cmd[1:-2],
                remote_path,
                _ensure_trailing_slash(wsl_path),
            ]
            try:
                wsl_result = subprocess.run(
                    wsl_cmd, check=False, capture_output=True, text=True, timeout=timeout
                )
                if wsl_result.returncode == 0:
                    return wsl_result
            except (
                subprocess.SubprocessError,
                subprocess.TimeoutExpired,
                FileNotFoundError,
                OSError,
            ):
                pass

    result = subprocess.run(rsync_cmd, check=False, capture_output=True, text=True, timeout=timeout)
    if (
        result.returncode == 0
        or os.name != "nt"
        or result.returncode != RSYNC_PARTIAL_TRANSFER_CODE
    ):
        return result

    wsl_path = _get_wsl_path_for_windows_path(local_dir)
    if not wsl_path:
        return result

    remote_path = rsync_cmd[-2]
    wsl_cmd = [
        get_command_path("wsl"),
        "rsync",
        *rsync_cmd[1:-2],
        remote_path,
        _ensure_trailing_slash(wsl_path),
    ]

    try:
        return subprocess.run(wsl_cmd, check=False, capture_output=True, text=True, timeout=timeout)
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        return result


def _interpret_rsync_exit_code(code: int) -> tuple[bool, str]:
    """Interpret rsync exit code.

    Args:
        code: Exit code from rsync command

    Returns:
        Tuple of (is_partial_success, error_message)
    """
    return _RSYNC_EXIT_CODES.get(code, (False, f"Unknown error (code {code})"))


def _count_rsync_files(output: str) -> int:
    """Count files copied from rsync output."""
    lines = output.split("\n")
    return sum(
        1
        for line in lines
        if line.strip() and not line.startswith(("sending", "sent", "total", "building"))
    )


def fetch_workspace_files(
    remote_host: Optional[str], remote_workspace: str, local_projects_dir: Path, hostname: str
) -> dict:
    """Fetch all files from a remote workspace using rsync."""
    validation_error = _validate_fetch_inputs(remote_host, remote_workspace)
    if validation_error:
        return validation_error

    workspace_path = remote_workspace.lstrip("-")
    local_workspace = f"remote_{hostname}_{workspace_path}"
    local_dir = local_projects_dir / local_workspace

    if not is_safe_path(local_projects_dir, local_dir):
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "Path traversal detected"}

    local_dir.mkdir(parents=True, exist_ok=True)
    # Escape special characters in remote path for rsync
    escaped_workspace = _escape_rsync_remote_path(remote_workspace)
    remote_path = f"{remote_host}:~/.claude/projects/{escaped_workspace}/"

    try:
        local_path_str = _convert_to_rsync_path(local_dir)
        rsync_cmd = [
            get_command_path("rsync"),
            "-avh",
            "--partial",  # Keep partial files on interrupted transfer
            "--include=*.jsonl",
            "--exclude=*",
            remote_path,
            local_path_str,
        ]

        result = _run_rsync_with_wsl_fallback(rsync_cmd, local_dir, SSH_TIMEOUT)

        if result.returncode != 0:
            is_partial, error_msg = _interpret_rsync_exit_code(result.returncode)
            if is_partial:
                # Partial success - some files may have transferred
                return {
                    "success": True,
                    "files_copied": _count_rsync_files(result.stdout),
                    "local_dir": local_workspace,
                    "output": result.stdout,
                    "warning": error_msg,
                }
            return {
                "success": False,
                "files_copied": 0,
                "bytes": 0,
                "error": f"{error_msg}: {result.stderr}",
            }

        return {
            "success": True,
            "files_copied": _count_rsync_files(result.stdout),
            "local_dir": local_workspace,
            "output": result.stdout,
        }

    except subprocess.TimeoutExpired:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "Timeout"}
    except FileNotFoundError:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "rsync not found"}


# ============================================================================
# Gemini Remote Operations (SSH)
# ============================================================================


def gemini_list_remote_workspaces(remote_host: Optional[str]) -> list:
    """List Gemini workspace directories (project hashes) on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of project hash directory names that contain sessions
    """
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # List hash directories in ~/.gemini/tmp/ that have chats/ with sessions
        # Output format: hash directories that contain session files
        cmd = 'for d in ~/.gemini/tmp/*/chats; do [ -d "$d" ] && ls "$d"/*.json >/dev/null 2>&1 && basename "$(dirname "$d")"; done 2>/dev/null'

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            return []

        # Parse output - one hash directory per line
        workspaces = [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def gemini_get_remote_session_info(
    remote_host: Optional[str], project_hash: Optional[str] = None
) -> list:
    """Get Gemini session file information from remote without downloading.

    Args:
        remote_host: Remote host specification
        project_hash: Optional specific project hash to query (all if None)

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count, workspace
    """
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Get file stats and extract cwd from session files
        # For Gemini, we need to extract the project hash from path and optionally cwd from JSON
        if project_hash:
            safe_hash = sanitize_for_shell(project_hash)
            glob_pattern = f"~/.gemini/tmp/{safe_hash}/chats/session-*.json"
        else:
            glob_pattern = "~/.gemini/tmp/*/chats/session-*.json"

        # Output: filepath|size|mtime|msg_count
        cmd = f"""for f in {glob_pattern}; do
            [ -f "$f" ] || continue
            size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null)
            mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null)
            # Count messages array length (simple grep for "type" entries)
            msgs=$(grep -c '"type":' "$f" 2>/dev/null || echo 0)
            echo "$f|$size|$mtime|$msgs"
        done"""

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=60,
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split("\n"):
            if not line or "|" not in line:
                continue

            parts = line.split("|")
            if len(parts) != 4:  # noqa: PLR2004
                continue

            filepath, size_bytes, mtime_epoch, msg_count = parts

            try:
                # Extract project hash from path: ~/.gemini/tmp/<hash>/chats/session-*.json
                path_parts = filepath.split("/")
                hash_idx = path_parts.index("tmp") + 1 if "tmp" in path_parts else -1
                workspace_hash = (
                    path_parts[hash_idx]
                    if hash_idx > 0 and hash_idx < len(path_parts)
                    else "unknown"
                )

                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(msg_count)
                filename = path_parts[-1] if path_parts else filepath

                sessions.append(
                    {
                        "filename": filename,
                        "filepath": filepath,
                        "size_kb": size_kb,
                        "modified": modified,
                        "message_count": message_count,
                        "workspace": workspace_hash,
                        "agent": AGENT_GEMINI,
                    }
                )
            except (ValueError, OSError, IndexError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def gemini_fetch_remote_sessions(
    remote_host: Optional[str],
    local_cache_dir: Path,
    hostname: str,
    project_hash: Optional[str] = None,
) -> dict:
    """Fetch Gemini sessions from remote using rsync.

    Args:
        remote_host: Remote host specification
        local_cache_dir: Local directory to cache sessions
        hostname: Hostname for cache prefix
        project_hash: Optional specific project hash (all if None)

    Returns:
        Dict with success, files_copied, local_dir, error
    """
    if not validate_remote_host(remote_host):
        return {"success": False, "files_copied": 0, "error": f"Invalid remote host: {remote_host}"}

    # Create local cache directory for remote Gemini sessions
    local_dir = local_cache_dir / f"remote_{hostname}_gemini"
    local_dir.mkdir(parents=True, exist_ok=True)

    if project_hash:
        # Fetch specific project hash (escape special chars for rsync)
        escaped_hash = _escape_rsync_remote_path(project_hash)
        remote_path = f"{remote_host}:~/.gemini/tmp/{escaped_hash}/chats/"
        local_subdir = local_dir / project_hash / "chats"
        local_subdir.mkdir(parents=True, exist_ok=True)
    else:
        # Fetch all Gemini sessions
        remote_path = f"{remote_host}:~/.gemini/tmp/"
        local_subdir = local_dir

    try:
        local_path_str = _convert_to_rsync_path(local_subdir)
        rsync_cmd = [
            get_command_path("rsync"),
            "-avh",
            "--partial",  # Keep partial files on interrupted transfer
            "--include=*/",
            "--include=*/chats/",
            "--include=*/chats/session-*.json",
            "--exclude=*",
            remote_path,
            local_path_str,
        ]
        result = _run_rsync_with_wsl_fallback(rsync_cmd, local_subdir, SSH_TIMEOUT)

        if result.returncode != 0:
            is_partial, error_msg = _interpret_rsync_exit_code(result.returncode)
            if is_partial:
                # Partial success - some files may have transferred
                return {
                    "success": True,
                    "files_copied": _count_rsync_files(result.stdout),
                    "local_dir": str(local_dir),
                    "output": result.stdout,
                    "warning": error_msg,
                }
            return {
                "success": False,
                "files_copied": 0,
                "error": f"{error_msg}: {result.stderr}",
            }

        return {
            "success": True,
            "files_copied": _count_rsync_files(result.stdout),
            "local_dir": str(local_dir),
            "output": result.stdout,
        }

    except subprocess.TimeoutExpired:
        return {"success": False, "files_copied": 0, "error": "Timeout"}
    except FileNotFoundError:
        return {"success": False, "files_copied": 0, "error": "rsync not found"}


def gemini_fetch_remote_hash_index(remote_host: Optional[str], local_cache_dir: Path) -> dict:
    """Fetch Gemini hash index from remote host if it exists.

    Args:
        remote_host: Remote host specification
        local_cache_dir: Local directory to store the index

    Returns:
        Dict with hash->path mappings from remote, or empty dict
    """
    if not validate_remote_host(remote_host):
        return {}

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Check if remote has a hash index file (prefer new path, fall back to legacy)
        check_cmd = (
            "if [ -f ~/.agent-history/gemini_hash_index.json ]; then "
            "cat ~/.agent-history/gemini_hash_index.json; "
            "elif [ -f ~/.claude-history/gemini_hash_index.json ]; then "
            "cat ~/.claude-history/gemini_hash_index.json; "
            "fi"
        )

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, check_cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0 or not result.stdout.strip():
            return {}

        # Parse the JSON index
        try:
            index_data = json.loads(result.stdout)
            return index_data.get("hashes", {})
        except json.JSONDecodeError:
            return {}

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return {}


# ============================================================================
# Codex Remote Operations (SSH)
# ============================================================================


def codex_list_remote_workspaces(remote_host: Optional[str]) -> list:
    """List unique Codex workspaces (cwds) from remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of unique workspace paths extracted from session files
    """
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Extract unique cwds from all Codex session files
        # Codex stores cwd in the first line of each jsonl file
        cmd = 'grep -h \'"cwd":\' ~/.codex/sessions/*/*/*/*.jsonl 2>/dev/null | sed \'s/.*"cwd":"\\([^"]*\\)".*/\\1/\' | sort -u'

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=60,
        )

        if result.returncode != 0:
            return []

        # Parse output - one cwd per line
        workspaces = [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def codex_get_remote_session_info(
    remote_host: Optional[str], workspace_filter: Optional[str] = None
) -> list:
    """Get Codex session file information from remote without downloading.

    Args:
        remote_host: Remote host specification
        workspace_filter: Optional workspace pattern to filter by

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count, workspace
    """
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # After validation, remote_host is known to be a valid string
    assert remote_host is not None

    try:
        # Get file stats and extract cwd from each session file
        # Output: filepath|size|mtime|msg_count|cwd
        cmd = """for f in ~/.codex/sessions/*/*/*/*.jsonl; do
            [ -f "$f" ] || continue
            size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null)
            mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null)
            lines=$(wc -l < "$f")
            cwd=$(head -1 "$f" | sed 's/.*"cwd":"\\([^"]*\\)".*/\\1/' 2>/dev/null)
            echo "$f|$size|$mtime|$lines|$cwd"
        done"""

        result = subprocess.run(
            [get_command_path("ssh"), remote_host, cmd],
            check=False,
            capture_output=True,
            text=True,
            timeout=120,
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split("\n"):
            if not line or "|" not in line:
                continue

            parts = line.split("|")
            if len(parts) != 5:  # noqa: PLR2004
                continue

            filepath, size_bytes, mtime_epoch, line_count, cwd = parts

            try:
                # Apply workspace filter if specified
                if workspace_filter and workspace_filter not in cwd:
                    continue

                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)
                filename = filepath.split("/")[-1] if "/" in filepath else filepath

                # Extract short workspace name from cwd
                workspace_name = cwd.split("/")[-1] if cwd else "unknown"

                sessions.append(
                    {
                        "filename": filename,
                        "filepath": filepath,
                        "size_kb": size_kb,
                        "modified": modified,
                        "message_count": message_count,
                        "workspace": workspace_name,
                        "workspace_full": cwd,
                        "agent": AGENT_CODEX,
                    }
                )
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def codex_fetch_remote_sessions(
    remote_host: Optional[str], local_cache_dir: Path, hostname: str
) -> dict:
    """Fetch Codex sessions from remote using rsync.

    Args:
        remote_host: Remote host specification
        local_cache_dir: Local directory to cache sessions
        hostname: Hostname for cache prefix

    Returns:
        Dict with success, files_copied, local_dir, error
    """
    if not validate_remote_host(remote_host):
        return {"success": False, "files_copied": 0, "error": f"Invalid remote host: {remote_host}"}

    # Create local cache directory for remote Codex sessions
    local_dir = local_cache_dir / f"remote_{hostname}_codex"
    local_dir.mkdir(parents=True, exist_ok=True)

    remote_path = f"{remote_host}:~/.codex/sessions/"

    try:
        local_path_str = _convert_to_rsync_path(local_dir)
        rsync_cmd = [
            get_command_path("rsync"),
            "-avh",
            "--partial",  # Keep partial files on interrupted transfer
            "--include=*/",
            "--include=*/*.jsonl",
            "--exclude=*",
            remote_path,
            local_path_str,
        ]

        result = _run_rsync_with_wsl_fallback(rsync_cmd, local_dir, SSH_TIMEOUT)

        if result.returncode != 0:
            is_partial, error_msg = _interpret_rsync_exit_code(result.returncode)
            if is_partial:
                # Partial success - some files may have transferred
                return {
                    "success": True,
                    "files_copied": _count_rsync_files(result.stdout),
                    "local_dir": str(local_dir),
                    "output": result.stdout,
                    "warning": error_msg,
                }
            return {
                "success": False,
                "files_copied": 0,
                "error": f"{error_msg}: {result.stderr}",
            }

        return {
            "success": True,
            "files_copied": _count_rsync_files(result.stdout),
            "local_dir": str(local_dir),
            "output": result.stdout,
        }

    except subprocess.TimeoutExpired:
        return {"success": False, "files_copied": 0, "error": "Timeout"}
    except FileNotFoundError:
        return {"success": False, "files_copied": 0, "error": "rsync not found"}


# ============================================================================
# Alias Storage
# ============================================================================


CONFIG_DIR_NAME = ".agent-history"
LEGACY_CONFIG_DIR_NAME = ".claude-history"


def _get_config_dirs() -> tuple[Path, Path]:
    """Return (new_config_dir, legacy_config_dir) under HOME."""
    home_env = os.environ.get("HOME")
    home = Path(home_env) if home_env else Path.home()
    return home / CONFIG_DIR_NAME, home / LEGACY_CONFIG_DIR_NAME


def _migrate_legacy_config_dir(new_dir: Path, legacy_dir: Path) -> Path:
    """Migrate legacy ~/.claude-history to ~/.agent-history if needed."""
    if new_dir.exists():
        if legacy_dir.exists():
            try:
                shutil.copytree(legacy_dir, new_dir, dirs_exist_ok=True)
                shutil.rmtree(legacy_dir, ignore_errors=True)
            except OSError as e:
                sys.stderr.write(
                    f"Warning: Could not clean up legacy config dir {legacy_dir}: {e}\n"
                )
        return new_dir
    if not legacy_dir.exists():
        return new_dir

    try:
        legacy_dir.rename(new_dir)
        return new_dir
    except OSError:
        try:
            shutil.copytree(legacy_dir, new_dir, dirs_exist_ok=True)
            shutil.rmtree(legacy_dir, ignore_errors=True)
            return new_dir
        except OSError as e:
            sys.stderr.write(
                f"Warning: Could not migrate legacy config dir {legacy_dir} -> {new_dir}: {e}\n"
            )
            return legacy_dir


def get_config_dir() -> Path:
    """Get the config storage directory (~/.agent-history/, migrates legacy on first use)."""
    new_dir, legacy_dir = _get_config_dirs()
    return _migrate_legacy_config_dir(new_dir, legacy_dir)


def get_aliases_dir() -> Path:
    """Get the aliases storage directory (~/.agent-history/)."""
    return get_config_dir()


def get_aliases_file() -> Path:
    """Get the aliases storage file path."""
    return get_aliases_dir() / "aliases.json"


def get_config_file() -> Path:
    """Get the config file path."""
    return get_config_dir() / "config.json"


def load_config() -> dict:
    """Load config from storage file. Returns empty structure if not found."""
    config_file = get_config_file()
    if not config_file.exists():
        return {"version": 1, "sources": []}

    try:
        with open(config_file, encoding="utf-8") as f:
            data = json.load(f)
            if "sources" not in data:
                data["sources"] = []
            if "version" not in data:
                data["version"] = 1
            return data
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Warning: Could not load config file: {e}\n")
        return {"version": 1, "sources": []}


def save_config(data: dict) -> bool:
    """Save config to storage file.

    Args:
        data: Config data dictionary with 'version' and settings

    Returns:
        True on success, False on failure (error printed to stderr)

    Side Effects:
        - Creates ~/.agent-history/ directory with mode 0o700 if missing
        - Writes to ~/.agent-history/config.json with mode 0o600
    """
    config_dir = get_config_dir()
    config_file = get_config_file()

    try:
        config_dir.mkdir(parents=True, exist_ok=True)
        # Set secure permissions on config directory (owner-only access)
        os.chmod(config_dir, 0o700)
        if "version" not in data:
            data["version"] = 1
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        # Set secure permissions on config file (owner read/write only)
        os.chmod(config_file, 0o600)
        return True
    except OSError as e:
        sys.stderr.write(f"Error saving config: {e}\n")
        return False


def get_saved_sources() -> list:
    """Get list of saved remote sources."""
    config = load_config()
    return config.get("sources", [])


def load_aliases() -> dict:
    """Load aliases from storage file. Returns empty structure if not found."""
    aliases_file = get_aliases_file()
    if not aliases_file.exists():
        return {"version": 1, "aliases": {}}

    try:
        with open(aliases_file, encoding="utf-8") as f:
            data = json.load(f)
            # Ensure structure is valid
            if "aliases" not in data:
                data["aliases"] = {}
            if "version" not in data:
                data["version"] = 1
            return _normalize_aliases(data)
    except json.JSONDecodeError:
        # Corrupted JSON - create backup and warn user prominently
        backup_path = aliases_file.with_suffix(
            f".corrupted.{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        try:
            import shutil

            shutil.copy2(aliases_file, backup_path)
            sys.stderr.write(f"Warning: aliases file corrupted, backed up to {backup_path}\n")
        except OSError:
            sys.stderr.write(f"Warning: aliases file corrupted at {aliases_file}\n")
        return {"version": 1, "aliases": {}}
    except OSError as e:
        sys.stderr.write(f"Warning: Could not load aliases file: {e}\n")
        return {"version": 1, "aliases": {}}


def _lock_file(file_handle, exclusive: bool = True):
    """Lock a file handle (cross-platform).

    Args:
        file_handle: Open file handle
        exclusive: True for write lock, False for read lock
    """
    if platform.system() == "Windows":
        import msvcrt

        msvcrt.locking(file_handle.fileno(), msvcrt.LK_NBLCK, 1)  # type: ignore[attr-defined]
    else:
        import fcntl

        lock_type = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH
        fcntl.flock(file_handle.fileno(), lock_type | fcntl.LOCK_NB)


def _unlock_file(file_handle):
    """Unlock a file handle (cross-platform)."""
    if platform.system() == "Windows":
        import msvcrt

        try:
            msvcrt.locking(file_handle.fileno(), msvcrt.LK_UNLCK, 1)  # type: ignore[attr-defined]
        except OSError:
            pass
    else:
        import fcntl

        fcntl.flock(file_handle.fileno(), fcntl.LOCK_UN)


def save_aliases(data: dict) -> bool:
    """Save aliases to storage file with file locking.

    Args:
        data: Alias data dictionary with 'version' and 'aliases' keys

    Returns:
        True on success, False on failure (error printed to stderr)

    Side Effects:
        - Creates ~/.agent-history/ directory with mode 0o700 if missing
        - Writes to ~/.agent-history/aliases.json
        - Uses file locking to prevent race conditions
    """
    aliases_dir = get_aliases_dir()
    aliases_file = get_aliases_file()

    try:
        # Create directory if needed
        aliases_dir.mkdir(parents=True, exist_ok=True)
        # Set secure permissions on aliases directory (owner-only access)
        os.chmod(aliases_dir, 0o700)

        # Ensure version is set
        if "version" not in data:
            data["version"] = 1

        # Write with file locking to prevent concurrent updates
        with open(aliases_file, "w", encoding="utf-8") as f:
            try:
                _lock_file(f, exclusive=True)
                json.dump(data, f, indent=2)
            finally:
                _unlock_file(f)

        # Set secure permissions on aliases file (owner read/write only)
        os.chmod(aliases_file, 0o600)
        return True
    except BlockingIOError:
        sys.stderr.write("Error: Aliases file is locked by another process\n")
        return False
    except OSError as e:
        sys.stderr.write(f"Error: Could not save aliases: {e}\n")
        return False


def _sanitize_alias_workspace_entry(workspace: str) -> str:
    """Normalize alias workspace entries, handling legacy absolute paths."""
    if not workspace:
        return workspace

    lowered = workspace.lower()

    if lowered.startswith("/mnt/"):
        return path_to_encoded_workspace(workspace)

    if workspace.startswith("/"):
        return path_to_encoded_workspace(workspace)

    if lowered.startswith("-mnt-") and len(workspace) > MNT_ENCODED_PREFIX_LEN:
        drive_letter = workspace[5]
        remainder = workspace[MNT_ENCODED_PREFIX_LEN:]
        if drive_letter.isalpha():
            return f"{drive_letter.upper()}--{remainder}"

    return workspace


def _normalize_aliases(data: dict) -> dict:
    """Normalize stored alias workspace entries for cross-platform compatibility."""
    aliases = data.get("aliases", {})
    changed = False

    for sources in aliases.values():
        for source_key, workspaces in list(sources.items()):
            normalized = []
            for workspace in workspaces:
                new_value = _sanitize_alias_workspace_entry(workspace)
                if new_value != workspace:
                    changed = True
                normalized.append(new_value)
            sources[source_key] = normalized

    if changed:
        save_aliases(data)
    return data


def _try_encode_workspace(candidate: str) -> str | None:
    """Attempt to encode a workspace path/name to standard encoded form."""
    try:
        lower = candidate.lower()
        if candidate.startswith("-") or _is_windows_encoded_path(candidate):
            return candidate
        if lower.startswith("/mnt/") or candidate.startswith("/"):
            return path_to_encoded_workspace(candidate)
        if len(candidate) > 1 and candidate[1] == ":":
            return _convert_windows_path_to_encoded(candidate)
    except Exception:
        pass
    return None


def _get_readable_from_encoded(encoded: str | None, fallback: str) -> str | None:
    """Convert encoded workspace to readable form, or use fallback."""
    if encoded:
        try:
            return normalize_workspace_name(encoded, verify_local=False)
        except Exception:
            return None
    return fallback


def _normalize_alias_workspace_input(workspace_input: str) -> tuple[str | None, str | None]:
    """Return (encoded, readable) forms for flexible alias operations."""
    if workspace_input is None:
        return None, None

    candidate = workspace_input.strip()
    if not candidate:
        return None, None

    # Support remote-style input user@host:/path
    if ":" in candidate and "@" in candidate.split(":", 1)[0]:
        candidate = candidate.split(":", 1)[1].strip()

    encoded = _try_encode_workspace(candidate)
    readable = _get_readable_from_encoded(encoded, candidate)

    return encoded, readable


def _match_alias_workspace(workspaces: list, workspace_input: str) -> str | None:
    """Find matching workspace entry from alias list."""
    encoded_input, readable_input = _normalize_alias_workspace_input(workspace_input)
    candidates = {workspace_input}
    if encoded_input:
        candidates.add(encoded_input)

    for entry in workspaces:
        if entry in candidates:
            return entry
        try:
            entry_readable = normalize_workspace_name(entry, verify_local=False)
        except Exception:
            entry_readable = None
        if readable_input and entry_readable == readable_input:
            return entry
    return None


def get_source_key(remote_host=None, wsl_distro=None, windows_user=None) -> str:
    """Get the source key for aliases based on current context."""
    if remote_host:
        if remote_host.startswith("wsl://"):
            return f"wsl:{remote_host[WSL_PREFIX_LEN:]}"
        elif remote_host.startswith("windows:"):
            return (
                f"windows:{remote_host[WINDOWS_PREFIX_LEN:]}"
                if len(remote_host) > WINDOWS_PREFIX_LEN
                else "windows"
            )
        elif remote_host == "windows":
            return "windows"
        else:
            # Preserve full remote spec (user@host) for SSH authentication
            return f"remote:{remote_host}"
    elif wsl_distro:
        return f"wsl:{wsl_distro}"
    elif windows_user is not None:
        return f"windows:{windows_user}" if windows_user else "windows"
    else:
        return "local"


def get_alias_session_count(alias_name: str, aliases_data: dict) -> int:
    """Count total sessions across all homes for an alias."""
    if alias_name not in aliases_data.get("aliases", {}):
        return 0

    alias_config = aliases_data["aliases"][alias_name]
    total = 0

    for _source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            try:
                # Try to get session count for this workspace
                sessions = get_workspace_sessions(workspace)
                total += len(sessions)
            except (OSError, PermissionError):
                pass  # Workspace may not exist or be accessible

    return total


def resolve_alias_workspaces(alias_name: str, source_filter: Optional[str] = None) -> list:
    """
    Resolve an alias to a list of (source_key, workspace, sessions) tuples.
    If source_filter is provided, only include matching sources.
    """
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        return []

    alias_config = aliases_data["aliases"][alias_name]
    results = []

    for source_key, workspaces in alias_config.items():
        if source_filter and source_key != source_filter:
            continue

        for workspace in workspaces:
            results.append((source_key, workspace))

    return results


def get_alias_for_workspace(workspace: str, source_key: str = "local") -> Optional[str]:
    """
    Find the alias that contains a given workspace, if any.

    Args:
        workspace: Encoded workspace name (e.g., '-home-user-project')
        source_key: Source identifier like 'local', 'windows', 'wsl:distro', 'remote:hostname'

    Returns:
        Alias name if found, None otherwise
    """
    aliases_data = load_aliases()

    for alias_name, alias_config in aliases_data.get("aliases", {}).items():
        # Check each source in the alias
        for src_key, workspaces in alias_config.items():
            # Handle source key matching (e.g., 'local' matches 'local', 'wsl:Ubuntu' matches 'wsl')
            if (
                src_key == source_key
                or source_key.startswith(src_key + ":")
                or src_key.startswith(source_key + ":")
            ):
                if workspace in workspaces:
                    return alias_name
            # Also check if workspace name (without source prefix) matches
            ws_name = get_workspace_name_from_path(workspace)
            for ws in workspaces:
                if get_workspace_name_from_path(ws) == ws_name:
                    return alias_name

    return None


def _get_remote_sessions(
    remote_spec: str, workspace: str, since_date, until_date, fetch_remote: bool
) -> list:
    """Get sessions from a remote source."""
    try:
        local_projects = get_claude_projects_dir()
    except (OSError, SystemExit):
        return []

    # Use hostname only for cache path (no @ symbol in directory names)
    hostname = get_remote_hostname(remote_spec)
    cached_workspace = f"remote_{hostname}_{workspace.lstrip('-')}"
    cached_path = local_projects / cached_workspace

    if cached_path.exists():
        return get_workspace_sessions(
            cached_workspace,
            quiet=True,
            since_date=since_date,
            until_date=until_date,
            include_cached=True,
        )

    if not fetch_remote:
        return []

    # Use full remote_spec for SSH operations (includes username)
    if not check_ssh_connection(remote_spec):
        sys.stderr.write(f"Cannot connect to remote '{remote_spec}' - skipping\n")
        return []

    sys.stderr.write(f"Fetching from {remote_spec}:{workspace}...\n")
    result = fetch_workspace_files(remote_spec, workspace, local_projects, hostname)

    if result.get("success"):
        if result.get("warning"):
            sys.stderr.write(f"Warning: {result['warning']}\n")
        return get_workspace_sessions(
            cached_workspace,
            quiet=True,
            since_date=since_date,
            until_date=until_date,
            include_cached=True,
        )
    return []


def get_sessions_for_source(
    source_key: str, workspace: str, since_date=None, until_date=None, fetch_remote: bool = False
) -> list:
    """Get sessions from a workspace on a specific source."""
    try:
        if source_key == "local":
            return get_workspace_sessions(
                workspace, quiet=True, since_date=since_date, until_date=until_date
            )

        if source_key == "windows" or source_key.startswith("windows:"):
            username = source_key.split(":", 1)[1] if ":" in source_key else None
            projects_dir = get_windows_projects_dir(username)
            if not projects_dir:
                return []
            return get_workspace_sessions(
                workspace,
                quiet=True,
                since_date=since_date,
                until_date=until_date,
                projects_dir=projects_dir,
            )

        if source_key.startswith("wsl:"):
            distro = source_key.split(":", 1)[1]
            wsl_projects_dir = get_wsl_projects_dir(distro)
            if wsl_projects_dir is None:
                return []
            return get_workspace_sessions(
                workspace,
                quiet=True,
                since_date=since_date,
                until_date=until_date,
                projects_dir=wsl_projects_dir,
            )

        if source_key.startswith("remote:"):
            remote_spec = source_key.split(":", 1)[1]
            return _get_remote_sessions(
                remote_spec, workspace, since_date, until_date, fetch_remote
            )

        return []

    except (OSError, PermissionError):
        return []


# ============================================================================
# Alias Commands
# ============================================================================


def _print_workspace_with_count(source_key: str, workspace: str) -> int:
    """Print workspace with session count. Returns count."""
    ws_readable = normalize_workspace_name(workspace)
    sessions = get_sessions_for_source(source_key, workspace)
    count = len(sessions)
    if count > 0 or source_key == "local":
        print(f"    {ws_readable}\t{count} sessions")
    else:
        print(f"    {ws_readable}\t(not cached)")
    return count


def _print_alias_sources(sources: dict, show_counts: bool) -> int:
    """Print alias sources and return total session count."""
    total_sessions = 0
    for source_key, workspaces in sorted(sources.items()):
        if not workspaces:
            continue
        print(f"  {source_key}:")
        for workspace in workspaces:
            if show_counts:
                total_sessions += _print_workspace_with_count(source_key, workspace)
            else:
                print(f"    {normalize_workspace_name(workspace)}")
    return total_sessions


def cmd_alias_list(args):
    """List all aliases with their workspaces."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})
    show_counts = getattr(args, "counts", False)

    if not aliases:
        print("No aliases defined.")
        print("\nCreate an alias with:")
        print("  agent-history alias create <name>")
        print("  agent-history alias add <name> <workspace>")
        return

    # Warn about slow sources when counting
    if show_counts:
        has_slow_sources = any(
            source_key in ("windows", "wsl") or source_key.startswith(("windows:", "wsl:"))
            for sources in aliases.values()
            for source_key in sources.keys()
        )
        if has_slow_sources:
            sys.stderr.write("Accessing Windows/WSL sources...\n")
            sys.stderr.flush()

    for alias_name, sources in sorted(aliases.items()):
        print(f"\n{alias_name}:")
        total_sessions = _print_alias_sources(sources, show_counts)
        if show_counts:
            print(f"  Total: {total_sessions} sessions")


def cmd_alias_show(args):
    """Show details of a single alias."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    alias_name = args.name
    if alias_name not in aliases:
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    sources = aliases[alias_name]
    total_sessions = 0

    print(f"{alias_name}:")

    for source_key, workspaces in sorted(sources.items()):
        if workspaces:
            print(f"  {source_key}:")
            for workspace in workspaces:
                # Use source-aware function to get sessions
                sessions = get_sessions_for_source(source_key, workspace)
                count = len(sessions)
                total_sessions += count
                ws_readable = normalize_workspace_name(workspace)
                if count > 0 or source_key == "local":
                    print(f"    {ws_readable}\t{count} sessions")
                else:
                    print(f"    {ws_readable}\t{count} sessions (not cached)")

    print(f"  Total: {total_sessions} sessions")


def cmd_alias_create(args):
    """Create a new empty alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' already exists\n")
        sys.exit(1)

    aliases_data["aliases"][alias_name] = {}

    if save_aliases(aliases_data):
        print(f"Created alias '{alias_name}'")
    else:
        sys.exit(1)


def cmd_alias_delete(args):
    """Delete an alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    del aliases_data["aliases"][alias_name]

    if save_aliases(aliases_data):
        print(f"Deleted alias '{alias_name}'")
    else:
        sys.exit(1)


def _build_all_homes_sources(args) -> list:
    """Build list of all homes to check for alias add."""
    sources = [("local", None)]

    # WSL distributions (if on Windows) or Windows users (if on WSL/Linux)
    if sys.platform == "win32":
        try:
            wsl_distros = get_wsl_distributions()
            for distro in wsl_distros:
                # distro is a dict with 'name', 'username', 'has_claude' keys
                distro_name = distro["name"]
                sources.append((f"wsl:{distro_name}", distro_name))
        except (OSError, subprocess.SubprocessError):
            pass
    else:
        try:
            windows_users = get_windows_users_with_claude()
            for user_info in windows_users:
                username = user_info.get("username", "")
                sources.append((f"windows:{username}" if username else "windows", username))
        except (OSError, subprocess.SubprocessError):
            pass

    # SSH remotes - store full spec (user@host) for authentication
    remotes = getattr(args, "remotes", None) or []
    if getattr(args, "remote", None):
        remotes.append(args.remote)
    for remote in remotes:
        sources.append((f"remote:{remote}", remote))

    return sources


def _resolve_workspaces_for_source(
    raw_workspaces: list, source_key: str, source_param, use_picker: bool, args
) -> list:
    """Resolve workspaces to add for a given source."""
    if use_picker:
        return interactive_workspace_picker(source_key, args)

    workspaces_to_add = []
    remote_value = source_param if source_key.startswith("remote:") else None
    lookup_args = ListCommandArgs(remote=remote_value)

    for ws_input in raw_workspaces:
        resolved = resolve_workspace_input(ws_input, source_key, lookup_args)
        if resolved:
            workspaces_to_add.extend(resolved)
    return workspaces_to_add


def _add_workspaces_to_alias(
    aliases_data: dict, alias_name: str, source_key: str, workspaces: list
) -> int:
    """Add workspaces to an alias, avoiding duplicates. Returns count added."""
    if source_key not in aliases_data["aliases"][alias_name]:
        aliases_data["aliases"][alias_name][source_key] = []

    added = 0
    for workspace in workspaces:
        if workspace not in aliases_data["aliases"][alias_name][source_key]:
            aliases_data["aliases"][alias_name][source_key].append(workspace)
            added += 1
    return added


def _alias_add_all_homes(
    args, aliases_data: dict, alias_name: str, raw_workspaces: list, use_picker: bool
):
    """Handle --ah (all homes) mode for alias add."""
    sources_to_check = _build_all_homes_sources(args)

    total_added = 0
    for source_key, source_param in sources_to_check:
        workspaces_to_add = _resolve_workspaces_for_source(
            raw_workspaces, source_key, source_param, use_picker, args
        )
        if not workspaces_to_add:
            continue

        added = _add_workspaces_to_alias(aliases_data, alias_name, source_key, workspaces_to_add)
        if added > 0:
            print(f"[{source_key}] Added {added} workspace(s)")
            total_added += added

    if save_aliases(aliases_data):
        print(f"Added {total_added} workspace(s) to alias '{alias_name}' from all homes")
    else:
        sys.exit(1)


def _alias_add_single_source(
    args, aliases_data: dict, alias_name: str, raw_workspaces: list, use_picker: bool
):
    """Handle single source mode for alias add."""
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,
        windows_user="" if windows_flag else None,
    )

    # Resolve workspaces
    if use_picker:
        workspaces_to_add = interactive_workspace_picker(source_key, args)
        if not workspaces_to_add:
            print("No workspaces selected.")
            return
    else:
        workspaces_to_add = []
        for ws_input in raw_workspaces:
            resolved = resolve_workspace_input(ws_input, source_key, args)
            if resolved:
                workspaces_to_add.extend(resolved)
            else:
                sys.stderr.write(f"Warning: No workspace found matching '{ws_input}'\n")

        if not workspaces_to_add:
            sys.stderr.write("Error: No valid workspaces found.\n")
            sys.exit(1)

    added = _add_workspaces_to_alias(aliases_data, alias_name, source_key, workspaces_to_add)

    if save_aliases(aliases_data):
        print(f"Added {added} workspace(s) to alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def cmd_alias_add(args):
    """Add workspace(s) to an alias."""
    aliases_data = load_aliases()

    # Determine alias name (auto-detect from cwd if not provided)
    alias_name = args.name
    if not alias_name:
        alias_name = Path.cwd().name

    # Get workspace patterns
    raw_workspaces = args.workspaces if args.workspaces else []
    use_picker = getattr(args, "pick", False)

    if not raw_workspaces and not use_picker:
        sys.stderr.write("Error: No workspaces specified. Use --pick for interactive mode.\n")
        sys.exit(1)

    # Ensure alias exists
    if alias_name not in aliases_data.get("aliases", {}):
        aliases_data["aliases"][alias_name] = {}

    # Dispatch to appropriate handler
    if getattr(args, "all_homes", False):
        _alias_add_all_homes(args, aliases_data, alias_name, raw_workspaces, use_picker)
    else:
        _alias_add_single_source(args, aliases_data, alias_name, raw_workspaces, use_picker)


def _display_workspace_list(workspaces: list, source_key: str):
    """Display numbered list of workspaces."""
    print(f"\nAvailable workspaces [{source_key}]:")
    for i, ws in enumerate(sorted(workspaces), 1):
        ws_readable = normalize_workspace_name(ws)
        print(f"  {i}. {ws_readable}")


def _parse_workspace_selection(selection: str, sorted_workspaces: list) -> list:
    """Parse user selection into list of workspace names."""
    selected = []
    for part in selection.split():
        try:
            idx = int(part) - 1
            if 0 <= idx < len(sorted_workspaces):
                selected.append(sorted_workspaces[idx])
        except ValueError:
            continue
    return selected


def interactive_workspace_picker(source_key: str, args) -> list:
    """Interactive workspace picker. Returns list of selected workspace names."""
    try:
        workspaces = _get_workspaces_for_source(source_key, args)
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error listing workspaces: {e}\n")
        return []

    if not workspaces:
        print("No workspaces found.")
        return []

    _display_workspace_list(workspaces, source_key)

    print("\nEnter workspace numbers (space-separated), or 'q' to quit:")
    try:
        selection = input("> ").strip()
    except (EOFError, KeyboardInterrupt):
        return []

    if selection.lower() == "q":
        return []

    return _parse_workspace_selection(selection, sorted(workspaces))


def cmd_alias_remove(args):
    """Remove workspace from an alias."""
    aliases_data = load_aliases()

    alias_name = args.name
    workspace = args.workspace

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    # Determine source key
    # For alias remove, --wsl and --windows are boolean flags
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,  # Empty string means auto-detect
        windows_user="" if windows_flag else None,  # Empty string means auto-detect
    )

    alias_config = aliases_data["aliases"][alias_name]

    if source_key not in alias_config:
        sys.stderr.write(f"Error: No workspaces from '{source_key}' in alias '{alias_name}'\n")
        sys.exit(1)

    target = _match_alias_workspace(alias_config[source_key], workspace)
    if not target:
        sys.stderr.write(
            f"Error: Workspace '{workspace}' not in alias '{alias_name}' [{source_key}]\n"
        )
        sys.exit(1)

    alias_config[source_key].remove(target)

    # Clean up empty source
    if not alias_config[source_key]:
        del alias_config[source_key]

    if not alias_config:
        del aliases_data["aliases"][alias_name]

    if save_aliases(aliases_data):
        print(f"Removed workspace from alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def cmd_alias_config_export(args):
    """Export aliases configuration to JSON (stdout)."""
    aliases_data = load_aliases()
    print(pretty_json(aliases_data))


def _collect_alias_sessions(alias_config: dict, since_date, until_date) -> list:
    """Collect sessions from all workspaces in an alias configuration."""
    all_sessions = []
    for source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            sessions = get_sessions_for_source(
                source_key, workspace, since_date=since_date, until_date=until_date
            )
            for session in sessions:
                session["source"] = source_key
                all_sessions.append(session)
    return all_sessions


def _format_date_range_message(since_date, until_date) -> str:
    """Format date range for error messages."""
    parts = []
    if since_date:
        parts.append(f"from {since_date.strftime('%Y-%m-%d')}")
    if until_date:
        parts.append(f"until {until_date.strftime('%Y-%m-%d')}")
    return " ".join(parts)


def _get_alias_workspace_patterns(alias_config: dict) -> list:
    """Extract workspace patterns from alias config for multi-agent scanning."""
    patterns = set()
    for source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            # Convert encoded workspace back to pattern
            readable = normalize_workspace_name(workspace)
            # Use the last path component as pattern
            pattern = readable.split("/")[-1] if "/" in readable else readable
            if pattern:
                patterns.add(pattern)
    return list(patterns)


def _filter_alias_config_by_flags(alias_config: dict, args) -> dict:
    """Filter alias config based on source selection flags."""
    local_flag = getattr(args, "local", False)
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    remotes = list(getattr(args, "remotes", None) or [])
    if getattr(args, "remote", None):
        remotes.append(args.remote)

    if not (local_flag or wsl_flag or windows_flag or remotes):
        return alias_config

    filtered = {}
    for source_key, workspaces in alias_config.items():
        if local_flag and source_key == "local":
            filtered[source_key] = workspaces
        elif wsl_flag and source_key.startswith("wsl:"):
            filtered[source_key] = workspaces
        elif windows_flag and (source_key == "windows" or source_key.startswith("windows:")):
            filtered[source_key] = workspaces
        elif remotes and source_key.startswith("remote:"):
            remote_name = source_key.split("remote:", 1)[-1]
            if remote_name in remotes:
                filtered[source_key] = workspaces

    return filtered


def _collect_non_claude_alias_sessions(  # noqa: C901, PLR0912, PLR0915
    patterns: list,
    since_date,
    until_date,
    agent: str,
    source_keys: Optional[list] = None,
) -> list:
    """Collect Codex and Gemini sessions for alias patterns."""
    source_keys = source_keys or ["local"]
    sessions = []
    for source_key in source_keys:
        if source_key == "local":
            for pattern in patterns:
                if agent in ("auto", AGENT_CODEX):
                    codex_sessions = codex_scan_sessions(
                        pattern=pattern, since_date=since_date, until_date=until_date
                    )
                    for s in codex_sessions:
                        s["source"] = "local"
                    sessions.extend(codex_sessions)
                if agent in ("auto", AGENT_GEMINI):
                    gemini_sessions = gemini_scan_sessions(
                        pattern=pattern, since_date=since_date, until_date=until_date
                    )
                    for s in gemini_sessions:
                        s["source"] = "local"
                    sessions.extend(gemini_sessions)
            continue

        if source_key.startswith("windows"):
            username = source_key.split(":", 1)[1] if ":" in source_key else None
            windows_users = None
            if username is None:
                windows_users = get_windows_users_with_claude()
            for agent_type in (AGENT_CODEX, AGENT_GEMINI):
                if agent not in ("auto", agent_type):
                    continue
                if windows_users is not None:
                    for user_info in windows_users:
                        user_name = user_info.get("username") or None
                        sessions_dir = get_agent_windows_dir(user_name, agent_type)
                        if not sessions_dir or not sessions_dir.exists():
                            continue
                        seen_files = set()
                        for pattern in patterns or [""]:
                            for session in _scan_codex_gemini_sessions(
                                agent_type,
                                [pattern],
                                since_date,
                                until_date,
                                sessions_dir,
                                source_key,
                                skip_message_count=False,
                            ):
                                file_key = str(session.get("file", ""))
                                if file_key in seen_files:
                                    continue
                                seen_files.add(file_key)
                                sessions.append(session)
                else:
                    sessions_dir = get_agent_windows_dir(username, agent_type)
                    if not sessions_dir or not sessions_dir.exists():
                        continue
                    seen_files = set()
                    for pattern in patterns or [""]:
                        for session in _scan_codex_gemini_sessions(
                            agent_type,
                            [pattern],
                            since_date,
                            until_date,
                            sessions_dir,
                            source_key,
                            skip_message_count=False,
                        ):
                            file_key = str(session.get("file", ""))
                            if file_key in seen_files:
                                continue
                            seen_files.add(file_key)
                            sessions.append(session)
            continue

        if source_key.startswith("wsl:"):
            distro_name = source_key.split(":", 1)[1]
            for agent_type in (AGENT_CODEX, AGENT_GEMINI):
                if agent not in ("auto", agent_type):
                    continue
                sessions_dir = get_agent_wsl_dir(distro_name, agent_type)
                if not sessions_dir or not sessions_dir.exists():
                    continue
                seen_files = set()
                for pattern in patterns or [""]:
                    for session in _scan_codex_gemini_sessions(
                        agent_type,
                        [pattern],
                        since_date,
                        until_date,
                        sessions_dir,
                        source_key,
                        skip_message_count=False,
                    ):
                        file_key = str(session.get("file", ""))
                        if file_key in seen_files:
                            continue
                        seen_files.add(file_key)
                        sessions.append(session)
            continue

        if source_key.startswith("remote:"):
            remote_host = source_key.split("remote:", 1)[1]
            if not check_ssh_connection(remote_host):
                sys.stderr.write(f"Warning: Cannot connect to remote: {remote_host}\n")
                continue
            hostname = get_remote_hostname(remote_host)
            if agent in ("auto", AGENT_CODEX):
                codex_sessions = _get_batch_ssh_codex_sessions(
                    remote_host, patterns, since_date, until_date, hostname
                )
                for s in codex_sessions:
                    s["source"] = source_key
                sessions.extend(codex_sessions)
            if agent in ("auto", AGENT_GEMINI):
                gemini_sessions = _get_batch_ssh_gemini_sessions(
                    remote_host, patterns, since_date, until_date, hostname
                )
                for s in gemini_sessions:
                    s["source"] = source_key
                sessions.extend(gemini_sessions)
    return sessions


def _print_alias_sessions(sessions: list):
    """Print alias sessions in tabular format."""
    print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
    for session in sessions:
        session_agent = session.get("agent", AGENT_CLAUDE)
        print(
            f"{session_agent}\t{session['source']}\t{session['workspace_readable']}\t"
            f"{session['filename']}\t{session['message_count']}\t"
            f"{session['modified'].strftime('%Y-%m-%d')}"
        )


def cmd_alias_lss(
    alias_name: str, since_date=None, until_date=None, agent: str = "auto", args=None
):
    """List sessions across all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    if args is None:
        args = SimpleNamespace(local=False, wsl=False, windows=False, remotes=[], remote=None)

    alias_config = _filter_alias_config_by_flags(aliases_data["aliases"][alias_name], args)
    all_sessions = []

    # Collect Claude sessions
    if agent in ("auto", AGENT_CLAUDE):
        all_sessions = _collect_alias_sessions(alias_config, since_date, until_date)

    # Collect Codex/Gemini sessions
    if agent in ("auto", AGENT_CODEX, AGENT_GEMINI):
        patterns = _get_alias_workspace_patterns(alias_config)
        all_sessions.extend(
            _collect_non_claude_alias_sessions(
                patterns, since_date, until_date, agent, source_keys=list(alias_config.keys())
            )
        )

    all_sessions.sort(key=lambda s: s["modified"], reverse=True)

    if not all_sessions:
        if since_date or until_date:
            sys.stderr.write(
                f"No sessions found {_format_date_range_message(since_date, until_date)}\n"
            )
        else:
            agent_msg = f" for agent '{agent}'" if agent != "auto" else ""
            sys.stderr.write(f"Error: No sessions found in alias '{alias_name}'{agent_msg}\n")
        sys.exit(1)

    _print_alias_sessions(all_sessions)


def _get_alias_export_options(args) -> dict:
    """Extract export options from args."""
    return {
        "since_date": parse_date_with_validation(getattr(args, "since", None), "--since"),
        "until_date": parse_date_with_validation(getattr(args, "until", None), "--until"),
        "force": getattr(args, "force", False),
        "minimal": getattr(args, "minimal", False),
        "split_lines": getattr(args, "split", None),
        "flat": getattr(args, "flat", False),
        "quiet": getattr(args, "quiet", False),
        "agent": getattr(args, "agent", None) or "auto",
    }


def _get_ws_output_path(output_path: Path, workspace: str, flat: bool) -> Path:
    """Get output path for workspace, creating directory if needed."""
    if flat:
        return output_path
    ws_name = normalize_workspace_name(workspace).replace("/", "-").replace("\\", "-")
    if ws_name.startswith("-"):
        ws_name = ws_name[1:]
    ws_path = output_path / ws_name
    ws_path.mkdir(parents=True, exist_ok=True)
    return ws_path


def _build_output_filename(jsonl_file: Path, source_tag: str, messages: list) -> str:
    """Build output filename with optional timestamp prefix."""
    ts_prefix = None
    if messages and messages[0].get("timestamp"):
        try:
            dt = datetime.fromisoformat(messages[0]["timestamp"].replace("Z", "+00:00"))
            ts_prefix = dt.strftime("%Y%m%d%H%M%S")
        except (ValueError, AttributeError):
            pass
    return (
        f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
        if ts_prefix
        else f"{source_tag}{jsonl_file.stem}.md"
    )


def _write_export_parts(
    messages: list, jsonl_file: Path, ws_output_path: Path, output_name: str, opts: dict
) -> bool:
    """Write split parts if applicable. Returns True if parts were written."""
    if not opts["split_lines"] or not messages or len(messages) <= MIN_MESSAGES_FOR_SPLIT:
        return False

    parts = generate_markdown_parts(messages, jsonl_file, opts["minimal"], opts["split_lines"])
    if not parts:
        return False

    base_name = output_name[:-3]
    for part_num, _, part_md, _, _ in parts:
        part_file = ws_output_path / f"{base_name}_part{part_num}.md"
        part_file.write_text(part_md, encoding="utf-8")
        if not opts["quiet"]:
            print(part_file)
    return True


def _export_session_file(
    session: dict, ws_output_path: Path, source_tag: str, opts: dict, stats: dict
):
    """Export a single session file. Updates stats dict in place."""
    jsonl_file = session["file"]
    agent_type = session.get("agent", AGENT_CLAUDE)

    try:
        messages = _read_session_messages(jsonl_file, agent_type)
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error reading {jsonl_file}: {e}\n")
        stats["failed"] += 1
        return
    if messages is None:
        stats["failed"] += 1
        return

    output_name = _build_output_filename(jsonl_file, source_tag, messages)
    output_file = ws_output_path / output_name

    # Check incremental skip
    if not opts["force"] and output_file.exists():
        if output_file.stat().st_mtime >= jsonl_file.stat().st_mtime:
            stats["skipped"] += 1
            return

    try:
        _export_single_session(
            jsonl_file,
            output_file,
            output_name,
            agent_type,
            messages,
            opts["minimal"],
            opts["split_lines"],
            opts["quiet"],
        )
        stats["exported"] += 1
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error exporting {jsonl_file}: {e}\n")
        stats["failed"] += 1


def cmd_alias_export(alias_name: str, output_dir: Union[str, Path], args):
    """Export sessions from all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        exit_with_error(f"Alias '{alias_name}' not found")

    alias_config = _filter_alias_config_by_flags(aliases_data["aliases"][alias_name], args)
    opts = _get_alias_export_options(args)
    agent_filter = opts.get("agent", "auto")
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    stats = {"exported": 0, "skipped": 0, "failed": 0}

    if agent_filter in ("auto", AGENT_CLAUDE):
        for source_key, workspaces in alias_config.items():
            source_tag = "" if source_key == "local" else source_key.replace(":", "_") + "_"

            for workspace in workspaces:
                sessions = get_sessions_for_source(
                    source_key,
                    workspace,
                    since_date=opts["since_date"],
                    until_date=opts["until_date"],
                    fetch_remote=True,
                )
                if not sessions:
                    continue

                ws_output_path = _get_ws_output_path(output_path, workspace, opts["flat"])
                for session in sessions:
                    _export_session_file(session, ws_output_path, source_tag, opts, stats)

    if agent_filter in ("auto", AGENT_CODEX, AGENT_GEMINI):
        patterns = _get_alias_workspace_patterns(alias_config)
        sessions = _collect_non_claude_alias_sessions(
            patterns,
            opts["since_date"],
            opts["until_date"],
            agent_filter,
            source_keys=list(alias_config.keys()),
        )
        for session in sessions:
            source_key = session.get("source", "local")
            source_tag = "" if source_key == "local" else source_key.replace(":", "_") + "_"
            workspace_name = session.get("workspace_readable") or session.get("workspace", "")
            ws_output_path = _get_ws_output_path(output_path, workspace_name, opts["flat"])
            _export_session_file(session, ws_output_path, source_tag, opts, stats)

    if stats["exported"] > 0 or stats["skipped"] > 0:
        print(
            f"\nAlias '{alias_name}': {stats['exported']} exported, "
            f"{stats['skipped']} skipped, {stats['failed']} failed"
        )


def _load_import_data(import_file: Optional[str]) -> dict:
    """Load import data from file or stdin."""
    try:
        if import_file:
            with open(import_file, encoding="utf-8") as f:
                return json.load(f)
        return json.load(sys.stdin)
    except json.JSONDecodeError as e:
        sys.stderr.write(f"Error: Invalid JSON: {e}\n")
        sys.exit(1)
    except OSError as e:
        sys.stderr.write(f"Error reading file: {e}\n")
        sys.exit(1)


def _validate_alias_source_key(source_key: str) -> bool:
    """Validate that an alias source key is valid.

    Valid source keys:
    - "local": Local workspaces
    - "windows": Windows workspaces (when in WSL)
    - "wsl": Dict of WSL distro names -> workspaces
    - "remote": Dict of hostname -> workspaces
    """
    valid_keys = {"local", "windows", "wsl", "remote"}
    return source_key in valid_keys


def _merge_nested_sources(existing: dict, incoming: dict):
    """Merge nested source dicts (wsl, remote) without duplicates."""
    for sub_key, ws_list in incoming.items():
        if sub_key not in existing:
            existing[sub_key] = ws_list
        else:
            for ws in ws_list:
                if ws not in existing[sub_key]:
                    existing[sub_key].append(ws)


def _merge_flat_list(existing: list, incoming: list):
    """Merge flat workspace lists without duplicates."""
    for ws in incoming:
        if ws not in existing:
            existing.append(ws)


def _merge_alias_sources(existing_sources: dict, new_sources: dict):
    """Merge new sources into existing alias sources."""
    for source_key, workspaces in new_sources.items():
        if not _validate_alias_source_key(source_key):
            sys.stderr.write(f"Warning: Ignoring invalid source key '{source_key}'\n")
            continue

        if source_key not in existing_sources:
            existing_sources[source_key] = workspaces
        elif isinstance(workspaces, dict) and isinstance(existing_sources[source_key], dict):
            _merge_nested_sources(existing_sources[source_key], workspaces)
        elif isinstance(workspaces, list):
            _merge_flat_list(existing_sources[source_key], workspaces)


def _backup_aliases_file() -> Path | None:
    """Create a backup of the aliases file. Returns backup path or None if no file to backup."""
    aliases_file = get_aliases_file()
    if not aliases_file.exists():
        return None

    backup_path = aliases_file.with_suffix(
        f".backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    try:
        import shutil

        shutil.copy2(aliases_file, backup_path)
        return backup_path
    except OSError as e:
        sys.stderr.write(f"Warning: Could not create backup: {e}\n")
        return None


def cmd_alias_config_import(args):
    """Import aliases from JSON file or stdin."""
    import_file = getattr(args, "file", None)
    replace_mode = getattr(args, "replace", False)

    import_data = _load_import_data(import_file)

    if "aliases" not in import_data:
        sys.stderr.write("Error: Invalid aliases format (missing 'aliases' key)\n")
        sys.exit(1)

    if replace_mode:
        # Create backup before replacing
        backup_path = _backup_aliases_file()
        if backup_path:
            sys.stderr.write(f"Created backup: {backup_path}\n")
        new_data = import_data
    else:
        existing_data = load_aliases()
        for alias_name, sources in import_data["aliases"].items():
            if alias_name not in existing_data["aliases"]:
                existing_data["aliases"][alias_name] = sources
            else:
                _merge_alias_sources(existing_data["aliases"][alias_name], sources)
        new_data = existing_data

    if save_aliases(new_data):
        alias_count = len(new_data.get("aliases", {}))
        mode_str = "Replaced" if replace_mode else "Merged"
        print(f"{mode_str} aliases ({alias_count} aliases)")
    else:
        sys.exit(1)


# ============================================================================
# Metrics Database (SQLite)
# ============================================================================

METRICS_DB_VERSION = 5


def get_metrics_db_path() -> Path:
    """Get the metrics database file path."""
    return get_aliases_dir() / "metrics.db"


def init_metrics_db(db_path: Optional[Path] = None) -> sqlite3.Connection:
    """Initialize the metrics database, creating tables if needed.

    Opens (or creates) the SQLite metrics database and ensures the schema
    is up to date. Handles migrations from older schema versions.

    Args:
        db_path: Path to database file. Defaults to ~/.agent-history/metrics.db

    Returns:
        Open sqlite3.Connection with row_factory set to sqlite3.Row

    Side Effects:
        - Creates parent directory (~/.agent-history/) with mode 0o700 if missing
        - Creates database file with mode 0o600 if missing
        - Runs schema migrations if database version is outdated
    """
    if db_path is None:
        db_path = get_metrics_db_path()

    # Ensure directory exists with secure permissions
    db_path.parent.mkdir(parents=True, exist_ok=True)
    os.chmod(db_path.parent, 0o700)

    # Track if this is a new database
    is_new_db = not db_path.exists()

    conn = sqlite3.connect(str(db_path), timeout=30.0)

    # Enable foreign key enforcement (disabled by default in SQLite)
    conn.execute("PRAGMA foreign_keys = ON")

    # Set secure permissions on new database file
    if is_new_db:
        os.chmod(db_path, 0o600)
    conn.row_factory = sqlite3.Row  # Enable column access by name

    # Create tables
    # Note: file_path is the primary key since multiple files can share the same session_id
    # (e.g., main session + agent files spawned from it)
    conn.executescript("""
        -- Schema version tracking
        CREATE TABLE IF NOT EXISTS schema_version (
            version INTEGER PRIMARY KEY
        );

        -- Sessions table (one row per JSONL file)
        CREATE TABLE IF NOT EXISTS sessions (
            file_path TEXT PRIMARY KEY,
            session_id TEXT,
            workspace TEXT NOT NULL,
            source TEXT NOT NULL DEFAULT 'local',
            agent TEXT NOT NULL DEFAULT 'claude',
            file_mtime REAL,
            is_agent INTEGER DEFAULT 0,
            parent_session_id TEXT,
            start_time TEXT,
            end_time TEXT,
            message_count INTEGER DEFAULT 0,
            git_branch TEXT,
            claude_version TEXT,
            cwd TEXT,
            work_period_seconds REAL DEFAULT 0,
            num_work_periods INTEGER DEFAULT 1
        );

        -- Messages table (aggregated stats per message)
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            parent_uuid TEXT,
            type TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            model TEXT,
            stop_reason TEXT,
            input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0,
            cache_creation_tokens INTEGER DEFAULT 0,
            cache_read_tokens INTEGER DEFAULT 0,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Tool uses table
        CREATE TABLE IF NOT EXISTS tool_uses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_use_id TEXT,
            message_uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            tool_name TEXT NOT NULL,
            is_error INTEGER DEFAULT 0,
            timestamp TEXT,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Synced files tracking (for incremental sync)
        CREATE TABLE IF NOT EXISTS synced_files (
            file_path TEXT PRIMARY KEY,
            mtime REAL NOT NULL,
            synced_at TEXT NOT NULL
        );

        -- Create indexes for common queries
        -- Note: idx_sessions_agent is created in migration to v4 (after agent column exists)
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace ON sessions(workspace);
        CREATE INDEX IF NOT EXISTS idx_sessions_source ON sessions(source);
        CREATE INDEX IF NOT EXISTS idx_sessions_start_time ON sessions(start_time);
        CREATE INDEX IF NOT EXISTS idx_sessions_session_id ON sessions(session_id);
        CREATE INDEX IF NOT EXISTS idx_messages_file_path ON messages(file_path);
        CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);
        CREATE INDEX IF NOT EXISTS idx_messages_model ON messages(model);
        CREATE INDEX IF NOT EXISTS idx_messages_session ON messages(session_id);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_file_path ON tool_uses(file_path);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_tool_name ON tool_uses(tool_name);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_session ON tool_uses(session_id);

        -- Composite indexes for common filter combinations
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace_source ON sessions(workspace, source);
        CREATE INDEX IF NOT EXISTS idx_sessions_source_time ON sessions(source, start_time);
    """)

    # Check/set schema version and handle migrations
    cursor = conn.execute("SELECT version FROM schema_version LIMIT 1")
    row = cursor.fetchone()
    current_version = row["version"] if row else 0

    if current_version < METRICS_DB_VERSION:
        _run_metrics_db_migrations(conn, current_version, row)

    conn.commit()
    return conn


def _run_metrics_db_migrations(conn: sqlite3.Connection, current_version: int, version_row) -> None:
    """Run database schema migrations."""
    # Migrate from version 2 to 3: add time tracking columns
    if current_version < DB_VERSION_TIME_TRACKING:
        try:
            conn.execute("ALTER TABLE sessions ADD COLUMN work_period_seconds REAL DEFAULT 0")
            conn.execute("ALTER TABLE sessions ADD COLUMN num_work_periods INTEGER DEFAULT 1")
        except sqlite3.OperationalError:
            pass  # Columns already exist

    # Migrate from version 3 to 4: add agent column with path-based detection
    # Session files are stored in agent-specific directories:
    #   Claude: ~/.claude/projects/.../*.jsonl
    #   Codex:  ~/.codex/sessions/*.jsonl
    #   Gemini: ~/.gemini/tmp/*/chats/session-*.json
    if current_version < DB_VERSION_AGENT_COLUMN:
        # Add agent column (may fail if already exists for new DBs)
        try:
            conn.execute("ALTER TABLE sessions ADD COLUMN agent TEXT DEFAULT 'claude'")
        except sqlite3.OperationalError as e:
            if "duplicate column" not in str(e).lower():
                sys.stderr.write(f"Warning: Migration to v4 failed (add column): {e}\n")

        # Create index (runs for both migrated and new databases)
        conn.execute("CREATE INDEX IF NOT EXISTS idx_sessions_agent ON sessions(agent)")

        # Set correct agent values for existing sessions based on file paths
        # (no-op for new databases with no sessions)
        conn.execute("UPDATE sessions SET agent = 'gemini' WHERE file_path LIKE '%/.gemini/%'")
        conn.execute("UPDATE sessions SET agent = 'codex' WHERE file_path LIKE '%/.codex/%'")

    # Migrate from version 4 to 5: fix Gemini sessions missing timestamps
    # Delete synced_files entries for Gemini sessions with NULL start_time to force resync
    if current_version < DB_VERSION_GEMINI_TIMESTAMPS:
        cursor = conn.execute(
            """DELETE FROM synced_files WHERE file_path IN (
                SELECT file_path FROM sessions WHERE agent = 'gemini' AND start_time IS NULL
            )"""
        )
        if cursor.rowcount > 0:
            sys.stderr.write(
                f"Migration: {cursor.rowcount} Gemini sessions will be resynced for timestamps\n"
            )

    # Update version
    if version_row is None:
        conn.execute("INSERT INTO schema_version (version) VALUES (?)", (METRICS_DB_VERSION,))
    else:
        conn.execute("UPDATE schema_version SET version = ?", (METRICS_DB_VERSION,))


def calculate_work_periods(
    timestamps: list, gap_threshold: float = WORK_PERIOD_GAP_THRESHOLD
) -> tuple:
    """Calculate work period time from a list of timestamps.

    Args:
        timestamps: List of ISO 8601 timestamp strings
        gap_threshold: Gap in seconds that marks end of a work period

    Returns:
        (work_period_seconds, num_work_periods, start_time, end_time)

    Note: Timestamps are sorted first since JSONL files may have
    out-of-order timestamps due to context restoration.
    """
    if not timestamps:
        return 0.0, 0, None, None

    if len(timestamps) == 1:
        return 0.0, 1, timestamps[0], timestamps[0]

    # Sort timestamps chronologically
    sorted_ts = sorted(timestamps)

    # Parse timestamps
    def parse_ts(ts):
        return datetime.fromisoformat(ts.replace("Z", "+00:00"))

    total_duration = 0.0
    num_periods = 1
    period_start = parse_ts(sorted_ts[0])
    prev_ts = period_start

    for ts_str in sorted_ts[1:]:
        ts = parse_ts(ts_str)
        gap = (ts - prev_ts).total_seconds()

        if gap > gap_threshold:
            # End current period, start new one
            total_duration += (prev_ts - period_start).total_seconds()
            period_start = ts
            num_periods += 1

        prev_ts = ts

    # Add final period
    total_duration += (prev_ts - period_start).total_seconds()

    return total_duration, num_periods, sorted_ts[0], sorted_ts[-1]


def _init_session_info(jsonl_file: Path, source: str) -> dict:
    """Initialize session info dictionary."""
    return {
        "session_id": None,
        "workspace": None,
        "source": source,
        "file_path": str(jsonl_file),
        "file_mtime": jsonl_file.stat().st_mtime if jsonl_file.exists() else None,
        "is_agent": False,
        "parent_session_id": None,
        "start_time": None,
        "end_time": None,
        "message_count": 0,
        "git_branch": None,
        "claude_version": None,
        "cwd": None,
        "work_period_seconds": 0.0,
        "num_work_periods": 0,
    }


def _update_session_from_entry(session_info: dict, entry: dict):
    """Update session info from first message entry."""
    if session_info["session_id"] is not None:
        return
    session_info["session_id"] = entry.get("sessionId")
    session_info["git_branch"] = entry.get("gitBranch")
    session_info["claude_version"] = entry.get("version")
    session_info["cwd"] = entry.get("cwd")
    session_info["is_agent"] = entry.get("isSidechain", False)
    if session_info["is_agent"]:
        session_info["parent_session_id"] = entry.get("sessionId")


def _extract_message_record(entry: dict, entry_type: str, timestamp: str) -> dict:
    """Extract message record from entry."""
    message_obj = entry.get("message", {})
    usage = message_obj.get("usage", {})
    return {
        "uuid": entry.get("uuid"),
        "session_id": entry.get("sessionId"),
        "parent_uuid": entry.get("parentUuid"),
        "type": entry_type,
        "timestamp": timestamp,
        "model": message_obj.get("model"),
        "stop_reason": message_obj.get("stop_reason"),
        "input_tokens": usage.get("input_tokens", 0),
        "output_tokens": usage.get("output_tokens", 0),
        "cache_creation_tokens": usage.get("cache_creation_input_tokens", 0),
        "cache_read_tokens": usage.get("cache_read_input_tokens", 0),
    }


def _extract_tool_uses_from_content(entry: dict, timestamp: str) -> list:
    """Extract tool uses from assistant message content."""
    message_obj = entry.get("message", {})
    content = message_obj.get("content", [])
    if not isinstance(content, list):
        return []
    tool_uses = []
    for block in content:
        if isinstance(block, dict) and block.get("type") == "tool_use":
            tool_uses.append(
                {
                    "tool_use_id": block.get("id"),
                    "message_uuid": entry.get("uuid"),
                    "session_id": entry.get("sessionId"),
                    "tool_name": block.get("name", "unknown"),
                    "is_error": False,
                    "timestamp": timestamp,
                }
            )
    return tool_uses


def _update_tool_errors_from_results(entry: dict, tool_uses: list):
    """Update tool use errors from tool result blocks."""
    message_obj = entry.get("message", {})
    content = message_obj.get("content", [])
    if not isinstance(content, list):
        return
    for block in content:
        if isinstance(block, dict) and block.get("type") == "tool_result":
            tool_use_id = block.get("tool_use_id")
            is_error = block.get("is_error", False)
            for tu in tool_uses:
                if tu["tool_use_id"] == tool_use_id:
                    tu["is_error"] = is_error
                    break


def extract_metrics_from_jsonl(jsonl_file: Path, source: str = "local") -> dict:  # noqa: C901
    """Extract metrics data from a JSONL file.

    Returns a dict with session, messages, and tool_uses.
    """
    session_info = _init_session_info(jsonl_file, source)
    messages = []
    tool_uses = []
    all_timestamps = []

    def handle_entry(entry: dict) -> None:
        entry_type = entry.get("type")
        if entry_type not in ("user", "assistant"):
            return

        timestamp = entry.get("timestamp", "")
        _update_session_from_entry(session_info, entry)

        if timestamp:
            all_timestamps.append(timestamp)
        session_info["message_count"] += 1

        messages.append(_extract_message_record(entry, entry_type, timestamp))

        if entry_type == "assistant":
            tool_uses.extend(_extract_tool_uses_from_content(entry, timestamp))
        elif entry_type == "user":
            _update_tool_errors_from_results(entry, tool_uses)

    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for raw_line in f:
                line = raw_line.strip()
                if not line:
                    continue

                try:
                    handle_entry(json.loads(line))
                except json.JSONDecodeError as e:
                    parsed = _iter_json_objects(line)
                    if not parsed:
                        if line.lstrip().startswith(("{", "[")):
                            sys.stderr.write(
                                f"Warning: Couldn't parse line in {jsonl_file.name}: {e}\n"
                            )
                        continue
                    for entry in parsed:
                        if isinstance(entry, dict):
                            handle_entry(entry)
    except OSError as e:
        sys.stderr.write(f"Warning: Error reading {jsonl_file}: {e}\n")

    work_secs, num_periods, start_time, end_time = calculate_work_periods(all_timestamps)
    session_info["work_period_seconds"] = work_secs
    session_info["num_work_periods"] = num_periods
    session_info["start_time"] = start_time
    session_info["end_time"] = end_time
    session_info["workspace"] = get_workspace_name_from_path(jsonl_file.parent.name)

    return {"session": session_info, "messages": messages, "tool_uses": tool_uses}


def _is_file_synced(conn: sqlite3.Connection, file_path: str, current_mtime: float) -> bool:
    """Check if file is already synced and up to date."""
    cursor = conn.execute("SELECT mtime FROM synced_files WHERE file_path = ?", (file_path,))
    row = cursor.fetchone()
    return row is not None and row["mtime"] >= current_mtime


def _prepare_gemini_session(
    session: dict[str, Any], metrics: MetricsDict, source: str, mtime: float
) -> None:
    """Prepare Gemini session dict with required fields (modifies session in place)."""
    # Gemini stores projectHash in 'cwd' field
    project_hash = session.get("cwd", "")
    if project_hash:
        real_path = gemini_get_path_for_hash(project_hash)
        if real_path:
            encoded = path_to_encoded_workspace(real_path)
            session["workspace"] = get_workspace_name_from_path(encoded)
        else:
            session["workspace"] = project_hash
    else:
        session["workspace"] = "gemini-unknown"

    session["source"] = source
    session["file_mtime"] = mtime
    session["is_agent"] = False
    session["parent_session_id"] = None
    session["message_count"] = len(metrics["messages"])
    session["git_branch"] = None
    session["claude_version"] = None
    session["work_period_seconds"] = 0
    session["num_work_periods"] = 1
    session["start_time"] = session.get("startTime")
    session["end_time"] = session.get("lastUpdated")


def _prepare_codex_session(
    session: dict[str, Any], metrics: MetricsDict, source: str, mtime: float
) -> None:
    """Prepare Codex session dict with required fields (modifies session in place)."""
    if session.get("cwd"):
        # Keep stats DB workspace short and stable across machines.
        encoded = path_to_encoded_workspace(session["cwd"])
        session["workspace"] = get_workspace_name_from_path(encoded)
    else:
        session["workspace"] = "unknown"

    session["source"] = source
    session["file_mtime"] = mtime
    session["is_agent"] = False
    session["parent_session_id"] = None
    session["start_time"] = None
    session["end_time"] = None
    session["message_count"] = len(metrics["messages"])
    session["git_branch"] = None
    session["claude_version"] = session.get("cli_version")
    session["work_period_seconds"] = 0
    session["num_work_periods"] = 1

    # Get timestamps from messages
    if metrics["messages"]:
        timestamps = [m.get("timestamp") for m in metrics["messages"] if m.get("timestamp")]
        if timestamps:
            session["start_time"] = min(timestamps)
            session["end_time"] = max(timestamps)


def _write_metrics_to_db(
    conn: sqlite3.Connection,
    file_path: str,
    session: dict[str, Any],
    metrics: dict[str, Any],
    agent: str,
    mtime: float,
) -> None:
    """Write session, messages, and tool uses to database."""
    # Delete existing data for this file (for re-sync)
    conn.execute("DELETE FROM tool_uses WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM messages WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM sessions WHERE file_path = ?", (file_path,))

    # Insert session
    conn.execute(
        """
        INSERT INTO sessions (
            file_path, session_id, workspace, source, agent, file_mtime,
            is_agent, parent_session_id, start_time, end_time,
            message_count, git_branch, claude_version, cwd,
            work_period_seconds, num_work_periods
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """,
        (
            file_path,
            session.get("session_id") or session.get("id"),
            session["workspace"],
            session["source"],
            agent,
            session["file_mtime"],
            1 if session.get("is_agent") else 0,
            session.get("parent_session_id"),
            session.get("start_time"),
            session.get("end_time"),
            session.get("message_count", len(metrics.get("messages", []))),
            session.get("git_branch"),
            session.get("claude_version"),
            session.get("cwd"),
            session.get("work_period_seconds", 0),
            session.get("num_work_periods", 1),
        ),
    )

    # Insert messages
    session_id = session.get("session_id") or session.get("id")
    for msg in metrics["messages"]:
        conn.execute(
            """
            INSERT INTO messages (
                uuid, file_path, session_id, parent_uuid, type, timestamp,
                model, stop_reason, input_tokens, output_tokens,
                cache_creation_tokens, cache_read_tokens
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                msg.get("uuid"),
                file_path,
                msg.get("session_id") or session_id,
                msg.get("parent_uuid"),
                msg.get("type") or msg.get("role"),
                msg.get("timestamp"),
                msg.get("model"),
                msg.get("stop_reason"),
                msg.get("input_tokens", 0),
                msg.get("output_tokens", 0),
                msg.get("cache_creation_tokens", 0),
                msg.get("cache_read_tokens", 0),
            ),
        )

    tokens_summary = metrics.get("tokens_summary")
    if agent == AGENT_CODEX and tokens_summary:
        conn.execute(
            """
            INSERT INTO messages (
                uuid, file_path, session_id, parent_uuid, type, timestamp,
                model, stop_reason, input_tokens, output_tokens,
                cache_creation_tokens, cache_read_tokens
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                None,
                file_path,
                session_id,
                None,
                "token_summary",
                tokens_summary.get("timestamp") or session.get("end_time"),
                None,
                None,
                tokens_summary.get("input_tokens", 0),
                tokens_summary.get("output_tokens", 0),
                0,
                tokens_summary.get("cache_read_tokens", 0),
            ),
        )

    # Insert tool uses
    for tu in metrics["tool_uses"]:
        conn.execute(
            """
            INSERT INTO tool_uses (
                tool_use_id, message_uuid, file_path, session_id, tool_name, is_error, timestamp
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                tu.get("tool_use_id"),
                tu.get("message_uuid"),
                file_path,
                tu.get("session_id") or session_id,
                tu.get("tool_name") or tu.get("name"),
                1 if tu.get("is_error") else 0,
                tu.get("timestamp"),
            ),
        )

    # Update synced files tracking
    conn.execute(
        """
        INSERT OR REPLACE INTO synced_files (file_path, mtime, synced_at)
        VALUES (?, ?, ?)
    """,
        (file_path, mtime, datetime.now().isoformat()),
    )


def sync_file_to_db(
    conn: sqlite3.Connection, jsonl_file: Path, source: str = "local", force: bool = False
) -> bool:
    """Sync a single JSONL file to the database.

    Returns True if file was synced, False if skipped (already up to date).
    """
    file_path = str(jsonl_file)
    current_mtime = jsonl_file.stat().st_mtime if jsonl_file.exists() else 0

    # Check if already synced and up to date
    if not force and _is_file_synced(conn, file_path, current_mtime):
        return False

    # Detect agent type and extract metrics
    agent = detect_agent_from_path(jsonl_file)

    if agent == AGENT_GEMINI:
        metrics = gemini_extract_metrics_from_json(jsonl_file)
        session = cast(dict[str, Any], metrics["session"])
        _prepare_gemini_session(session, metrics, source, current_mtime)
    elif agent == AGENT_CODEX:
        metrics = codex_extract_metrics_from_jsonl(jsonl_file)
        session = cast(dict[str, Any], metrics["session"])
        _prepare_codex_session(session, metrics, source, current_mtime)
    else:
        metrics = extract_metrics_from_jsonl(jsonl_file, source)
        session = metrics["session"]

    # Skip empty files
    if session.get("message_count", 0) == 0 and len(metrics.get("messages", [])) == 0:
        return False

    # Write to database with transaction
    try:
        metrics_dict = cast(dict[str, Any], metrics)
        _write_metrics_to_db(conn, file_path, session, metrics_dict, agent, current_mtime)
        conn.commit()
        return True
    except sqlite3.Error as e:
        conn.rollback()
        sys.stderr.write(f"Warning: Database error syncing {jsonl_file}: {e}\n")
        return False


def _matches_pattern(name: str, patterns: list) -> bool:
    """Check if name matches any of the patterns."""
    if not patterns or not patterns[0]:
        return True
    return any(p in name for p in patterns)


def _sync_source_to_db(
    conn,
    projects_dir: Path,
    source: str,
    source_label: str,
    patterns: list,
    force: bool,
    show_progress: bool = False,
) -> dict:
    """Sync a single source to the metrics database. Returns stats dict."""
    stats = {"synced": 0, "skipped": 0, "errors": 0}

    if not projects_dir or not projects_dir.exists():
        return stats

    print(f"Scanning {source_label}...")
    workspaces = []
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir() or is_cached_workspace(workspace_dir.name):
            continue
        if not _matches_pattern(workspace_dir.name, patterns):
            continue
        workspaces.append(workspace_dir)

    if show_progress and workspaces:
        total = len(workspaces)
        for idx, workspace_dir in enumerate(workspaces, 1):
            print(f"  [{idx}/{total}] {workspace_dir.name}")
            _sync_workspace_files_to_db(conn, workspace_dir, source, stats, force)
    else:
        for workspace_dir in workspaces:
            _sync_workspace_files_to_db(conn, workspace_dir, source, stats, force)

    return stats


def _sync_workspace_files_to_db(conn, local_dir: Path, source_key: str, stats: dict, force: bool):
    """Sync all JSONL files in a workspace directory to database."""
    if not local_dir.exists():
        return
    for jsonl_file in local_dir.glob("*.jsonl"):
        try:
            if sync_file_to_db(conn, jsonl_file, source_key, force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {jsonl_file.name}: {e}\n")
            stats["errors"] += 1


def _sync_codex_to_db(
    conn,
    patterns: list,
    force: bool,
    sessions_dir: Optional[Path] = None,
    source_key: str = "codex",
    source_label: Optional[str] = None,
) -> dict:
    """Sync Codex sessions from ~/.codex/sessions/ to database.

    Codex stores sessions in ~/.codex/sessions/YYYY/MM/DD/rollout-*.jsonl
    """
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    sessions_dir = sessions_dir or codex_get_home_dir()

    if not sessions_dir.exists():
        return stats

    label = source_label or "Codex sessions"
    print(f"Scanning {label}...")

    # Walk through YYYY/MM/DD structure
    for jsonl_file in sessions_dir.glob("*/*/*/rollout-*.jsonl"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = codex_get_workspace_from_session(jsonl_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, jsonl_file, source_key, force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {jsonl_file.name}: {e}\n")
            stats["errors"] += 1

    return stats


def _sync_gemini_to_db(
    conn,
    patterns: list,
    force: bool,
    sessions_dir: Optional[Path] = None,
    source_key: str = "gemini",
    source_label: Optional[str] = None,
) -> dict:
    """Sync Gemini sessions from ~/.gemini/tmp/*/chats/ to database.

    Gemini stores sessions in ~/.gemini/tmp/<project_hash>/chats/session-*.json
    """
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    sessions_dir = sessions_dir or gemini_get_home_dir()

    if not sessions_dir.exists():
        return stats

    label = source_label or "Gemini sessions"
    print(f"Scanning {label}...")

    # Walk through <hash>/chats/ structure
    for json_file in sessions_dir.glob("*/chats/session-*.json"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = gemini_get_workspace_from_session(json_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, json_file, source_key, force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"  Error syncing {json_file.name}: {e}\n")
            stats["errors"] += 1

    return stats


def _filter_workspaces_by_pattern(workspaces: list, patterns: list) -> list:
    """Filter workspaces by patterns (substring match)."""
    if not patterns or not patterns[0]:
        return workspaces
    return [ws for ws in workspaces if any(p in ws for p in patterns)]


def _sync_remote_workspace(
    conn, remote: str, ws: str, local_projects_dir: Path, hostname: str, stats: dict, force: bool
):
    """Sync a single remote workspace to database."""
    fetch_result = fetch_workspace_files(remote, ws, local_projects_dir, hostname)
    if not fetch_result.get("success"):
        return
    if fetch_result.get("warning"):
        sys.stderr.write(f"Warning: {fetch_result['warning']}\n")
    workspace_path = ws.lstrip("-")
    local_dir = local_projects_dir / f"remote_{hostname}_{workspace_path}"
    _sync_workspace_files_to_db(conn, local_dir, f"remote:{hostname}", stats, force)


def _sync_ssh_remote_claude_to_db(
    conn, remote: str, hostname: str, patterns: list, stats: dict, force: bool
):
    """Sync Claude sessions from SSH remote to database."""
    remote_workspaces = list_remote_workspaces(remote)
    if not remote_workspaces:
        return

    remote_workspaces = _filter_workspaces_by_pattern(remote_workspaces, patterns)
    if not remote_workspaces:
        return

    try:
        local_projects_dir = get_claude_projects_dir()
    except SystemExit:
        # Claude projects dir doesn't exist locally, create temp cache
        local_projects_dir = get_config_dir() / "remote_cache"
        local_projects_dir.mkdir(parents=True, exist_ok=True)

    print(f"  Syncing Claude sessions ({len(remote_workspaces)} workspaces)...")

    for ws in remote_workspaces:
        try:
            _sync_remote_workspace(conn, remote, ws, local_projects_dir, hostname, stats, force)
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"    Error fetching {ws}: {e}\n")
            stats["errors"] += 1


def _sync_ssh_remote_gemini_to_db(
    conn, remote: str, hostname: str, patterns: list, stats: dict, force: bool
):
    """Sync Gemini sessions from SSH remote to database."""
    local_cache_dir = get_config_dir()

    # Fetch Gemini sessions from remote
    result = gemini_fetch_remote_sessions(remote, local_cache_dir, hostname)
    if not result.get("success"):
        if result.get("error") and "No such file" not in result.get("error", ""):
            sys.stderr.write(f"    Gemini fetch failed: {result.get('error')}\n")
        return
    if result.get("warning"):
        sys.stderr.write(f"Warning: gemini: {result['warning']}\n")

    # Scan fetched sessions
    local_gemini_dir = local_cache_dir / f"remote_{hostname}_gemini"
    if not local_gemini_dir.exists():
        return

    print("  Syncing Gemini sessions...")

    for json_file in local_gemini_dir.glob("*/chats/session-*.json"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = gemini_get_workspace_from_session(json_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, json_file, f"remote:{hostname}", force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"    Error syncing {json_file.name}: {e}\n")
            stats["errors"] += 1


def _sync_ssh_remote_codex_to_db(
    conn, remote: str, hostname: str, patterns: list, stats: dict, force: bool
):
    """Sync Codex sessions from SSH remote to database."""
    local_cache_dir = get_config_dir()

    # Fetch Codex sessions from remote
    result = codex_fetch_remote_sessions(remote, local_cache_dir, hostname)
    if not result.get("success"):
        if result.get("error") and "No such file" not in result.get("error", ""):
            sys.stderr.write(f"    Codex fetch failed: {result.get('error')}\n")
        return
    if result.get("warning"):
        sys.stderr.write(f"Warning: codex: {result['warning']}\n")

    # Scan fetched sessions
    local_codex_dir = local_cache_dir / f"remote_{hostname}_codex"
    if not local_codex_dir.exists():
        return

    print("  Syncing Codex sessions...")

    for jsonl_file in local_codex_dir.glob("*/*/*/*.jsonl"):
        # Filter by workspace pattern if specified
        if patterns and patterns[0]:
            workspace = codex_get_workspace_from_session(jsonl_file)
            if not _matches_pattern(workspace, patterns):
                continue

        try:
            if sync_file_to_db(conn, jsonl_file, f"remote:{hostname}", force):
                stats["synced"] += 1
            else:
                stats["skipped"] += 1
        except (OSError, json.JSONDecodeError, sqlite3.Error) as e:
            sys.stderr.write(f"    Error syncing {jsonl_file.name}: {e}\n")
            stats["errors"] += 1


def _sync_ssh_remote_to_db(conn, remote: str, patterns: list, force: bool) -> dict:
    """Sync SSH remote to the metrics database. Returns stats dict."""
    stats = {"synced": 0, "skipped": 0, "errors": 0}
    hostname = get_remote_hostname(remote)
    print(f"Fetching from remote {remote}...")

    if not check_ssh_connection(remote):
        sys.stderr.write(f"  Cannot connect to {remote}\n")
        stats["errors"] += 1
        return stats

    # Sync Claude sessions
    _sync_ssh_remote_claude_to_db(conn, remote, hostname, patterns, stats, force)

    # Sync Gemini sessions
    _sync_ssh_remote_gemini_to_db(conn, remote, hostname, patterns, stats, force)

    # Sync Codex sessions
    _sync_ssh_remote_codex_to_db(conn, remote, hostname, patterns, stats, force)

    return stats


def _accumulate_stats(totals: dict, new_stats: dict):
    """Accumulate sync stats into totals."""
    totals["synced"] += new_stats["synced"]
    totals["skipped"] += new_stats["skipped"]
    totals["errors"] += new_stats["errors"]


def _sync_wsl_windows_to_db(  # noqa: C901
    conn,
    totals: dict,
    patterns: list,
    force: bool,
    include_wsl: bool = True,
    include_windows: bool = True,
    show_progress: bool = False,
):
    """Sync WSL and Windows sources to database."""
    if include_wsl:
        for distro in get_wsl_distributions():
            if distro.get("has_claude"):
                wsl_projects = get_wsl_projects_dir(distro["name"])
                if wsl_projects:
                    stats = _sync_source_to_db(
                        conn,
                        wsl_projects,
                        f"wsl:{distro['name']}",
                        f"WSL ({distro['name']})",
                        patterns,
                        force,
                        show_progress=show_progress,
                    )
                    _accumulate_stats(totals, stats)
            if distro.get("has_codex"):
                codex_sessions = get_agent_wsl_dir(distro["name"], AGENT_CODEX)
                if codex_sessions:
                    stats = _sync_codex_to_db(
                        conn,
                        patterns,
                        force,
                        sessions_dir=codex_sessions,
                        source_key=f"wsl:{distro['name']}",
                        source_label=f"WSL ({distro['name']}) Codex sessions",
                    )
                    _accumulate_stats(totals, stats)
            if distro.get("has_gemini"):
                gemini_sessions = get_agent_wsl_dir(distro["name"], AGENT_GEMINI)
                if gemini_sessions:
                    stats = _sync_gemini_to_db(
                        conn,
                        patterns,
                        force,
                        sessions_dir=gemini_sessions,
                        source_key=f"wsl:{distro['name']}",
                        source_label=f"WSL ({distro['name']}) Gemini sessions",
                    )
                    _accumulate_stats(totals, stats)

    if include_windows:
        try:
            windows_users = get_windows_users_with_claude()
            for user in windows_users:
                win_projects = get_windows_projects_dir(user["username"])
                if win_projects:
                    stats = _sync_source_to_db(
                        conn,
                        win_projects,
                        f"windows:{user['username']}",
                        f"Windows ({user['username']})",
                        patterns,
                        force,
                        show_progress,
                    )
                    _accumulate_stats(totals, stats)
                codex_sessions = get_agent_windows_dir(user["username"], AGENT_CODEX)
                if codex_sessions:
                    stats = _sync_codex_to_db(
                        conn,
                        patterns,
                        force,
                        sessions_dir=codex_sessions,
                        source_key=f"windows:{user['username']}",
                        source_label=f"Windows ({user['username']}) Codex sessions",
                    )
                    _accumulate_stats(totals, stats)
                gemini_sessions = get_agent_windows_dir(user["username"], AGENT_GEMINI)
                if gemini_sessions:
                    stats = _sync_gemini_to_db(
                        conn,
                        patterns,
                        force,
                        sessions_dir=gemini_sessions,
                        source_key=f"windows:{user['username']}",
                        source_label=f"Windows ({user['username']}) Gemini sessions",
                    )
                    _accumulate_stats(totals, stats)
        except (OSError, PermissionError):
            pass


def _sync_remote_to_db(conn, remote: str, totals: dict, patterns: list, force: bool):
    """Sync a single remote (WSL, Windows, or SSH) to database."""
    if remote.startswith("wsl://"):
        distro = remote[6:]
        wsl_projects = get_wsl_projects_dir(distro)
        if wsl_projects:
            stats = _sync_source_to_db(
                conn, wsl_projects, f"wsl:{distro}", f"WSL ({distro})", patterns, force
            )
            _accumulate_stats(totals, stats)
        codex_sessions = get_agent_wsl_dir(distro, AGENT_CODEX)
        if codex_sessions:
            stats = _sync_codex_to_db(
                conn,
                patterns,
                force,
                sessions_dir=codex_sessions,
                source_key=f"wsl:{distro}",
                source_label=f"WSL ({distro}) Codex sessions",
            )
            _accumulate_stats(totals, stats)
        gemini_sessions = get_agent_wsl_dir(distro, AGENT_GEMINI)
        if gemini_sessions:
            stats = _sync_gemini_to_db(
                conn,
                patterns,
                force,
                sessions_dir=gemini_sessions,
                source_key=f"wsl:{distro}",
                source_label=f"WSL ({distro}) Gemini sessions",
            )
            _accumulate_stats(totals, stats)
    elif remote == "windows" or remote.startswith("windows:"):
        # Parse optional username from windows:<user> format
        win_user = None
        if remote.startswith("windows:"):
            win_user = remote[8:]  # Extract user after "windows:"
        win_projects = get_windows_projects_dir(win_user)
        if win_projects:
            source_tag = f"windows:{win_user}" if win_user else "windows"
            display_name = f"Windows ({win_user})" if win_user else "Windows"
            stats = _sync_source_to_db(
                conn, win_projects, source_tag, display_name, patterns, force
            )
            _accumulate_stats(totals, stats)
    else:
        stats = _sync_ssh_remote_to_db(conn, remote, patterns, force)
        _accumulate_stats(totals, stats)


def _sync_remote_to_db_with_new_conn(remote: str, patterns: list, force: bool) -> dict:
    """Sync a single remote using its own database connection."""
    conn = init_metrics_db()
    totals = {"synced": 0, "skipped": 0, "errors": 0}
    try:
        _sync_remote_to_db(conn, remote, totals, patterns, force)
    finally:
        conn.close()
    return totals


def cmd_stats_sync(args):  # noqa: C901
    """Sync JSONL files to metrics database."""
    force = getattr(args, "force", False)
    all_homes = getattr(args, "all_homes", False)
    remotes = getattr(args, "remotes", None) or []
    patterns = getattr(args, "patterns", [""])
    jobs = max(1, int(getattr(args, "jobs", 1) or 1))
    no_remote = getattr(args, "no_remote", False)
    no_wsl = getattr(args, "no_wsl", False)
    no_windows = getattr(args, "no_windows", False)
    show_progress = getattr(args, "show_progress", False)

    # Include saved sources when --ah is used
    if all_homes:
        if no_remote:
            if remotes:
                sys.stderr.write("Warning: --no-remote set; ignoring explicit remotes\n")
            remotes = []
        else:
            for src in get_saved_sources():
                if src not in remotes:
                    remotes.append(src)
    elif no_remote and remotes:
        sys.stderr.write("Warning: --no-remote set; ignoring explicit remotes\n")
        remotes = []

    conn = init_metrics_db()
    totals = {"synced": 0, "skipped": 0, "errors": 0}

    # Sync local Claude sessions
    stats = _sync_source_to_db(
        conn, get_claude_projects_dir(), "local", "local", patterns, force, show_progress
    )
    _accumulate_stats(totals, stats)

    # Sync local Codex sessions (~/.codex/sessions/)
    stats = _sync_codex_to_db(conn, patterns, force)
    _accumulate_stats(totals, stats)

    # Sync local Gemini sessions (~/.gemini/tmp/)
    stats = _sync_gemini_to_db(conn, patterns, force)
    _accumulate_stats(totals, stats)

    # Sync WSL/Windows if --ah flag
    if all_homes:
        _sync_wsl_windows_to_db(
            conn,
            totals,
            patterns,
            force,
            include_wsl=not no_wsl,
            include_windows=not no_windows,
            show_progress=show_progress,
        )

    # Sync explicit remotes
    if remotes:
        if jobs > 1 and len(remotes) > 1:
            with ThreadPoolExecutor(max_workers=jobs) as executor:
                futures = [
                    executor.submit(_sync_remote_to_db_with_new_conn, remote, patterns, force)
                    for remote in remotes
                ]
                for future in futures:
                    _accumulate_stats(totals, future.result())
        else:
            for remote in remotes:
                _sync_remote_to_db(conn, remote, totals, patterns, force)

    conn.close()
    print(
        f"\nSync complete: {totals['synced']} synced, {totals['skipped']} up-to-date, {totals['errors']} errors"
    )


def _get_stats_workspace_patterns(args) -> list:
    """Get workspace patterns for stats command, applying alias detection."""
    workspace_patterns = getattr(args, "workspace", None) or []
    all_workspaces = getattr(args, "all_workspaces", False)
    this_only = getattr(args, "this_only", False)
    agent = getattr(args, "agent", None) or "auto"
    source_filter = getattr(args, "source", None)

    if workspace_patterns or all_workspaces:
        return workspace_patterns

    # If filtering by source, default to all workspaces for that source unless --this is set.
    if source_filter and not this_only:
        return []

    pattern, exists = check_current_workspace_exists()
    if not exists:
        sys.stderr.write(
            "Not in a Claude Code workspace.\n\n"
            "To show stats, either:\n"
            "  â€¢ Run from within a workspace directory\n"
            "  â€¢ Specify a workspace pattern: stats <pattern>\n"
            "  â€¢ Use --aw to include all workspaces: stats --aw\n"
        )
        sys.exit(1)

    full_pattern = get_current_workspace_pattern()
    ws_name = get_workspace_name_from_path(full_pattern)

    # Aliases are Claude-specific; skip for other agents
    if not this_only and agent in ("auto", "claude"):
        alias_name = get_alias_for_workspace(full_pattern, "local")
        if alias_name:
            sys.stderr.write(f"Using alias @{alias_name} (use --this for current workspace only)\n")
            return [f"@{alias_name}"]

    return [ws_name]


def _expand_alias_to_conditions(alias_name: str, aliases: dict) -> tuple:
    """Expand alias to SQL conditions and params. Returns (conditions, params)."""
    if alias_name not in aliases:
        print(f" Alias '{alias_name}' not found. Treating as workspace pattern.")
        return (["s.workspace LIKE ?"], [f"%{alias_name}%"])

    conditions = []
    params = []
    for source_key, workspaces in aliases[alias_name].items():
        if isinstance(workspaces, list):
            for ws in workspaces:
                conditions.append("s.workspace = ?")
                params.append(get_workspace_name_from_path(ws))
    return conditions, params


def _build_stats_where_clause(args, workspace_patterns: list) -> tuple:  # noqa: C901
    """Build WHERE clause for stats query. Returns (where_sql, params)."""
    where_clauses = []
    params = []

    if workspace_patterns:
        ws_conditions = []
        aliases = load_aliases().get("aliases", {})

        for pattern in workspace_patterns:
            if pattern.startswith("@"):
                conds, prms = _expand_alias_to_conditions(pattern[1:], aliases)
                ws_conditions.extend(conds)
                params.extend(prms)
            else:
                ws_conditions.append("s.workspace LIKE ?")
                params.append(f"%{pattern}%")

        if ws_conditions:
            where_clauses.append(f"({' OR '.join(ws_conditions)})")

    source_filter = getattr(args, "source", None)
    if source_filter:
        if source_filter == "wsl" and is_running_in_wsl():
            where_clauses.append("(s.source = ? OR s.source LIKE ? OR s.source IN (?, ?, ?))")
            params.extend([source_filter, f"{source_filter}:%", "local", "codex", "gemini"])
        elif source_filter in ("windows", "wsl", "remote"):
            where_clauses.append("(s.source = ? OR s.source LIKE ?)")
            params.extend([source_filter, f"{source_filter}:%"])
        else:
            where_clauses.append("s.source = ?")
            params.append(source_filter)

    # Filter by agent backend (claude, codex, gemini)
    agent_filter = getattr(args, "agent", None) or "auto"
    if agent_filter != "auto":
        where_clauses.append("s.agent = ?")
        params.append(agent_filter)

    since_str = getattr(args, "since", None)
    if since_str:
        where_clauses.append("s.start_time >= ?")
        params.append(since_str)

    until_str = getattr(args, "until", None)
    if until_str:
        where_clauses.append("s.start_time <= ?")
        params.append(until_str + "T23:59:59")

    where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"
    return where_sql, params


def _display_selected_stats(conn, args, where_sql: str, params: list):
    """Display the appropriate stats view based on args."""
    top_limit = getattr(args, "top_ws", None)
    if top_limit is not None:
        try:
            top_limit = int(top_limit)
            if top_limit < 1:
                raise ValueError
        except (TypeError, ValueError):
            sys.stderr.write("Error: --top-ws must be a positive integer\n")
            sys.exit(1)

    agent = getattr(args, "agent", None) or "auto"

    if getattr(args, "tools", False):
        display_tool_stats(conn, where_sql, params)
    elif getattr(args, "models", False):
        display_model_stats(conn, where_sql, params)
    elif getattr(args, "time", False):
        display_time_stats(conn, where_sql, params)
    elif getattr(args, "by_workspace", False):
        display_workspace_stats(conn, where_sql, params)
    elif getattr(args, "by_day", False):
        display_daily_stats(conn, where_sql, params)
    else:
        display_summary_stats(conn, where_sql, params, top_limit=top_limit, agent=agent)


def cmd_stats(args):
    """Display metrics statistics."""
    db_path = get_metrics_db_path()

    if not db_path.exists():
        print("No metrics database found. Run 'stats --sync' first.")
        print("\nUsage:")
        print("  agent-history stats --sync        # Sync local sessions")
        print("  agent-history stats --sync --ah   # Sync from all homes")
        sys.exit(1)

    conn = init_metrics_db(db_path)
    workspace_patterns = _get_stats_workspace_patterns(args)
    where_sql, params = _build_stats_where_clause(args, workspace_patterns)
    _display_selected_stats(conn, args, where_sql, params)
    conn.close()


def _query_session_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query session statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_sessions,
            SUM(CASE WHEN is_agent = 0 THEN 1 ELSE 0 END) as main_sessions,
            SUM(CASE WHEN is_agent = 1 THEN 1 ELSE 0 END) as agent_sessions,
            SUM(message_count) as total_messages
        FROM sessions s
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _query_token_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query token statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COALESCE(SUM(m.input_tokens), 0) as total_input,
            COALESCE(SUM(m.output_tokens), 0) as total_output,
            COALESCE(SUM(m.cache_creation_tokens), 0) as total_cache_creation,
            COALESCE(SUM(m.cache_read_tokens), 0) as total_cache_read
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _query_tool_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Query tool statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_tool_uses,
            SUM(CASE WHEN is_error = 1 THEN 1 ELSE 0 END) as tool_errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    return cursor.fetchone()


def _build_workspace_to_alias_map(aliases: Optional[dict] = None) -> dict:
    """Build mapping from (workspace, source) to alias name."""
    if aliases is None:
        aliases_data = load_aliases()
        aliases = aliases_data.get("aliases", {})
    else:
        aliases = aliases or {}

    workspace_to_alias = {}
    for alias_name, alias_config in aliases.items():
        for source_key, workspaces in alias_config.items():
            if isinstance(workspaces, list):
                normalized_source = "windows" if source_key.startswith("windows:") else source_key
                for ws in workspaces:
                    ws_name = get_workspace_name_from_path(ws)
                    workspace_to_alias[(ws_name, normalized_source)] = alias_name
    return workspace_to_alias


def _aggregate_workspace_stats(all_workspace_stats, workspace_to_alias: dict) -> list:
    """Aggregate workspace stats, grouping by alias where applicable."""
    alias_stats = {}
    unaliased_workspaces = []

    for row in all_workspace_stats:
        ws_name = row["workspace"]
        source = row["source"]
        sessions = row["sessions"]
        messages = row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {
                    "sessions": 0,
                    "messages": 0,
                    "workspaces": set(),
                    "sources": set(),
                }
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["workspaces"].add(ws_name)
            alias_stats[alias_name]["sources"].add(source)
        else:
            unaliased_workspaces.append(
                {
                    "name": ws_name,
                    "sessions": sessions,
                    "messages": messages,
                    "is_alias": False,
                    "source": source,
                }
            )

    top_workspaces = []
    for alias_name, stats in alias_stats.items():
        top_workspaces.append(
            {
                "name": f"@{alias_name}",
                "sessions": stats["sessions"],
                "messages": stats["messages"],
                "is_alias": True,
                "workspace_count": len(stats["workspaces"]),
                "source": ", ".join(sorted(stats["sources"])) if stats["sources"] else "alias",
            }
        )
    top_workspaces.extend(unaliased_workspaces)
    top_workspaces.sort(key=lambda x: x["sessions"], reverse=True)
    return top_workspaces


def _print_session_section(session_stats):
    """Print session statistics section."""
    print("\nSessions")
    print(f"   Total: {(session_stats['total_sessions'] or 0):,}")
    print(f"   Main sessions: {(session_stats['main_sessions'] or 0):,}")
    print(f"   Agent tasks: {(session_stats['agent_sessions'] or 0):,}")
    print(f"   Total messages: {(session_stats['total_messages'] or 0):,}")


def _print_token_section(token_stats):
    """Print token statistics section."""
    total_input = token_stats["total_input"] or 0
    total_output = token_stats["total_output"] or 0
    cache_creation = token_stats["total_cache_creation"] or 0
    cache_read = token_stats["total_cache_read"] or 0
    cache_total = cache_creation + cache_read
    cache_hit_ratio = (cache_read / cache_total * 100) if cache_total > 0 else 0

    print("\nTokens")
    print(f"   Input: {total_input:,}")
    print(f"   Output: {total_output:,}")
    print(f"   Cache created: {cache_creation:,}")
    print(f"   Cache read: {cache_read:,}")
    print(f"   Cache hit ratio: {cache_hit_ratio:.1f}%")


def _print_tool_section(tool_stats):
    """Print tool statistics section."""
    total_tool_uses = tool_stats["total_tool_uses"] or 0
    tool_errors = tool_stats["tool_errors"] or 0

    print("\nTools")
    print(f"   Total uses: {total_tool_uses:,}")
    print(f"   Errors: {tool_errors:,}")
    if total_tool_uses > 0:
        print(f"   Error rate: {tool_errors / total_tool_uses * 100:.1f}%")


def _print_sources_section(sources):
    """Print homes/sources section."""
    if not sources:
        return
    print("\nHomes")
    for src in sources:
        print(f"   {src['source']}: {src['count']:,} sessions")


def _print_models_section(models):
    """Print models section."""
    if not models:
        return
    print("\nModels (top 3)")
    for model in models:
        model_name = model["model"] or "unknown"
        short_name = (
            model_name.replace("claude-", "").replace("-20250929", "").replace("-20251001", "")
        )
        print(f"   {short_name}: {model['count']:,} messages")


def _print_workspaces_section(top_workspaces):
    """Print top workspaces section."""
    if not top_workspaces:
        return
    print("\nTop Workspaces")
    for ws in top_workspaces:
        if ws.get("is_alias"):
            ws_count = ws.get("workspace_count", 0)
            print(
                f"   {ws['name']} ({ws_count} workspaces): {ws['sessions']} sessions, {ws['messages']} msgs"
            )
        else:
            print(f"   {ws['name']}: {ws['sessions']} sessions, {ws['messages']} msgs")


def _parse_top_limit(top_limit) -> int | None:
    """Parse top_limit to a valid integer or None."""
    if top_limit is None:
        return None
    try:
        result = int(top_limit)
        return result if result >= 1 else None
    except (TypeError, ValueError):
        return None


def _print_workspace_item(ws: dict):
    """Print a single workspace item with appropriate formatting."""
    if ws.get("is_alias"):
        ws_count = ws.get("workspace_count", 0)
        print(
            f"      Workspace: {ws['name']} ({ws_count} workspaces)"
            f" â€” {ws['sessions']} sessions, {ws['messages']} msgs"
        )
    else:
        print(f"      Workspace: {ws['name']} â€” {ws['sessions']} sessions, {ws['messages']} msgs")


def _print_homes_and_workspaces(sources, top_workspaces, top_limit=None):
    """Print homes and workspaces together at the top of stats output."""
    if not sources and not top_workspaces:
        return
    print("\nHomes & Workspaces")

    source_map = {s.get("source"): s.get("count", 0) for s in (sources or [])}

    grouped = {}
    for ws in top_workspaces or []:
        home_label = ws.get("source") or "home"
        grouped.setdefault(home_label, []).append(ws)

    top_limit_int = _parse_top_limit(top_limit)

    for ws_items in grouped.values():
        ws_items.sort(key=lambda x: x.get("sessions", 0), reverse=True)

    ordered_homes = list(source_map.keys()) + [h for h in grouped if h not in source_map]

    for home in ordered_homes:
        total = source_map.get(home)
        print(f"   Home: {home} ({total:,} sessions)" if total is not None else f"   Home: {home}")
        ws_items = grouped.get(home, [])[:top_limit_int] if top_limit_int else grouped.get(home, [])
        for ws in ws_items:
            _print_workspace_item(ws)


def _print_summary_stats(stats: SummaryStatsData):
    """Print the formatted summary statistics."""
    agent_names = {
        "claude": "CLAUDE CODE",
        "codex": "CODEX CLI",
        "gemini": "GEMINI CLI",
        "auto": "AI ASSISTANT",
    }
    agent_name = agent_names.get(stats.agent, "AI ASSISTANT")
    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print(f"{agent_name} METRICS SUMMARY")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    _print_homes_and_workspaces(stats.sources, stats.top_workspaces, top_limit=stats.top_limit)
    _print_session_section(stats.session_stats)
    _print_token_section(stats.token_stats)
    _print_tool_section(stats.tool_stats)
    _print_models_section(stats.models)
    if stats.time_stats is not None:
        _print_time_summary(stats.time_stats, include_breakdown=False)
    print()


def display_summary_stats(
    conn: sqlite3.Connection, where_sql: str, params: list, top_limit=None, agent: str = "auto"
):
    """Display summary statistics dashboard."""
    session_stats = _query_session_stats(conn, where_sql, params)
    token_stats = _query_token_stats(conn, where_sql, params)
    tool_stats = _query_tool_stats(conn, where_sql, params)

    # Source breakdown
    cursor = conn.execute(
        f"SELECT source, COUNT(*) as count FROM sessions s WHERE {where_sql} GROUP BY source ORDER BY count DESC",
        params,
    )
    sources = [dict(row) for row in cursor.fetchall()]

    # Model breakdown (top 3)
    cursor = conn.execute(
        f"""SELECT m.model, COUNT(*) as count FROM messages m JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL GROUP BY m.model ORDER BY count DESC LIMIT 3""",
        params,
    )
    models = cursor.fetchall()

    # Top workspaces with alias aggregation
    cursor = conn.execute(
        f"""SELECT workspace, source, COUNT(*) as sessions, SUM(message_count) as messages FROM sessions s
        WHERE {where_sql} GROUP BY workspace, source ORDER BY sessions DESC""",
        params,
    )
    all_workspace_stats = cursor.fetchall()
    workspace_to_alias = _build_workspace_to_alias_map()
    top_workspaces = _aggregate_workspace_stats(all_workspace_stats, workspace_to_alias)

    time_stats = _compute_time_stats(conn, where_sql, params)

    stats = SummaryStatsData(
        session_stats=session_stats,
        token_stats=token_stats,
        tool_stats=tool_stats,
        sources=sources,
        models=models,
        top_workspaces=top_workspaces,
        top_limit=top_limit,
        time_stats=time_stats,
        agent=agent,
    )
    _print_summary_stats(stats)


def display_tool_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display tool usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            t.tool_name,
            COUNT(*) as uses,
            SUM(CASE WHEN t.is_error = 1 THEN 1 ELSE 0 END) as errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
        GROUP BY t.tool_name
        ORDER BY uses DESC
    """,
        params,
    )
    tools = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("TOOL USAGE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    print(f"\n{'Tool':<25} {'Uses':>10} {'Errors':>10} {'Error %':>10}")
    print("-" * 55)

    for row in tools:
        uses = row["uses"]
        errors = row["errors"]
        error_pct = (errors / uses * 100) if uses > 0 else 0
        print(f"{row['tool_name']:<25} {uses:>10,} {errors:>10,} {error_pct:>9.1f}%")

    print()


def display_model_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display model usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            m.model,
            COUNT(*) as messages,
            COUNT(DISTINCT m.session_id) as sessions,
            SUM(m.input_tokens) as input_tokens,
            SUM(m.output_tokens) as output_tokens
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL
        GROUP BY m.model
        ORDER BY messages DESC
    """,
        params,
    )
    models = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("MODEL USAGE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    print(f"\n{'Model':<35} {'Messages':>10} {'Sessions':>10} {'Avg Out':>10}")
    print("-" * 65)

    for row in models:
        model = row["model"] or "unknown"
        short_model = (
            model.replace("claude-", "")
            .replace("-20250929", "")
            .replace("-20251001", "")
            .replace("-20251101", "")
        )
        messages = row["messages"]
        avg_output = (row["output_tokens"] / messages) if messages > 0 else 0
        print(f"{short_model:<35} {messages:>10,} {row['sessions']:>10,} {avg_output:>10,.0f}")


def _aggregate_workspace_stats_detailed(all_workspaces: list, workspace_to_alias: dict) -> tuple:
    """Aggregate workspace stats by alias. Returns (alias_stats, unaliased)."""
    alias_stats = {}
    unaliased = []

    for row in all_workspaces:
        ws_name, source = row["workspace"], row["source"]
        sessions, messages = row["sessions"], row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {"sessions": 0, "messages": 0, "sources": set()}
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["sources"].add(source)
        else:
            unaliased.append(
                {"name": ws_name, "source": source, "sessions": sessions, "messages": messages}
            )

    return alias_stats, unaliased


def display_workspace_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display per-workspace statistics with alias aggregation."""
    cursor = conn.execute(
        f"""
        SELECT s.workspace, s.source, COUNT(*) as sessions, SUM(s.message_count) as messages,
               SUM(m_stats.input_tokens) as input_tokens, SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (SELECT session_id, SUM(input_tokens) as input_tokens, SUM(output_tokens) as output_tokens
                   FROM messages GROUP BY session_id) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql}
        GROUP BY s.workspace, s.source ORDER BY sessions DESC
    """,
        params,
    )
    all_workspaces = cursor.fetchall()

    aliases_data = load_aliases()
    workspace_to_alias = _build_workspace_to_alias_map(aliases_data.get("aliases", {}))
    alias_stats, unaliased = _aggregate_workspace_stats_detailed(all_workspaces, workspace_to_alias)

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("WORKSPACE STATISTICS")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    if alias_stats:
        print("\nAliases")
        print(f"{'Alias':<30} {'Sources':<15} {'Sessions':>8} {'Messages':>10}")
        print("-" * 65)
        for alias_name, stats in sorted(
            alias_stats.items(), key=lambda x: x[1]["sessions"], reverse=True
        ):
            sources_str = ",".join(sorted(stats["sources"]))[:13]
            print(
                f"@{alias_name:<29} {sources_str:<15} {stats['sessions']:>8,} {stats['messages']:>10,}"
            )

    print(f"\nIndividual Workspaces (Top {MAX_TOP_WORKSPACES})")
    print(f"{'Workspace':<30} {'Source':<10} {'Sessions':>8} {'Messages':>10}")
    print("-" * DISPLAY_SEPARATOR_WIDTH)

    for row in unaliased[:MAX_TOP_WORKSPACES]:
        ws_name = (
            row["name"][:MAX_WORKSPACE_NAME_DISPLAY]
            if len(row["name"]) > MAX_WORKSPACE_NAME_DISPLAY
            else row["name"]
        )
        print(f"{ws_name:<30} {row['source']:<10} {row['sessions']:>8,} {row['messages']:>10,}")

    print()


def display_daily_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display daily usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            DATE(s.start_time) as date,
            COUNT(*) as sessions,
            SUM(s.message_count) as messages,
            SUM(m_stats.input_tokens) as input_tokens,
            SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (
            SELECT session_id,
                   SUM(input_tokens) as input_tokens,
                   SUM(output_tokens) as output_tokens
            FROM messages
            GROUP BY session_id
        ) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql} AND s.start_time IS NOT NULL
        GROUP BY DATE(s.start_time)
        ORDER BY date DESC
        LIMIT 30
    """,
        params,
    )
    days = cursor.fetchall()

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("DAILY STATISTICS (Last 30 days)")
    print("=" * DISPLAY_SEPARATOR_WIDTH)

    max_sessions = max((row["sessions"] or 0) for row in days) if days else 0
    bar_header = "Bar (sessions)"
    print(f"\n{'Date':<12} {'Sessions':>10} {'Messages':>10} {'Input Tokens':>15}  {bar_header}")
    print("-" * 70)

    for row in days:
        date_str = row["date"] or "unknown"
        sessions = row["sessions"]
        messages = row["messages"] or 0
        tokens = row["input_tokens"] or 0
        bar = _build_bar(sessions, max_sessions)
        print(f"{date_str:<12} {sessions:>10,} {messages:>10,} {tokens:>15,}  {bar}")

    print()


def _build_bar(value: int, max_value: int, width: int = 20) -> str:
    """Return an ASCII bar scaled to the maximum value."""
    if max_value <= 0 or value <= 0:
        return ""
    ratio = value / max_value
    length = max(1, min(width, math.ceil(ratio * width)))
    return "#" * length


def format_duration_hm(seconds: float) -> str:
    """Format seconds as Xh Ym."""
    if seconds < SECONDS_PER_MINUTE:
        return f"{seconds:.0f}s"
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    if hours > 0:
        return f"{hours}h {minutes}m"
    return f"{minutes}m"


def _parse_timestamp_strings(raw_timestamps: list) -> list:
    """Parse timestamp strings to datetime objects, filtering invalid ones."""
    parsed = []
    for ts_str in raw_timestamps:
        if not ts_str:
            continue
        try:
            parsed.append(datetime.fromisoformat(ts_str.replace("Z", "+00:00")))
        except (ValueError, TypeError):
            continue
    return parsed


def _ensure_date_entry(stats: dict, date_str: str):
    """Ensure a date entry exists in the stats dict."""
    if date_str not in stats:
        stats[date_str] = {"work_seconds": 0.0, "messages": 0, "work_periods": 0}


def _add_period_time_to_stats(
    stats: dict, start_dt: datetime, end_dt: datetime, is_new_period: bool = False
):
    """Add work period time to stats, splitting across day boundaries if needed."""
    if start_dt >= end_dt:
        return

    current = start_dt
    first_day = True
    while current < end_dt:
        date_str = current.strftime("%Y-%m-%d")
        _ensure_date_entry(stats, date_str)

        if is_new_period and first_day:
            stats[date_str]["work_periods"] += 1
            first_day = False

        next_midnight = (current + timedelta(days=1)).replace(
            hour=0, minute=0, second=0, microsecond=0
        )

        if next_midnight >= end_dt:
            stats[date_str]["work_seconds"] += (end_dt - current).total_seconds()
            break
        stats[date_str]["work_seconds"] += (next_midnight - current).total_seconds()
        current = next_midnight


def calculate_daily_work_time(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Calculate work time per day from message timestamps.

    This properly distributes work time across days, even for sessions
    that span multiple days. Uses gap detection (30 min threshold) to
    identify work periods.

    IMPORTANT: Merges ALL timestamps across all files to avoid double-counting
    overlapping time from concurrent agents.

    Returns dict: {date_str: {'work_seconds': float, 'messages': int, 'work_periods': int}}
    """
    cursor = conn.execute(
        f"""
        SELECT m.timestamp
        FROM messages m
        JOIN sessions s ON m.file_path = s.file_path
        WHERE {where_sql} AND m.timestamp IS NOT NULL
        ORDER BY m.timestamp
    """,
        params,
    )

    parsed_timestamps = _parse_timestamp_strings([row["timestamp"] for row in cursor])
    if not parsed_timestamps:
        return {}
    parsed_timestamps.sort()

    daily_stats = {}
    prev_ts = None
    period_start = None

    for ts in parsed_timestamps:
        date_str = ts.strftime("%Y-%m-%d")
        _ensure_date_entry(daily_stats, date_str)
        daily_stats[date_str]["messages"] += 1

        if prev_ts is None:
            period_start = ts
            daily_stats[date_str]["work_periods"] += 1
        elif (ts - prev_ts).total_seconds() > WORK_PERIOD_GAP_THRESHOLD:
            _add_period_time_to_stats(daily_stats, period_start, prev_ts)
            period_start = ts
            daily_stats[date_str]["work_periods"] += 1

        prev_ts = ts

    if period_start and prev_ts:
        _add_period_time_to_stats(daily_stats, period_start, prev_ts)

    return daily_stats


def _calculate_time_totals(daily_stats: dict) -> tuple:
    """Calculate total work, messages, and periods from daily stats."""
    total_work = sum(d["work_seconds"] for d in daily_stats.values())
    total_messages = sum(d["messages"] for d in daily_stats.values())
    total_periods = sum(d["work_periods"] for d in daily_stats.values())
    return total_work, total_messages, total_periods


def _print_daily_breakdown(
    daily_stats: dict, total_work: int, total_periods: int, total_messages: int
):
    """Print daily breakdown table."""
    print("\nDaily Breakdown")
    print(f"\n{'Date':<12} {'Work Time':>12} {'Periods':>10} {'Messages':>10}  Bar (time)")
    print("-" * 70)

    max_work = (
        max((stats["work_seconds"] or 0) for stats in daily_stats.values()) if daily_stats else 0
    )

    for date_str in sorted(daily_stats.keys(), reverse=True):
        stats = daily_stats[date_str]
        work_seconds = stats["work_seconds"]
        work_time = format_duration_hm(work_seconds)
        bar = _build_bar(int(work_seconds), int(max_work))
        print(
            f"{date_str:<12} {work_time:>12} {stats['work_periods']:>10} {stats['messages']:>10}  {bar}"
        )

    print("-" * 70)
    print(
        f"{'TOTAL':<12} {format_duration_hm(total_work):>12} {total_periods:>10} {total_messages:>10}"
    )


def _compute_time_stats(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Compute time stats used by both summary and --time output."""
    daily_stats = calculate_daily_work_time(conn, where_sql, params)
    total_work, total_messages, total_periods = _calculate_time_totals(daily_stats)

    cursor = conn.execute(f"SELECT COUNT(*) as num_files FROM sessions s WHERE {where_sql}", params)
    num_files = cursor.fetchone()["num_files"] or 0

    dates = sorted(daily_stats.keys()) if daily_stats else []
    first_date, last_date = (dates[0], dates[-1]) if dates else (None, None)

    return {
        "daily_stats": daily_stats,
        "total_work": total_work,
        "total_messages": total_messages,
        "total_periods": total_periods,
        "num_files": num_files,
        "first_date": first_date,
        "last_date": last_date,
    }


def _print_time_summary(time_stats: dict, include_breakdown: bool = False):
    """Print time tracking summary (optionally with daily breakdown)."""
    daily_stats = time_stats["daily_stats"]
    total_work = time_stats["total_work"]
    total_messages = time_stats["total_messages"]
    total_periods = time_stats["total_periods"]
    num_files = time_stats["num_files"]
    first_date = time_stats["first_date"]
    last_date = time_stats["last_date"]

    print("\nTime")
    print(f"   Total work time: {format_duration_hm(total_work)}")
    print(f"   Work periods: {total_periods}")
    print(f"   Session files: {num_files}")
    if first_date and last_date:
        print(f"   Date range: {first_date} to {last_date}")

    if include_breakdown and daily_stats:
        _print_daily_breakdown(daily_stats, total_work, total_periods, total_messages)


def display_time_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display time tracking statistics with daily breakdown."""
    time_stats = _compute_time_stats(conn, where_sql, params)

    print("=" * DISPLAY_SEPARATOR_WIDTH)
    print("TIME TRACKING")
    print("=" * DISPLAY_SEPARATOR_WIDTH)
    _print_time_summary(time_stats, include_breakdown=True)
    print()


# ============================================================================
# Commands
# ============================================================================


def _scan_codex_gemini_sessions(
    scan_agent: str,
    patterns: list,
    since_date,
    until_date,
    sessions_dir: Path,
    source: str,
    skip_message_count: bool = True,
    use_cached_counts: bool = False,
) -> list:
    """Scan Codex or Gemini sessions from a directory."""
    pattern_str = patterns[0] if patterns else ""
    if scan_agent == AGENT_CODEX:
        sessions = codex_scan_sessions(
            pattern=pattern_str,
            since_date=since_date,
            until_date=until_date,
            sessions_dir=sessions_dir,
            skip_message_count=skip_message_count,
            use_cached_counts=use_cached_counts,
        )
    else:  # AGENT_GEMINI
        sessions = gemini_scan_sessions(
            pattern=pattern_str,
            since_date=since_date,
            until_date=until_date,
            sessions_dir=sessions_dir,
            skip_message_count=skip_message_count,
            use_cached_counts=use_cached_counts,
        )
    for s in sessions:
        s["source"] = source
    return sessions


def _show_no_sessions_error(agent: str, location: str, platform_flag: str):
    """Show error message when no sessions are found."""
    if agent == "auto":
        sys.stderr.write(f"Error: No matching sessions found in {location}\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  â€¢ Check that Claude/Codex/Gemini is installed\n")
        sys.stderr.write(f"  â€¢ Try: agent-history lsh {platform_flag}\n")
        sys.stderr.write("  â€¢ Try: agent-history lss --aw (all workspaces)\n")
    else:
        sys.stderr.write(f"Error: No matching {agent} sessions found in {location}\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write(f"  â€¢ Check that {agent} is installed\n")
        sys.stderr.write(f"  â€¢ Try: agent-history --agent auto lss {platform_flag}\n")
        sys.stderr.write("  â€¢ Try: agent-history lss --aw (all workspaces)\n")
    sys.exit(1)


def _list_windows_sessions(args, patterns: list, since_date, until_date):
    """List sessions from Windows (called from WSL)."""
    agent = getattr(args, "agent", None) or "auto"
    workspaces_only = getattr(args, "workspaces_only", False)

    remote_host = getattr(args, "remote", None)
    username = remote_host[10:] if remote_host and remote_host.startswith("windows://") else None

    all_sessions = []
    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]

    for scan_agent in agents_to_scan:
        sessions_dir = get_agent_windows_dir(username, scan_agent)
        if not sessions_dir or not sessions_dir.exists():
            continue

        if scan_agent == AGENT_CLAUDE:
            sessions = collect_sessions_with_dedup(
                patterns,
                since_date,
                until_date,
                projects_dir=sessions_dir,
                skip_message_count=workspaces_only,
                agent=AGENT_CLAUDE,
            )
            all_sessions.extend(sessions)
        else:
            all_sessions.extend(
                _scan_codex_gemini_sessions(
                    scan_agent,
                    patterns,
                    since_date,
                    until_date,
                    sessions_dir,
                    "windows",
                    skip_message_count=workspaces_only,
                )
            )

    if not all_sessions:
        _show_no_sessions_error(agent, "Windows", "--windows")

    source_label = f"Windows ({username})" if username else "Windows"
    print_sessions_output(all_sessions, source_label, workspaces_only)


def _list_wsl_claude_workspaces(sessions_dir: Path, patterns: list) -> list:
    """List Claude workspaces from WSL directory."""
    raw_workspaces = _get_workspaces_from_dir(sessions_dir)
    workspaces = _filter_workspaces_by_patterns(raw_workspaces, patterns)
    base_path = _detect_wsl_base_path(sessions_dir)
    return [
        {
            "workspace": ws,
            "workspace_readable": normalize_workspace_name(ws, base_path=base_path),
            "agent": AGENT_CLAUDE,
        }
        for ws in workspaces
    ]


def _should_skip_wsl_message_count(args, workspaces_only: bool) -> bool:
    """Return True if WSL message counts should be skipped (Windows -> WSL)."""
    if workspaces_only:
        return True
    if getattr(args, "counts", False) or getattr(args, "wsl_counts", False):
        return False
    return True


def _list_wsl_sessions(args, patterns: list, since_date, until_date):
    """List sessions from WSL (called from Windows)."""
    agent = getattr(args, "agent", None) or "auto"
    workspaces_only = getattr(args, "workspaces_only", False)

    remote_host = getattr(args, "remote", None)
    # remote_host should be a wsl:// URL for WSL sessions
    assert remote_host is not None
    distro_name = remote_host[WSL_PREFIX_LEN:]

    all_sessions = []
    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]
    source = f"wsl:{distro_name}"
    skip_counts = _should_skip_wsl_message_count(args, workspaces_only)
    use_cached_counts = getattr(args, "counts", False) or getattr(args, "wsl_counts", False)

    for scan_agent in agents_to_scan:
        sessions_dir = get_agent_wsl_dir(distro_name, scan_agent)
        if not sessions_dir or not sessions_dir.exists():
            continue

        if scan_agent == AGENT_CLAUDE:
            if workspaces_only:
                all_sessions.extend(_list_wsl_claude_workspaces(sessions_dir, patterns))
            else:
                sessions = collect_sessions_with_dedup(
                    patterns,
                    since_date,
                    until_date,
                    projects_dir=sessions_dir,
                    skip_message_count=skip_counts,
                    use_cached_counts=use_cached_counts,
                    agent=AGENT_CLAUDE,
                )
                all_sessions.extend(sessions)
        else:
            all_sessions.extend(
                _scan_codex_gemini_sessions(
                    scan_agent,
                    patterns,
                    since_date,
                    until_date,
                    sessions_dir,
                    source,
                    skip_message_count=skip_counts,
                    use_cached_counts=use_cached_counts,
                )
            )

    if not all_sessions:
        _show_no_sessions_error(agent, f"WSL '{distro_name}'", "--wsl")

    source_label = f"WSL ({distro_name})"
    print_sessions_output(all_sessions, source_label, workspaces_only)


# ============================================================================
# SSH Remote Session Listing - Claude
# ============================================================================


def _list_remote_claude_workspaces_only(remote_host: Optional[str], patterns: list):
    """List Claude workspaces only for remote host."""
    batch_workspaces = get_remote_workspaces_batch(remote_host)
    if not batch_workspaces:
        return []

    batch_workspaces = [
        ws for ws in batch_workspaces if matches_any_pattern(ws["encoded"], patterns)
    ]

    return [{"decoded": ws["decoded"], "agent": AGENT_CLAUDE} for ws in batch_workspaces]


def _collect_remote_claude_session_details(
    remote_host: Optional[str], patterns: list, since_date, until_date
):
    """Collect detailed Claude session info from remote workspaces."""
    remote_workspaces = list_remote_workspaces(remote_host)
    if not remote_workspaces:
        return []

    remote_workspaces = [ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)]
    if not remote_workspaces:
        return []

    sessions = []
    for remote_workspace in remote_workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        workspace_sessions = get_remote_session_info(remote_host, remote_workspace)

        for session_info in workspace_sessions:
            session = _filter_sessions_by_date([session_info], since_date, until_date)
            if session:
                sessions.append(
                    {
                        "workspace": remote_workspace,
                        "workspace_readable": readable_name,
                        "filename": session_info["filename"],
                        "size_kb": session_info["size_kb"],
                        "modified": session_info["modified"],
                        "message_count": session_info["message_count"],
                        "agent": AGENT_CLAUDE,
                    }
                )
    return sessions


# ============================================================================
# SSH Remote Session Listing - Gemini
# ============================================================================


def _list_remote_gemini_workspaces_only(remote_host: Optional[str], patterns: list):
    """List Gemini workspaces (project hashes) for remote host."""
    workspaces = gemini_list_remote_workspaces(remote_host)
    if not workspaces:
        return []

    # Try to resolve hash names using remote hash index
    remote_index = gemini_fetch_remote_hash_index(remote_host, get_config_dir())

    result = []
    for ws_hash in workspaces:
        # Use resolved path if available, otherwise show hash
        resolved_path = remote_index.get(ws_hash, ws_hash)
        if matches_any_pattern(resolved_path, patterns) or matches_any_pattern(ws_hash, patterns):
            result.append({"decoded": resolved_path, "agent": AGENT_GEMINI})

    return result


def _collect_remote_gemini_session_details(
    remote_host: Optional[str], patterns: list, since_date, until_date
):
    """Collect detailed Gemini session info from remote."""
    sessions_info = gemini_get_remote_session_info(remote_host)
    if not sessions_info:
        return []

    # Try to resolve hash names using remote hash index
    remote_index = gemini_fetch_remote_hash_index(remote_host, get_config_dir())

    sessions = []
    for session_info in sessions_info:
        ws_hash = session_info.get("workspace", "unknown")
        resolved_path = remote_index.get(ws_hash, ws_hash)

        # Apply pattern filter
        if not (
            matches_any_pattern(resolved_path, patterns) or matches_any_pattern(ws_hash, patterns)
        ):
            continue

        # Apply date filter
        filtered = _filter_sessions_by_date([session_info], since_date, until_date)
        if filtered:
            sessions.append(
                {
                    "workspace": ws_hash,
                    "workspace_readable": resolved_path,
                    "filename": session_info["filename"],
                    "size_kb": session_info["size_kb"],
                    "modified": session_info["modified"],
                    "message_count": session_info["message_count"],
                    "agent": AGENT_GEMINI,
                }
            )

    return sessions


# ============================================================================
# SSH Remote Session Listing - Codex
# ============================================================================


def _list_remote_codex_workspaces_only(remote_host: Optional[str], patterns: list):
    """List Codex workspaces (cwds) for remote host."""
    workspaces = codex_list_remote_workspaces(remote_host)
    if not workspaces:
        return []

    result = []
    for ws_path in workspaces:
        # Keep full path for consistency with local Codex output, but allow matching by basename too.
        basename = ws_path.split("/")[-1] if "/" in ws_path else ws_path
        if matches_any_pattern(ws_path, patterns) or matches_any_pattern(basename, patterns):
            result.append({"decoded": ws_path, "agent": AGENT_CODEX})

    return result


def _collect_remote_codex_session_details(
    remote_host: Optional[str], patterns: list, since_date, until_date
):
    """Collect detailed Codex session info from remote."""
    sessions_info = codex_get_remote_session_info(remote_host)
    if not sessions_info:
        return []

    sessions = []
    for session_info in sessions_info:
        ws_short = session_info.get("workspace", "unknown")
        ws_full = session_info.get("workspace_full", ws_short)

        # Apply pattern filter
        if not (matches_any_pattern(ws_full, patterns) or matches_any_pattern(ws_short, patterns)):
            continue

        # Apply date filter
        filtered = _filter_sessions_by_date([session_info], since_date, until_date)
        if filtered:
            sessions.append(
                {
                    "workspace": ws_short,
                    "workspace_readable": ws_full,
                    "filename": session_info["filename"],
                    "size_kb": session_info["size_kb"],
                    "modified": session_info["modified"],
                    "message_count": session_info["message_count"],
                    "agent": AGENT_CODEX,
                }
            )

    return sessions


# ============================================================================
# SSH Remote Session Listing - Unified Dispatch
# ============================================================================


def _list_remote_workspaces_only(remote_host: Optional[str], patterns: list, agent: str = "auto"):
    """List workspaces for remote host, dispatching by agent type."""
    all_workspaces = []

    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_GEMINI, AGENT_CODEX]

    for scan_agent in agents_to_scan:
        if scan_agent == AGENT_CLAUDE:
            all_workspaces.extend(_list_remote_claude_workspaces_only(remote_host, patterns))
        elif scan_agent == AGENT_GEMINI:
            all_workspaces.extend(_list_remote_gemini_workspaces_only(remote_host, patterns))
        elif scan_agent == AGENT_CODEX:
            all_workspaces.extend(_list_remote_codex_workspaces_only(remote_host, patterns))

    if not all_workspaces:
        pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
        exit_with_error(f"No workspaces matching pattern '{pattern_str}' on {remote_host}")

    # Deduplicate across agents. In auto mode, the same workspace can appear in multiple backends.
    # Since lsw output is just workspace names (no agent column), duplicates are confusing.
    seen = set()
    for ws_info in sorted(all_workspaces, key=lambda x: x["decoded"]):
        decoded = ws_info["decoded"]
        if decoded in seen:
            continue
        seen.add(decoded)
        print(decoded)


def _collect_remote_session_details(
    remote_host: Optional[str], patterns: list, since_date, until_date, agent: str = "auto"
):
    """Collect detailed session info from remote workspaces, dispatching by agent type."""
    all_sessions = []

    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_GEMINI, AGENT_CODEX]

    for scan_agent in agents_to_scan:
        if scan_agent == AGENT_CLAUDE:
            all_sessions.extend(
                _collect_remote_claude_session_details(
                    remote_host, patterns, since_date, until_date
                )
            )
        elif scan_agent == AGENT_GEMINI:
            all_sessions.extend(
                _collect_remote_gemini_session_details(
                    remote_host, patterns, since_date, until_date
                )
            )
        elif scan_agent == AGENT_CODEX:
            all_sessions.extend(
                _collect_remote_codex_session_details(remote_host, patterns, since_date, until_date)
            )

    return all_sessions


def _list_ssh_remote_sessions(args, patterns: list, since_date, until_date):
    """List sessions from SSH remote."""
    remote_host = getattr(args, "remote", None)
    agent = getattr(args, "agent", None) or "auto"

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        exit_with_error(f"Setup: ssh-copy-id {remote_host}")

    workspaces_only = getattr(args, "workspaces_only", False)

    if workspaces_only:
        _list_remote_workspaces_only(remote_host, patterns, agent)
    else:
        sessions = _collect_remote_session_details(
            remote_host, patterns, since_date, until_date, agent
        )
        if not sessions:
            exit_with_error("No sessions found")
        print_sessions_output(sessions, f"Remote ({remote_host})", workspaces_only=False)


def _list_local_sessions(args, patterns: list, since_date, until_date):
    """List sessions from local filesystem.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
    """
    projects_dir = getattr(args, "projects_dir", None)
    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        include_cached=True,
        projects_dir=projects_dir,
        agent=agent,
    )

    workspaces_only = getattr(args, "workspaces_only", False)

    if not sessions:
        if not workspaces_only:
            pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
            if since_date or until_date:
                date_range = []
                if since_date:
                    date_range.append(f"from {since_date.strftime('%Y-%m-%d')}")
                if until_date:
                    date_range.append(f"until {until_date.strftime('%Y-%m-%d')}")
                sys.stderr.write(f"No sessions found {' '.join(date_range)}\n")
            else:
                sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
            sys.exit(1)
        return

    in_wsl = is_running_in_wsl()
    source_label = "Local (WSL)" if in_wsl else "Local"
    print_sessions_output(sessions, source_label, workspaces_only)


def cmd_list(args):
    """List sessions for a workspace."""
    remote_host = getattr(args, "remote", None)
    if remote_host and remote_host.startswith("wsl://") and getattr(args, "no_wsl", False):
        exit_with_error("Conflicting flags: --no-wsl with WSL listing")
    if remote_host == "windows" and getattr(args, "no_windows", False):
        exit_with_error("Conflicting flags: --no-windows with Windows listing")

    # Parse and validate date filters
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)
    since_date, until_date = parse_and_validate_dates(since_str, until_str)

    # Get patterns
    patterns = get_patterns_from_args(args)

    # Dispatch to appropriate handler based on source type
    if remote_host:
        if is_windows_remote(remote_host):
            _list_windows_sessions(args, patterns, since_date, until_date)
        elif is_wsl_remote(remote_host):
            _list_wsl_sessions(args, patterns, since_date, until_date)
        else:
            _list_ssh_remote_sessions(args, patterns, since_date, until_date)
    else:
        _list_local_sessions(args, patterns, since_date, until_date)


def _parse_file_to_markdown(jsonl_file: Path, agent_type: str) -> str:
    """Parse a file to markdown using the appropriate parser for the agent type."""
    if agent_type == AGENT_GEMINI:
        return gemini_parse_json_to_markdown(jsonl_file)
    if agent_type == AGENT_CODEX:
        return codex_parse_jsonl_to_markdown(jsonl_file)
    return parse_jsonl_to_markdown(jsonl_file)


def _parse_file_to_markdown_with_display(
    local_file: Path, agent_type: str, display_file: Optional[str] = None
) -> str:
    """Parse a file to markdown, overriding the displayed filename for Claude.

    This is primarily used for remote conversions where we download to a temporary
    path but want headers to show the original remote location.
    """
    if agent_type == AGENT_GEMINI:
        return gemini_parse_json_to_markdown(local_file)
    if agent_type == AGENT_CODEX:
        return codex_parse_jsonl_to_markdown(local_file)
    return parse_jsonl_to_markdown(local_file, display_file=display_file)


def _download_remote_file(remote_host: str, remote_path: str, local_path: Path) -> bool:
    """Download a file from remote host via scp. Returns True on success."""
    result = subprocess.run(
        [get_command_path("scp"), f"{remote_host}:{remote_path}", str(local_path)],
        check=False,
        capture_output=True,
        text=True,
        timeout=SCP_TIMEOUT,
    )
    if result.returncode != 0:
        sys.stderr.write(f"Error downloading file: {result.stderr}\n")
        return False
    return True


def _convert_remote_file(args, remote_host: str):
    """Convert a remote file to markdown."""
    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as tmp:
        tmp_path = Path(tmp.name)

    try:
        if not _download_remote_file(remote_host, args.jsonl_file, tmp_path):
            tmp_path.unlink()
            sys.exit(1)

        filename = Path(args.jsonl_file).name
        output_file = Path(args.output) if args.output else Path(filename).with_suffix(".md")
        agent_type = detect_agent_from_path(Path(args.jsonl_file))

        display_file = f"{remote_host}:{args.jsonl_file}"
        markdown = _parse_file_to_markdown_with_display(tmp_path, agent_type, display_file)
        output_file.write_text(markdown, encoding="utf-8")
        if not getattr(args, "quiet", False):
            print(output_file)
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


def _convert_local_file(args):
    """Convert a local file to markdown."""
    jsonl_file = Path(args.jsonl_file)

    if not jsonl_file.exists():
        sys.stderr.write(f"Error: {jsonl_file} not found\n")
        sys.exit(1)

    output_file = Path(args.output) if args.output else jsonl_file.with_suffix(".md")

    try:
        agent_type = detect_agent_from_path(jsonl_file)
        markdown = _parse_file_to_markdown(jsonl_file, agent_type)
        output_file.write_text(markdown, encoding="utf-8")
        if not getattr(args, "quiet", False):
            print(output_file)
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error converting file: {e}\n")
        sys.exit(1)


def cmd_convert(args):
    """Convert a single .jsonl file to markdown."""
    remote_host = getattr(args, "remote", None)

    if remote_host:
        _convert_remote_file(args, remote_host)
    else:
        _convert_local_file(args)


def _get_batch_windows_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from Windows for batch export.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter

    Returns:
        List of session dictionaries.
    """
    remote_host = getattr(args, "remote", None)
    username = None
    if remote_host and remote_host.startswith("windows://"):
        username = remote_host[10:]

    windows_projects_dir = get_windows_projects_dir(username)
    if not windows_projects_dir:
        sys.stderr.write("Error: Cannot access Windows projects directory\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  â€¢ Make sure Claude Code is installed on Windows\n")
        sys.stderr.write("  â€¢ Try: agent-history lsh --windows\n")
        sys.exit(1)

    agent = getattr(args, "agent", None) or "auto"
    sessions = collect_sessions_with_dedup(
        patterns,
        since_date,
        until_date,
        projects_dir=windows_projects_dir,
        skip_message_count=True,  # Slow over WSL mount
        agent=agent,
    )

    if not sessions:
        sys.stderr.write("Error: No native sessions found in Windows\n")
        sys.exit(1)

    return sessions


def _get_batch_wsl_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from WSL for batch export.

    Args:
        args: Parsed arguments
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter

    Returns:
        List of session dictionaries.
    """
    remote_host = getattr(args, "remote", None)
    # remote_host should be a wsl:// URL for WSL sessions
    assert remote_host is not None
    distro_name = remote_host[WSL_PREFIX_LEN:]  # Remove 'wsl://' prefix

    agent = getattr(args, "agent", None) or "auto"
    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]

    sessions = []
    for scan_agent in agents_to_scan:
        if scan_agent == AGENT_CLAUDE:
            wsl_projects_dir = get_wsl_projects_dir(distro_name)
            if not wsl_projects_dir:
                if agent == AGENT_CLAUDE:
                    sys.stderr.write(f"Error: Cannot access WSL distribution '{distro_name}'\n")
                    sys.stderr.write("\nTips:\n")
                    sys.stderr.write("  â€¢ Make sure Claude Code is installed in WSL\n")
                    sys.stderr.write("  â€¢ Try: agent-history lsh --wsl\n")
                    sys.exit(1)
                continue
            sessions.extend(
                collect_sessions_with_dedup(
                    patterns,
                    since_date,
                    until_date,
                    projects_dir=wsl_projects_dir,
                    skip_message_count=True,  # Slow over Windows/WSL boundary
                    agent=AGENT_CLAUDE,
                )
            )
            continue

        sessions_dir = get_agent_wsl_dir(distro_name, scan_agent)
        if not sessions_dir or not sessions_dir.exists():
            if agent == scan_agent:
                sys.stderr.write(f"Error: Cannot access WSL distribution '{distro_name}'\n")
                sys.stderr.write("\nTips:\n")
                sys.stderr.write(f"  â€¢ Make sure {scan_agent} is installed in WSL\n")
                sys.stderr.write("  â€¢ Try: agent-history lsh --wsl\n")
                sys.exit(1)
            continue

        sessions.extend(
            _scan_codex_gemini_sessions(
                scan_agent,
                patterns,
                since_date,
                until_date,
                sessions_dir,
                f"wsl:{distro_name}",
            )
        )

    if not sessions:
        sys.stderr.write(f"Error: No sessions found in WSL '{distro_name}'\n")
        sys.exit(1)

    return sessions


def _fetch_remote_workspaces(
    remote_host: Optional[str], workspaces: list, local_projects_dir: Path, hostname: str
):
    """Fetch multiple remote workspaces to local cache."""
    for remote_workspace in workspaces:
        result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)
        if not result["success"]:
            sys.stderr.write(
                f"Error fetching {remote_workspace}: {result.get('error', 'Unknown error')}\n"
            )
        elif result.get("warning"):
            sys.stderr.write(f"Warning: {remote_workspace}: {result['warning']}\n")


def _deduplicate_sessions(sessions: list, patterns: list) -> list:
    """Filter sessions by pattern and remove duplicates."""
    result = []
    seen_files = set()
    for session in sessions:
        if not matches_any_pattern(session["workspace"], patterns):
            continue
        file_key = str(session["file"])
        if file_key not in seen_files:
            seen_files.add(file_key)
            result.append(session)
    return result


def _get_batch_ssh_claude_sessions(
    remote_host: Optional[str], patterns: list, since_date, until_date, hostname: str
) -> list:
    """Get Claude sessions from SSH remote for batch export."""
    all_remote_workspaces = list_remote_workspaces(remote_host)
    if not all_remote_workspaces:
        return []

    remote_workspaces = [ws for ws in all_remote_workspaces if matches_any_pattern(ws, patterns)]
    if not remote_workspaces:
        return []

    local_projects_dir = get_claude_projects_dir()
    _fetch_remote_workspaces(remote_host, remote_workspaces, local_projects_dir, hostname)

    cached_prefix = f"remote_{hostname}_"
    all_cached_sessions = get_workspace_sessions(
        cached_prefix, quiet=True, since_date=since_date, until_date=until_date, include_cached=True
    )
    return _deduplicate_sessions(all_cached_sessions, patterns)


def _get_batch_ssh_gemini_sessions(
    remote_host: Optional[str], patterns: list, since_date, until_date, hostname: str
) -> list:
    """Get Gemini sessions from SSH remote for batch export."""
    # Fetch all Gemini sessions from remote
    local_cache_dir = get_config_dir()
    result = gemini_fetch_remote_sessions(remote_host, local_cache_dir, hostname)

    if not result["success"]:
        sys.stderr.write(
            f"Warning: Failed to fetch Gemini sessions: {result.get('error', 'Unknown')}\n"
        )
        return []
    if result.get("warning"):
        sys.stderr.write(f"Warning: gemini: {result['warning']}\n")

    # Scan the local cache for Gemini sessions
    local_gemini_dir = local_cache_dir / f"remote_{hostname}_gemini"
    if not local_gemini_dir.exists():
        return []

    # Try to get remote hash index for workspace name resolution
    remote_index = gemini_fetch_remote_hash_index(remote_host, local_cache_dir)

    sessions = []
    for json_file in local_gemini_dir.glob("*/chats/session-*.json"):
        # Extract project hash from path
        path_parts = json_file.parts
        try:
            # Find the hash directory (parent of 'chats')
            chats_idx = path_parts.index("chats")
            ws_hash = path_parts[chats_idx - 1]
        except (ValueError, IndexError):
            ws_hash = "unknown"

        # Resolve workspace name
        workspace = remote_index.get(ws_hash, ws_hash)

        # Apply pattern filter
        if not (matches_any_pattern(workspace, patterns) or matches_any_pattern(ws_hash, patterns)):
            continue

        modified = datetime.fromtimestamp(json_file.stat().st_mtime)

        # Apply date filter
        if not is_date_in_range(modified, since_date, until_date):
            continue

        # Count messages
        message_count = 0
        try:
            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)
                message_count = len(data.get("messages", []))
        except (OSError, json.JSONDecodeError):
            pass

        sessions.append(
            {
                "file": json_file,
                "workspace": workspace,
                "workspace_readable": workspace,
                "filename": json_file.name,
                "modified": modified,
                "message_count": message_count,
                "size_kb": json_file.stat().st_size / 1024,
                "agent": AGENT_GEMINI,
            }
        )

    return sessions


def _get_batch_ssh_codex_sessions(
    remote_host: Optional[str], patterns: list, since_date, until_date, hostname: str
) -> list:
    """Get Codex sessions from SSH remote for batch export."""
    # Fetch all Codex sessions from remote
    local_cache_dir = get_config_dir()
    result = codex_fetch_remote_sessions(remote_host, local_cache_dir, hostname)

    if not result["success"]:
        sys.stderr.write(
            f"Warning: Failed to fetch Codex sessions: {result.get('error', 'Unknown')}\n"
        )
        return []
    if result.get("warning"):
        sys.stderr.write(f"Warning: codex: {result['warning']}\n")

    # Scan the local cache for Codex sessions
    local_codex_dir = local_cache_dir / f"remote_{hostname}_codex"
    if not local_codex_dir.exists():
        return []

    sessions = []
    for jsonl_file in local_codex_dir.glob("*/*/*/*.jsonl"):
        # Extract workspace from first line
        workspace = codex_get_workspace_from_session(jsonl_file)
        if not workspace:
            workspace = "unknown"

        ws_short = workspace.split("/")[-1] if "/" in workspace else workspace

        # Apply pattern filter
        if not (
            matches_any_pattern(workspace, patterns) or matches_any_pattern(ws_short, patterns)
        ):
            continue

        modified = datetime.fromtimestamp(jsonl_file.stat().st_mtime)

        # Apply date filter
        if not is_date_in_range(modified, since_date, until_date):
            continue

        # Count messages (use helper to count valid messages only)
        message_count = _count_file_messages(jsonl_file, skip_count=False)

        sessions.append(
            {
                "file": jsonl_file,
                "workspace": ws_short,
                "workspace_readable": ws_short,
                "filename": jsonl_file.name,
                "modified": modified,
                "message_count": message_count,
                "size_kb": jsonl_file.stat().st_size / 1024,
                "agent": AGENT_CODEX,
            }
        )

    return sessions


def _get_batch_ssh_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get sessions from SSH remote for batch export."""
    remote_host = getattr(args, "remote", None)
    agent = getattr(args, "agent", None) or "auto"

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    hostname = get_remote_hostname(remote_host)
    all_sessions = []

    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_GEMINI, AGENT_CODEX]

    for scan_agent in agents_to_scan:
        if scan_agent == AGENT_CLAUDE:
            all_sessions.extend(
                _get_batch_ssh_claude_sessions(
                    remote_host, patterns, since_date, until_date, hostname
                )
            )
        elif scan_agent == AGENT_GEMINI:
            all_sessions.extend(
                _get_batch_ssh_gemini_sessions(
                    remote_host, patterns, since_date, until_date, hostname
                )
            )
        elif scan_agent == AGENT_CODEX:
            all_sessions.extend(
                _get_batch_ssh_codex_sessions(
                    remote_host, patterns, since_date, until_date, hostname
                )
            )

    return all_sessions


def _get_batch_local_sessions(patterns: list, since_date, until_date, agent: str = "auto") -> list:
    """Get sessions from local filesystem for batch export.

    Args:
        patterns: List of workspace patterns
        since_date: Optional start date filter
        until_date: Optional end date filter
        agent: Agent backend - 'auto', 'claude', or 'codex'

    Returns:
        List of session dictionaries.
    """
    return collect_sessions_with_dedup(
        patterns, since_date, until_date, include_cached=True, agent=agent
    )


def _extract_timestamp_prefix(messages: list) -> Optional[str]:
    """Extract timestamp prefix from first message for filename."""
    if messages and messages[0].get("timestamp"):
        try:
            dt = datetime.fromisoformat(messages[0]["timestamp"].replace("Z", "+00:00"))
            return dt.strftime("%Y%m%d%H%M%S")
        except ValueError:
            pass
    return None


def _get_export_output_path(
    session: dict,
    ts_prefix: Optional[str],
    output_dir: Path,
    flat: bool,
    remote_host: Optional[str],
) -> tuple:
    """Get output path and name for export file.

    Returns:
        Tuple of (output_file, output_name)
    """
    jsonl_file = session["file"]

    if flat:
        output_name = f"{ts_prefix}_{jsonl_file.stem}.md" if ts_prefix else f"{jsonl_file.stem}.md"
        return output_dir / output_name, output_name

    source_tag = get_source_tag(remote_host)
    output_name = (
        f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
        if ts_prefix
        else f"{source_tag}{jsonl_file.stem}.md"
    )
    workspace = session["workspace"]
    # Handle absolute paths (Gemini returns real paths like /home/user/project)
    if workspace.startswith("/") or (len(workspace) > 1 and workspace[1] == ":"):
        # Use last path component for absolute paths
        workspace_name = Path(workspace).name
    else:
        workspace_name = get_workspace_name_from_path(workspace)
    workspace_subdir = output_dir / workspace_name
    workspace_subdir.mkdir(parents=True, exist_ok=True)
    return workspace_subdir / output_name, output_name


def _read_session_messages(jsonl_file: Path, agent_type: str) -> Optional[list]:
    """Read messages from a session file based on agent type.

    Returns:
        List of messages, or None if reading failed.
    """
    try:
        if agent_type == AGENT_GEMINI:
            messages, _ = gemini_read_json_messages(jsonl_file)
        elif agent_type == AGENT_CODEX:
            messages, _ = codex_read_jsonl_messages(jsonl_file)
        else:
            messages = read_jsonl_messages(jsonl_file)
        return messages
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error reading {jsonl_file.name}: {e}\n")
        return None


def _is_export_up_to_date(output_file: Path, jsonl_file: Path, force: bool) -> bool:
    """Check if export output is already up-to-date.

    Returns:
        True if output is up-to-date and can be skipped.
    """
    if force:
        return False
    if not output_file.exists():
        return False
    return output_file.stat().st_mtime >= jsonl_file.stat().st_mtime


def _export_single_session(
    jsonl_file: Path,
    output_file: Path,
    output_name: str,
    agent_type: str,
    messages: list,
    minimal: bool,
    split_lines: Optional[int],
    quiet: bool,
):
    """Export a single session to markdown file(s)."""
    try:
        # Codex/Gemini files don't support splitting yet; use direct export
        if agent_type in (AGENT_CODEX, AGENT_GEMINI):
            _write_single_file(
                jsonl_file,
                output_file,
                minimal,
                messages=messages,
                agent=agent_type,
                quiet=quiet,
            )
        elif split_lines and messages:
            parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)
            if parts:
                _write_split_parts(parts, output_name, output_file, quiet)
            else:
                _write_single_file(jsonl_file, output_file, minimal, messages=messages, quiet=quiet)
        else:
            _write_single_file(jsonl_file, output_file, minimal, messages=messages, quiet=quiet)
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Error converting {jsonl_file.name}: {e}\n")


def _export_sessions_to_markdown(sessions: list, args, output_dir: Path):  # noqa: C901
    """Export sessions to markdown files.

    Args:
        sessions: List of session dictionaries
        args: Parsed arguments (for force, flat, minimal, split, remote)
        output_dir: Output directory path
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    force = getattr(args, "force", False)
    flat = getattr(args, "flat", False)
    remote_host = getattr(args, "remote", None)
    minimal = getattr(args, "minimal", False)
    split_lines = getattr(args, "split", None)
    quiet = getattr(args, "quiet", False)
    jobs = max(1, int(getattr(args, "jobs", 1) or 1))
    total_sessions = len(sessions)
    progress_interval = 0

    if not quiet and total_sessions:
        sys.stderr.write(f"Exporting {total_sessions} sessions...\n")
        if total_sessions >= EXPORT_PROGRESS_MIN:
            progress_interval = max(EXPORT_PROGRESS_MIN, total_sessions // 20)

    def export_one(session: dict) -> None:
        jsonl_file = session["file"]
        agent_type = session.get("agent") or detect_agent_from_path(jsonl_file)

        messages = _read_session_messages(jsonl_file, agent_type)
        if messages is None:
            return

        ts_prefix = _extract_timestamp_prefix(messages)
        output_file, output_name = _get_export_output_path(
            session, ts_prefix, output_dir, flat, remote_host
        )

        if _is_export_up_to_date(output_file, jsonl_file, force):
            return

        _export_single_session(
            jsonl_file,
            output_file,
            output_name,
            agent_type,
            messages,
            minimal,
            split_lines,
            quiet,
        )

    if jobs > 1 and len(sessions) > 1:
        completed = 0
        lock = threading.Lock()

        def export_one_with_progress(session: dict) -> None:
            export_one(session)
            nonlocal completed
            if quiet:
                return
            with lock:
                completed += 1
                if completed == total_sessions or (
                    progress_interval and completed % progress_interval == 0
                ):
                    sys.stderr.write(f"Processed {completed}/{total_sessions} sessions\n")

        with ThreadPoolExecutor(max_workers=jobs) as executor:
            futures = [executor.submit(export_one_with_progress, session) for session in sessions]
            for future in futures:
                future.result()
    else:
        completed = 0
        for session in sessions:
            export_one(session)
            if quiet:
                continue
            completed += 1
            if completed == total_sessions or (
                progress_interval and completed % progress_interval == 0
            ):
                sys.stderr.write(f"Processed {completed}/{total_sessions} sessions\n")


def _write_split_parts(parts: list, output_name: str, output_file: Path, quiet: bool):
    """Write split conversation parts with navigation links."""
    base_name = output_name.rsplit(".md", 1)[0]
    total_parts = len(parts)

    for part_num, _, part_md, _, _ in parts:
        part_filename = f"{base_name}_part{part_num}.md"
        part_file = output_file.parent / part_filename

        # Build navigation footer
        nav_parts = [f"**Part {part_num} of {total_parts}**"]
        if part_num > 1:
            nav_parts.append(f"[â† Part {part_num - 1}]({base_name}_part{part_num - 1}.md)")
        if part_num < total_parts:
            nav_parts.append(f"[Part {part_num + 1} â†’]({base_name}_part{part_num + 1}.md)")

        part_file.write_text(part_md + "\n---\n\n> " + " | ".join(nav_parts), encoding="utf-8")
        if not quiet:
            print(part_file)


def _write_single_file(
    jsonl_file: Path,
    output_file: Path,
    minimal: bool,
    messages: Optional[list] = None,
    agent: str = AGENT_CLAUDE,
    quiet: bool = False,
):
    """Write a single markdown file.

    Args:
        jsonl_file: Source JSONL/JSON file path
        output_file: Destination markdown file path
        minimal: If True, produce minimal output without metadata
        messages: Pre-read messages (optional, avoids re-reading file)
        agent: Agent type (claude, codex, or gemini)
    """
    if agent == AGENT_GEMINI:
        markdown = gemini_parse_json_to_markdown(jsonl_file, minimal=minimal)
    elif agent == AGENT_CODEX:
        markdown = codex_parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
    else:
        markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal, messages=messages)
    output_file.write_text(markdown, encoding="utf-8")
    if not quiet:
        print(output_file)


def _get_batch_sessions(args, patterns: list, since_date, until_date) -> list:
    """Get batch sessions based on source type."""
    remote_host = getattr(args, "remote", None)
    agent = getattr(args, "agent", None) or "auto"
    if not remote_host:
        return _get_batch_local_sessions(patterns, since_date, until_date, agent=agent)
    if is_windows_remote(remote_host):
        return _get_batch_windows_sessions(args, patterns, since_date, until_date)
    if is_wsl_remote(remote_host):
        return _get_batch_wsl_sessions(args, patterns, since_date, until_date)
    return _get_batch_ssh_sessions(args, patterns, since_date, until_date)


def _handle_empty_sessions(patterns: list, since_date, until_date, lenient: bool) -> bool:
    """Handle case when no sessions found.

    Returns:
        True if should continue (lenient mode), False if error (caller should exit).
    """
    if lenient:
        return True

    pattern_str = (
        ", ".join(patterns) if patterns and len(patterns) > 1 else (patterns[0] if patterns else "")
    )
    if since_date or until_date:
        date_range = []
        if since_date:
            date_range.append(f"from {since_date.strftime('%Y-%m-%d')}")
        if until_date:
            date_range.append(f"until {until_date.strftime('%Y-%m-%d')}")
        sys.stderr.write(f"No sessions found {' '.join(date_range)}\n")
    else:
        sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
    return False


def cmd_batch(args) -> bool:
    """Batch convert all sessions from a workspace.

    Returns:
        True on success, False if no sessions found (and not lenient).
    """
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)
    since_date, until_date = parse_and_validate_dates(since_str, until_str)

    patterns = get_patterns_from_args(args)
    sessions = _get_batch_sessions(args, patterns, since_date, until_date)

    if not sessions:
        return _handle_empty_sessions(
            patterns, since_date, until_date, getattr(args, "lenient", False)
        )

    _export_sessions_to_markdown(sessions, args, Path(args.output_dir))
    return True


def cmd_version(args):
    """Show version information."""
    print(f"agent-history {__version__}")


def cmd_list_wsl(args):
    """List available WSL distributions with agent workspaces."""
    distributions = get_wsl_distributions()

    if not distributions:
        sys.stderr.write("No WSL distributions found or WSL is not available.\n")
        sys.stderr.write("Make sure WSL is installed on Windows.\n")
        sys.exit(1)

    # Filter to only distributions with agent data
    active_distros = _filter_wsl_distros_for_agent(distributions, "auto")

    if not active_distros:
        sys.stderr.write("No WSL distributions with agent data found.\n")
        sys.exit(1)

    # Print tab-separated output
    print("DISTRO\tUSERNAME\tAGENTS\tPATH")
    for distro in active_distros:
        agents = []
        if distro.get("has_claude"):
            agents.append(AGENT_CLAUDE)
        if distro.get("has_codex"):
            agents.append(AGENT_CODEX)
        if distro.get("has_gemini"):
            agents.append(AGENT_GEMINI)
        agents_label = ",".join(agents) if agents else "none"
        path_hint = (
            distro.get("path") or distro.get("codex_path") or distro.get("gemini_path") or ""
        )
        print(f"{distro['name']}\t{distro['username']}\t{agents_label}\t{path_hint}")


def cmd_list_windows(args):
    """List available Windows users with Claude Code workspaces (from WSL)."""
    if not is_running_in_wsl():
        sys.stderr.write("Error: --list-windows is only available from WSL.\n")
        sys.stderr.write("To access Windows sessions from Windows, use the tool directly.\n")
        sys.exit(1)

    users = get_windows_users_with_claude()

    if not users:
        sys.stderr.write("No Windows users with Claude Code workspaces found.\n")
        sys.stderr.write("Make sure Claude Code is installed on Windows.\n")
        sys.exit(1)

    # Print tab-separated output
    print("USERNAME\tDRIVE\tWORKSPACES\tPATH")
    for user in users:
        print(f"{user['username']}\t{user['drive']}\t{user['workspace_count']}\t{user['path']}")


def _collect_local_sessions(patterns, since_date, until_date, in_wsl, agent: str = "auto"):
    """Collect sessions from local environment."""
    results = []
    try:
        local_label = "Local (WSL)" if in_wsl else "Local"
        native_sessions = collect_sessions_with_dedup(
            patterns,
            since_date=since_date,
            until_date=until_date,
            include_cached=False,
            agent=agent,
        )
        if native_sessions:
            results.append((local_label, native_sessions))
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")
    return results


def _collect_windows_sessions_from_wsl(
    patterns, since_date, until_date, agent: str = "auto", workspaces_only: bool = False
):
    """Collect sessions from Windows when running in WSL."""
    results = []
    try:
        windows_users = get_windows_users_with_claude()
        agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]
        for user in windows_users:
            try:
                all_sessions = []
                for scan_agent in agents_to_scan:
                    sessions_dir = get_agent_windows_dir(user["username"], scan_agent)
                    if not sessions_dir or not sessions_dir.exists():
                        continue
                    if scan_agent == AGENT_CLAUDE:
                        sessions = collect_sessions_with_dedup(
                            patterns,
                            since_date=since_date,
                            until_date=until_date,
                            projects_dir=sessions_dir,
                            include_cached=False,
                            skip_message_count=workspaces_only,
                            agent=scan_agent,
                        )
                    else:
                        sessions = _scan_codex_gemini_sessions(
                            scan_agent,
                            patterns,
                            since_date,
                            until_date,
                            sessions_dir,
                            "windows",
                            skip_message_count=workspaces_only,
                        )
                    all_sessions.extend(sessions)
                if all_sessions:
                    results.append((f"Windows ({user['username']})", all_sessions))
            except (OSError, PermissionError) as e:
                sys.stderr.write(f"Error accessing Windows ({user['username']}): {e}\n")
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error listing Windows users: {e}\n")
    return results


def _collect_wsl_sessions_from_windows(
    patterns,
    since_date,
    until_date,
    agent: str = "auto",
    workspaces_only: bool = False,
    wsl_counts: bool = False,
):
    """Collect sessions from WSL when running on Windows."""
    results = []
    try:
        wsl_distros = get_wsl_distributions()
        filtered_distros = _filter_wsl_distros_for_agent(wsl_distros, agent)
        agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]
        unknown_agent = agent not in ("auto", AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI)
        skip_counts = workspaces_only or not wsl_counts
        for distro in filtered_distros:
            try:
                all_sessions = []
                for scan_agent in agents_to_scan:
                    if scan_agent == AGENT_CLAUDE or unknown_agent:
                        wsl_projects_dir = get_wsl_projects_dir(distro["name"])
                        if not wsl_projects_dir:
                            continue
                        sessions = collect_sessions_with_dedup(
                            patterns,
                            since_date=since_date,
                            until_date=until_date,
                            projects_dir=wsl_projects_dir,
                            include_cached=False,
                            skip_message_count=skip_counts,
                            use_cached_counts=wsl_counts,
                            agent=scan_agent,
                        )
                    else:
                        sessions_dir = get_agent_wsl_dir(distro["name"], scan_agent)
                        if not sessions_dir or not sessions_dir.exists():
                            continue
                        sessions = _scan_codex_gemini_sessions(
                            scan_agent,
                            patterns,
                            since_date,
                            until_date,
                            sessions_dir,
                            f"wsl:{distro['name']}",
                            skip_message_count=skip_counts,
                            use_cached_counts=wsl_counts,
                        )
                    all_sessions.extend(sessions)
                if all_sessions:
                    results.append((f"WSL ({distro['name']})", all_sessions))
            except (OSError, PermissionError) as e:
                sys.stderr.write(f"Error accessing WSL ({distro['name']}): {e}\n")
    except (OSError, subprocess.SubprocessError) as e:
        sys.stderr.write(f"Error listing WSL distributions: {e}\n")
    return results


def _filter_workspaces_by_patterns(workspaces, patterns):
    """Filter workspace list by patterns (any pattern matches)."""
    if not patterns or patterns == [""]:
        return workspaces
    filtered = []
    for ws in workspaces:
        for pattern in patterns:
            if pattern in ws:
                filtered.append(ws)
                break
    return filtered


def _session_in_date_range(session: dict, since_date, until_date) -> bool:
    """Check if session falls within date range."""
    return is_date_in_range(session.get("modified"), since_date, until_date)


def _enrich_and_filter_sessions(ws_info, ws_name, since_date, until_date) -> list:
    """Filter sessions by date and add workspace info."""
    sessions = []
    for session in ws_info:
        if not _session_in_date_range(session, since_date, until_date):
            continue
        session["workspace"] = ws_name
        session["workspace_readable"] = normalize_workspace_name(ws_name)
        sessions.append(session)
    return sessions


def _collect_remote_claude_sessions_simple(remote, patterns, since_date, until_date):
    """Collect Claude sessions from a single SSH remote (no caching)."""
    all_remote_workspaces = list_remote_workspaces(remote)
    remote_workspaces = _filter_workspaces_by_patterns(all_remote_workspaces, patterns)

    if not remote_workspaces:
        return []

    sessions = []
    for ws_name in remote_workspaces:
        ws_info = get_remote_session_info(remote, ws_name)
        for session in ws_info:
            if not _session_in_date_range(session, since_date, until_date):
                continue
            session["workspace"] = ws_name
            session["workspace_readable"] = normalize_workspace_name(ws_name)
            session["agent"] = AGENT_CLAUDE
            sessions.append(session)
    return sessions


def _collect_remote_gemini_sessions_simple(remote, patterns, since_date, until_date):
    """Collect Gemini sessions from a single SSH remote (no caching)."""
    sessions_info = gemini_get_remote_session_info(remote)
    if not sessions_info:
        return []

    # Try to resolve hash names using remote hash index
    remote_index = gemini_fetch_remote_hash_index(remote, get_config_dir())

    sessions = []
    for session_info in sessions_info:
        ws_hash = session_info.get("workspace", "unknown")
        resolved_path = remote_index.get(ws_hash, ws_hash)

        # Apply pattern filter
        if not (
            matches_any_pattern(resolved_path, patterns) or matches_any_pattern(ws_hash, patterns)
        ):
            continue

        # Apply date filter
        if not _session_in_date_range(session_info, since_date, until_date):
            continue

        session_info["workspace"] = ws_hash
        session_info["workspace_readable"] = resolved_path
        session_info["agent"] = AGENT_GEMINI
        sessions.append(session_info)

    return sessions


def _collect_remote_codex_sessions_simple(remote, patterns, since_date, until_date):
    """Collect Codex sessions from a single SSH remote (no caching)."""
    sessions_info = codex_get_remote_session_info(remote)
    if not sessions_info:
        return []

    sessions = []
    for session_info in sessions_info:
        ws_short = session_info.get("workspace", "unknown")
        ws_full = session_info.get("workspace_full", ws_short)

        # Apply pattern filter
        if not (matches_any_pattern(ws_full, patterns) or matches_any_pattern(ws_short, patterns)):
            continue

        # Apply date filter
        if not _session_in_date_range(session_info, since_date, until_date):
            continue

        session_info["workspace_readable"] = ws_full
        session_info["agent"] = AGENT_CODEX
        sessions.append(session_info)

    return sessions


def _collect_remote_sessions(remote, patterns, since_date, until_date, agent: str = "auto"):
    """Collect sessions from a single SSH remote."""
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Cannot connect to remote: {remote}\n")
        return None

    hostname = get_remote_hostname(remote)
    all_sessions = []

    agents_to_scan = [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_GEMINI, AGENT_CODEX]

    for scan_agent in agents_to_scan:
        if scan_agent == AGENT_CLAUDE:
            all_sessions.extend(
                _collect_remote_claude_sessions_simple(remote, patterns, since_date, until_date)
            )
        elif scan_agent == AGENT_GEMINI:
            all_sessions.extend(
                _collect_remote_gemini_sessions_simple(remote, patterns, since_date, until_date)
            )
        elif scan_agent == AGENT_CODEX:
            all_sessions.extend(
                _collect_remote_codex_sessions_simple(remote, patterns, since_date, until_date)
            )

    if all_sessions:
        return (f"Remote ({hostname})", all_sessions)
    return None


def _collect_all_remote_sessions(remotes, patterns, since_date, until_date, agent: str = "auto"):
    """Collect sessions from all SSH remotes."""
    results = []
    for remote in remotes:
        try:
            result = _collect_remote_sessions(remote, patterns, since_date, until_date, agent)
            if result:
                results.append(result)
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")
    return results


def _print_all_homes_results(all_results, workspaces_only):
    """Print results from all homes listing."""
    if not all_results:
        msg = "No workspaces found.\n" if workspaces_only else "No sessions found.\n"
        sys.stderr.write(msg)
        return

    if workspaces_only:
        for source_label, sessions in all_results:
            print(f"# {source_label}")
            workspaces = sorted({s["workspace_readable"] for s in sessions})
            for ws in workspaces:
                print(f"  {ws}")
            print()
    else:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for source_label, sessions in all_results:
            for session in sessions:
                print(format_session_line(session, source_label))


def cmd_list_all_homes(args):
    """List workspaces or sessions from all homes: local, WSL/Windows, and remotes."""
    in_wsl = is_running_in_wsl()
    workspaces_only = getattr(args, "workspaces_only", False)
    agent = getattr(args, "agent", None) or "auto"
    no_wsl = getattr(args, "no_wsl", False)
    no_windows = getattr(args, "no_windows", False)
    wsl_counts = getattr(args, "counts", False) or getattr(args, "wsl_counts", False)

    patterns = getattr(args, "patterns", None)
    if patterns is None:
        patterns = [getattr(args, "workspace", "")]

    since_date = getattr(args, "since_date", None)
    until_date = getattr(args, "until_date", None)
    remotes = getattr(args, "remotes", []) or []

    saved_sources = get_saved_sources()
    for src in saved_sources:
        if src not in remotes:
            remotes.append(src)

    all_results = []

    # Collect from all sources
    all_results.extend(_collect_local_sessions(patterns, since_date, until_date, in_wsl, agent))

    if in_wsl:
        if not no_windows:
            all_results.extend(
                _collect_windows_sessions_from_wsl(
                    patterns, since_date, until_date, agent, workspaces_only=workspaces_only
                )
            )
    elif not no_wsl:
        all_results.extend(
            _collect_wsl_sessions_from_windows(
                patterns,
                since_date,
                until_date,
                agent,
                workspaces_only=workspaces_only,
                wsl_counts=wsl_counts,
            )
        )

    all_results.extend(
        _collect_all_remote_sessions(remotes, patterns, since_date, until_date, agent)
    )

    _print_all_homes_results(all_results, workspaces_only)


def _lsh_show_local():
    """Show local home folder."""
    try:
        projects_dir = get_claude_projects_dir()
        if projects_dir.exists():
            workspace_count = len(
                [d for d in projects_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]
            )
            print("Local:")
            print(f"  {projects_dir.parent}\t{workspace_count} workspaces")
            print()
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")


def _lsh_show_wsl():
    """Show WSL distributions (when on Windows)."""
    print("WSL Distributions:")
    try:
        wsl_distros = _filter_wsl_distros_for_agent(get_wsl_distributions(), "auto")

        if wsl_distros:
            for distro in wsl_distros:
                agents = []
                if distro.get("has_claude"):
                    agents.append(AGENT_CLAUDE)
                if distro.get("has_codex"):
                    agents.append(AGENT_CODEX)
                if distro.get("has_gemini"):
                    agents.append(AGENT_GEMINI)
                agents_label = ",".join(agents) if agents else "none"

                workspace_count = "n/a"
                projects_dir = None
                if distro.get("has_claude"):
                    projects_dir = get_wsl_projects_dir(distro["name"])
                    if projects_dir:
                        workspace_count = len(
                            [
                                d
                                for d in projects_dir.iterdir()
                                if d.is_dir() and not d.name.startswith(".")
                            ]
                        )
                path_hint = (
                    str(projects_dir)
                    if projects_dir
                    else (distro.get("codex_path") or distro.get("gemini_path") or "")
                )
                print(
                    f"  {distro['name']}\t{distro['username']}\t{agents_label}\t"
                    f"{workspace_count} workspaces\t{path_hint}"
                )
        else:
            print("  No WSL distributions with agent data found")
        print()
    except (OSError, subprocess.SubprocessError) as e:
        sys.stderr.write(f"Error accessing WSL: {e}\n")


def _lsh_show_windows():
    """Show Windows users (when on WSL)."""
    print("Windows Users:")
    try:
        windows_users = get_windows_users_with_claude()

        if windows_users:
            for user in windows_users:
                print(f"  {user['username']}\t{user['path']}\t{user['workspace_count']} workspaces")
        else:
            print("  No Windows users with Claude Code found")
        print()
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing Windows: {e}\n")


def _lsh_show_remotes():
    """Show configured SSH remotes."""
    sources = get_saved_sources()
    print("SSH Remotes:")
    if sources:
        for source in sources:
            print(f"  {source}")
    else:
        print("  (none configured)")
    print()


def _get_lsh_display_flags(args) -> tuple:
    """Get flags for what to display in lsh command.

    Returns:
        Tuple of (show_local, show_wsl, show_windows, show_remotes)
    """
    no_filters = not args.wsl and not args.windows and not getattr(args, "remotes", False)
    show_local = args.local or (no_filters and not args.local)
    show_remotes = getattr(args, "remotes", False) or (
        not args.local and not args.wsl and not args.windows
    )
    return show_local, args.wsl, args.windows, show_remotes


def cmd_lsh(args):
    """List all home folders with Claude Code installations."""
    # Check for subcommands first
    lsh_action = getattr(args, "lsh_action", None)

    if lsh_action == "add":
        cmd_lsh_add(args)
        return
    if lsh_action == "remove":
        cmd_lsh_remove(args)
        return
    if lsh_action == "clear":
        cmd_lsh_clear(args)
        return

    # Default: list all hosts
    in_wsl = is_running_in_wsl()
    show_local, show_wsl, show_windows, show_remotes = _get_lsh_display_flags(args)

    if show_local:
        _lsh_show_local()
    if show_wsl:
        _lsh_show_wsl()
    if show_windows and in_wsl:
        _lsh_show_windows()
    if show_remotes:
        _lsh_show_remotes()


def cmd_lsh_add(args):
    """Add a remote source to configuration."""
    source = args.source

    # WSL and Windows are auto-detected, no need to save them
    if source.startswith("wsl://") or source == "windows":
        sys.stderr.write(f"Note: {source} is auto-detected by --ah flag, no need to add it.\n")
        sys.stderr.write("This command is for SSH remotes (user@hostname).\n")
        return

    # Validate source format (SSH remote)
    if "@" not in source:
        sys.stderr.write(f"Error: Invalid source format: {source}\n")
        sys.stderr.write("Expected: user@hostname (e.g., alice@server.example.com)\n")
        sys.exit(1)

    config = load_config()
    sources = config.get("sources", [])

    if source in sources:
        print(f"Source '{source}' already configured.")
        return

    sources.append(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Added source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_remove(args):
    """Remove a remote source from configuration."""
    source = args.source

    config = load_config()
    sources = config.get("sources", [])

    if source not in sources:
        sys.stderr.write(f"Error: Source '{source}' not found.\n")
        sys.stderr.write(f"Configured sources: {', '.join(sources) if sources else '(none)'}\n")
        sys.exit(1)

    sources.remove(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Removed source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_clear(args):
    """Clear all saved remote sources."""
    config = load_config()
    config["sources"] = []

    if save_config(config):
        print("Cleared all SSH remotes.")
    else:
        sys.exit(1)


def _get_reset_items(what: Optional[str], config_dir: Path) -> list:
    """Get list of (description, path) tuples for reset operation."""
    items = []
    reset_all = what in (None, "all")

    if reset_all or what == "db":
        db_path = get_metrics_db_path()
        if db_path.exists():
            items.append(("Metrics database", db_path))
    if reset_all or what == "settings":
        config_file = config_dir / "config.json"
        if config_file.exists():
            items.append(("Settings (SSH remotes)", config_file))
    if reset_all or what == "aliases":
        aliases_file = config_dir / "aliases.json"
        if aliases_file.exists():
            items.append(("Aliases", aliases_file))

    return items


def _confirm_reset(items: list, yes: bool) -> bool:
    """Confirm reset operation. Returns True if confirmed."""
    print("This will delete:")
    for desc, path in items:
        print(f"  - {desc}: {path}")

    if yes:
        return True

    try:
        response = input("\nProceed? [y/N] ").strip().lower()
        return response in ("y", "yes")
    except (EOFError, KeyboardInterrupt):
        print("\nCancelled.")
        return False


def cmd_reset(args):
    """Reset agent-history data (database, settings, aliases)."""
    items = _get_reset_items(getattr(args, "what", None), get_config_dir())

    if not items:
        print("Nothing to reset.")
        return

    if not _confirm_reset(items, getattr(args, "yes", False)):
        print("Cancelled.")
        return

    print()
    for desc, path in items:
        path.unlink()
        print(f"Deleted {desc.lower()}")


def _gemini_index_list(full_hash: bool):
    """List all mappings in the Gemini hash index."""
    index = gemini_load_hash_index()
    mappings = index.get("hashes", {})

    if not mappings:
        print("Hash index is empty. Use 'gemini-index <path>' to add mappings.")
        return

    print(f"Hash Index Mappings ({len(mappings)} entries):\n")
    for project_hash, path in sorted(mappings.items(), key=lambda x: x[1]):
        if full_hash:
            print(f"  {project_hash}")
            print(f"    â†’ {path}")
        else:
            print(f"  [hash:{project_hash[:HASH_DISPLAY_LEN]}] â†’ {path}")


def _gemini_index_add(paths: list):
    """Add paths to the Gemini hash index."""
    # Convert to Path objects (allow non-existent paths - hash may still have sessions)
    path_objects = []
    for p in paths:
        path = Path(p).expanduser().resolve()
        path_objects.append(path)

    if not path_objects:
        sys.stderr.write("Error: No paths provided\n")
        sys.exit(1)

    print(f"Adding {len(path_objects)} path(s) to Gemini index...\n")

    result = gemini_add_paths_to_index(path_objects)

    # Report results for each path
    for mapping in result["mappings"]:
        status = mapping["status"]
        path = mapping["path"]
        hash_short = mapping["hash"]
        path_missing = mapping.get("path_missing", False)
        missing_note = " [path doesn't exist]" if path_missing else ""

        if status == "added":
            print(f"  + {path}{missing_note}")
            print(f"    hash:{hash_short} (added)")
        elif status == "existing":
            print(f"  = {path}{missing_note}")
            print(f"    hash:{hash_short} (existing)")
        elif status == "no_sessions":
            print(f"  - {path}{missing_note}")
            print(f"    hash:{hash_short} (no sessions)")

    # Summary
    print(
        f"\nSummary: {result['added']} added, {result['existing']} existing, "
        f"{result['no_sessions']} skipped"
    )

    # Show current index stats
    index = gemini_load_hash_index()
    print(f"Total mappings in index: {len(index['hashes'])}")


def cmd_gemini_index(args):
    """Manage Gemini hashâ†’path index: list mappings or add paths."""
    add_paths = getattr(args, "add_paths", None)

    # If --add was specified (even with no paths), add paths
    if add_paths is not None:
        # Default to current directory if --add given with no paths
        paths = add_paths if add_paths else ["."]
        _gemini_index_add(paths)
    else:
        # Default behavior: list the index
        _gemini_index_list(getattr(args, "full_hash", False))


def _filter_fetch_workspaces(workspaces: list, pattern: str) -> list:
    """Filter workspaces by pattern for fetch command."""
    if not pattern or pattern in ("", "*", "all"):
        return workspaces
    filtered = [ws for ws in workspaces if pattern in ws]
    if not filtered:
        sys.stderr.write(f"Error: No workspaces matching pattern '{pattern}'\n")
        sys.exit(1)
    return filtered


def _fetch_and_report(remote_host: str, workspaces: list, local_projects_dir: Path, hostname: str):
    """Fetch each workspace and report results."""
    for remote_workspace in workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        workspace_path = remote_workspace.lstrip("-")
        local_name = f"remote_{hostname}_{workspace_path}"

        result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)
        if result["success"]:
            if result.get("warning"):
                sys.stderr.write(f"Warning: {readable_name}: {result['warning']}\n")
            print(normalize_workspace_name(local_name, verify_local=False))
        else:
            error = result.get("error", "Unknown error")
            sys.stderr.write(f"Error fetching {readable_name}: {error}\n")


def cmd_fetch(args):
    """Fetch remote conversation sessions via SSH."""
    remote_host = args.remote_host
    workspace_pattern = getattr(args, "pattern", None) or ""

    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    hostname = get_remote_hostname(remote_host)
    remote_workspaces = list_remote_workspaces(remote_host)

    if not remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    remote_workspaces = _filter_fetch_workspaces(remote_workspaces, workspace_pattern)
    local_projects_dir = get_claude_projects_dir()
    _fetch_and_report(remote_host, remote_workspaces, local_projects_dir, hostname)


def _validate_local_access() -> list:
    """Validate local Claude projects access. Returns list of errors."""
    try:
        projects_dir = get_claude_projects_dir()
        if not projects_dir.exists():
            return [f"Local: Claude projects directory not found: {projects_dir}"]
    except (OSError, PermissionError) as e:
        return [f"Local: {e}"]
    return []


def _validate_windows_from_wsl() -> list:
    """Validate Windows access from WSL. Returns list of errors."""
    errors = []
    try:
        for user in get_windows_users_with_claude():
            try:
                projects_dir = get_windows_projects_dir(user["username"])
                if not projects_dir or not projects_dir.exists():
                    errors.append(
                        f"Windows ({user['username']}): Projects directory not accessible"
                    )
            except (OSError, PermissionError) as e:
                errors.append(f"Windows ({user['username']}): {e}")
    except (OSError, PermissionError) as e:
        errors.append(f"Windows: {e}")
    return errors


def _validate_wsl_from_windows(agent: Optional[str] = None) -> list:
    """Validate WSL access from Windows. Returns list of errors."""
    errors = []
    try:
        wsl_distros = _filter_wsl_distros_for_agent(get_wsl_distributions(), agent or "auto")
        for distro in wsl_distros:
            try:
                if agent in (None, "auto", AGENT_CLAUDE) and distro.get("has_claude"):
                    projects_dir = get_wsl_projects_dir(distro["name"])
                    if not projects_dir or not projects_dir.exists():
                        errors.append(f"WSL ({distro['name']}): Claude projects not accessible")
                if agent in (None, "auto", AGENT_CODEX) and distro.get("has_codex"):
                    codex_dir = get_agent_wsl_dir(distro["name"], AGENT_CODEX)
                    if not codex_dir or not codex_dir.exists():
                        errors.append(f"WSL ({distro['name']}): Codex sessions not accessible")
                if agent in (None, "auto", AGENT_GEMINI) and distro.get("has_gemini"):
                    gemini_dir = get_agent_wsl_dir(distro["name"], AGENT_GEMINI)
                    if not gemini_dir or not gemini_dir.exists():
                        errors.append(f"WSL ({distro['name']}): Gemini sessions not accessible")
            except (OSError, PermissionError) as e:
                errors.append(f"WSL ({distro['name']}): {e}")
    except (OSError, subprocess.SubprocessError) as e:
        errors.append(f"WSL: {e}")
    return errors


def _validate_ssh_remotes(args) -> list:
    """Validate SSH remote access. Returns list of errors."""
    if not hasattr(args, "remotes") or not args.remotes:
        return []
    errors = []
    for remote_host in args.remotes:
        if is_windows_remote(remote_host):
            continue
        if not check_ssh_connection(remote_host):
            errors.append(f"Remote ({remote_host}): Cannot connect via passwordless SSH")
    return errors


def validate_export_all_homes(args, in_wsl):
    """
    Validate all homes before starting export.
    Returns (success, errors) where errors is a list of error messages.
    """
    errors = _validate_local_access()

    no_wsl = getattr(args, "no_wsl", False)
    no_windows = getattr(args, "no_windows", False)
    no_remote = getattr(args, "no_remote", False)

    if in_wsl:
        if not no_windows:
            errors.extend(_validate_windows_from_wsl())
    elif not no_wsl:
        errors.extend(_validate_wsl_from_windows(getattr(args, "agent", None) or "auto"))

    if not no_remote:
        errors.extend(_validate_ssh_remotes(args))

    return (len(errors) == 0, errors)


def _export_local_sessions(args, output_dir: Path, patterns: list, local_label: str) -> dict:
    """Export local sessions and return result stats."""
    result = {"sessions": 0, "errors": 0, "source": None}
    print(f"[Local] Exporting from {local_label}...")
    try:
        config = ExportConfig.from_args(
            args,
            output_dir=str(output_dir),
            patterns=patterns,
            flat=False,  # Always organized for export-all
            remote=None,  # Local
            lenient=True,  # Don't fail on empty - export-all handles this
        )

        # Get local sessions count before export
        agent = getattr(args, "agent", None) or "auto"
        local_sessions = collect_sessions_with_dedup(patterns, include_cached=False, agent=agent)
        if local_sessions:
            cmd_batch(config)
            result["source"] = {"source": local_label, "sessions": len(local_sessions)}
            result["sessions"] = len(local_sessions)
            print(f"[OK] Local: {len(local_sessions)} sessions exported\n")
        else:
            print("[OK] Local: No matching sessions\n")
    except (OSError, json.JSONDecodeError) as e:
        print(f"[ERROR] Local export failed: {e}\n")
        result["errors"] = 1
    return result


def _export_windows_from_wsl(args, output_dir: Path, patterns: list) -> dict:
    """Export from Windows users when running in WSL."""
    result: dict[str, Any] = {"sessions": 0, "errors": 0, "sources": []}
    print("[Windows] Checking Windows users with Claude...")
    try:
        windows_users = get_windows_users_with_claude()

        if windows_users:
            for user in windows_users:
                print(f"  Exporting from Windows: {user['username']}...")
                try:
                    username = user["username"]

                    # Count sessions first
                    windows_projects_dir = get_windows_projects_dir(username)
                    agent = getattr(args, "agent", None) or "auto"
                    native_sessions = collect_sessions_with_dedup(
                        patterns,
                        projects_dir=windows_projects_dir,
                        include_cached=False,
                        agent=agent,
                    )
                    session_count = len(native_sessions)

                    if session_count > 0:
                        config = ExportConfig.from_args(
                            args,
                            output_dir=str(output_dir),
                            patterns=patterns,
                            flat=False,  # Always organized
                            remote=f"windows://{username}",
                            lenient=True,
                        )
                        cmd_batch(config)
                        result["sources"].append(
                            {"source": f"Windows: {username}", "sessions": session_count}
                        )
                        result["sessions"] += session_count
                        print(f"  [OK] {username}: {session_count} sessions exported")
                    else:
                        print(f"  [OK] {username}: No matching sessions")
                except (OSError, json.JSONDecodeError) as e:
                    print(f"  [ERROR] {username} failed: {e}")
                    result["errors"] += 1
            print()
        else:
            print("  No Windows users with Claude Code found\n")
    except (OSError, PermissionError) as e:
        print(f"[ERROR] Windows export failed: {e}\n")
    return result


def _export_wsl_from_windows(args, output_dir: Path, patterns: list) -> dict:
    """Export from WSL distributions when running on Windows."""
    result: dict[str, Any] = {"sessions": 0, "errors": 0, "sources": []}
    print("[WSL] Checking WSL distributions...")
    try:
        wsl_distros = get_wsl_distributions()
        agent = getattr(args, "agent", None) or "auto"
        filtered_distros = _filter_wsl_distros_for_agent(wsl_distros, agent)

        if filtered_distros:
            for distro in filtered_distros:
                print(f"  Exporting from WSL: {distro['name']}...")
                try:
                    distro_name = distro["name"]
                    agents_to_scan = (
                        [agent] if agent != "auto" else [AGENT_CLAUDE, AGENT_CODEX, AGENT_GEMINI]
                    )

                    wsl_sessions = []
                    for scan_agent in agents_to_scan:
                        if scan_agent == AGENT_CLAUDE:
                            wsl_projects_dir = get_wsl_projects_dir(distro_name)
                            if not wsl_projects_dir:
                                continue
                            wsl_sessions.extend(
                                collect_sessions_with_dedup(
                                    patterns,
                                    projects_dir=wsl_projects_dir,
                                    include_cached=False,
                                    skip_message_count=True,
                                    agent=AGENT_CLAUDE,
                                )
                            )
                        else:
                            sessions_dir = get_agent_wsl_dir(distro_name, scan_agent)
                            if not sessions_dir or not sessions_dir.exists():
                                continue
                            wsl_sessions.extend(
                                _scan_codex_gemini_sessions(
                                    scan_agent,
                                    patterns,
                                    None,
                                    None,
                                    sessions_dir,
                                    f"wsl:{distro_name}",
                                )
                            )

                    session_count = len(wsl_sessions)
                    if session_count > 0:
                        config = ExportConfig.from_args(
                            args,
                            output_dir=str(output_dir),
                            patterns=patterns,
                            flat=False,  # Always organized
                            remote=f"wsl://{distro_name}",
                            lenient=True,
                        )
                        cmd_batch(config)
                        result["sources"].append(
                            {"source": f"WSL: {distro['name']}", "sessions": session_count}
                        )
                        result["sessions"] += session_count
                        print(f"  [OK] {distro['name']}: {session_count} sessions exported")
                    else:
                        print(f"  [OK] {distro['name']}: No matching sessions")
                except (OSError, json.JSONDecodeError) as e:
                    print(f"  [ERROR] {distro['name']} failed: {e}")
                    result["errors"] += 1
            print()
        else:
            print("  No WSL distributions with matching agent data found\n")
    except (OSError, subprocess.SubprocessError) as e:
        print(f"[ERROR] WSL export failed: {e}\n")
    return result


def _export_remote_host(args, output_dir: Path, patterns: list, remote_host: str) -> dict:
    """Export from a single remote host."""
    result = {"sessions": 0, "errors": 0, "source": None}
    print(f"  Exporting from {remote_host}...")
    try:
        config = ExportConfig.from_args(
            args,
            output_dir=str(output_dir),
            patterns=patterns,
            flat=False,
            remote=remote_host,
            lenient=True,
        )

        # Export - with lenient=True, cmd_batch returns True even if no sessions
        cmd_batch(config)

        # Count sessions (cached)
        hostname = get_remote_hostname(remote_host)
        cached_pattern = f"remote_{hostname}_"
        # Filter by workspace patterns if specified
        cached_sessions = get_workspace_sessions(cached_pattern, quiet=True)
        if patterns:
            # Further filter by patterns
            filtered = []
            for s in cached_sessions:
                for p in patterns:
                    if p in s["workspace"] or p in s.get("workspace_readable", ""):
                        filtered.append(s)
                        break
            session_count = len(filtered)
        else:
            session_count = len(cached_sessions)

        if session_count > 0:
            result["source"] = {"source": f"Remote: {remote_host}", "sessions": session_count}
            result["sessions"] = session_count
            print(f"  [OK] {remote_host}: {session_count} sessions exported")
        else:
            print(f"  [OK] {remote_host}: No matching sessions")
    except (OSError, subprocess.SubprocessError, json.JSONDecodeError) as e:
        print(f"  [ERROR] {remote_host} failed: {e}")
        result["errors"] = 1
    return result


def _export_all_remotes(args, output_dir: Path, patterns: list) -> dict:
    """Export from all remote hosts."""
    result: dict[str, Any] = {"sessions": 0, "errors": 0, "sources": []}
    if not hasattr(args, "remotes") or not args.remotes:
        return result

    print("[Remote] Exporting from remote hosts...")
    for remote_host in args.remotes:
        r = _export_remote_host(args, output_dir, patterns, remote_host)
        result["sessions"] += r["sessions"]
        result["errors"] += r["errors"]
        if r["source"]:
            result["sources"].append(r["source"])
    print()
    return result


def _print_export_all_summary(
    sources_processed: list, total_sessions: int, total_errors: int, output_dir: Path
):
    """Print summary of export-all operation."""
    print("=" * 80)
    print("Export-All Summary")
    print("=" * 80)
    for source_info in sources_processed:
        print(f"{source_info['source']:.<40} {source_info['sessions']:>5} sessions")
    print("-" * 80)
    print(f"Total sessions exported: {total_sessions}")
    if total_errors > 0:
        print(f"Errors encountered: {total_errors}")
    print(f"Output directory: {output_dir.absolute()}")
    print("=" * 80)


def _accumulate_export_results(totals: ExportTotals, result: dict, is_multi: bool = False):
    """Accumulate export results into totals dict."""
    if is_multi:
        totals["sources"].extend(result["sources"])
    elif result.get("source"):
        totals["sources"].append(result["source"])
    totals["sessions"] += result["sessions"]
    totals["errors"] += result["errors"]


def _get_export_all_patterns(args) -> list:
    """Get patterns list for export all command."""
    patterns = getattr(args, "patterns", None)
    if patterns is not None:
        return patterns
    ws = getattr(args, "workspace", "")
    return [ws] if ws else []


def cmd_export_all(args):  # noqa: C901
    """Export from all homes: local, WSL distributions (Windows), Windows (WSL), and optionally remotes."""
    in_wsl = is_running_in_wsl()
    local_label = "Local WSL" if in_wsl else "Local Windows"
    patterns = _get_export_all_patterns(args)
    no_wsl = getattr(args, "no_wsl", False)
    no_windows = getattr(args, "no_windows", False)
    no_remote = getattr(args, "no_remote", False)

    # Include saved sources in remotes
    remotes = getattr(args, "remotes", []) or []
    if no_remote:
        if remotes:
            print("Warning: --no-remote set; ignoring explicit remotes")
        remotes = []
    else:
        for src in get_saved_sources():
            if src not in remotes:
                remotes.append(src)
    args.remotes = remotes

    # Pre-flight validation
    print("[Validation] Checking access to all homes...")
    valid, errors = validate_export_all_homes(args, in_wsl)
    if not valid:
        print("\nExport aborted: Cannot access all homes\n")
        for error in errors:
            print(error)
        print("\nPlease fix the issues above and try again.")
        sys.exit(1)
    print("All homes accessible\n")

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    totals: ExportTotals = {"sources": [], "sessions": 0, "errors": 0}

    _accumulate_export_results(
        totals, _export_local_sessions(args, output_dir, patterns, local_label)
    )

    if in_wsl:
        if not no_windows:
            _accumulate_export_results(
                totals, _export_windows_from_wsl(args, output_dir, patterns), is_multi=True
            )
    elif not no_wsl:
        _accumulate_export_results(
            totals, _export_wsl_from_windows(args, output_dir, patterns), is_multi=True
        )

    if not no_remote:
        _accumulate_export_results(
            totals, _export_all_remotes(args, output_dir, patterns), is_multi=True
        )

    _print_export_all_summary(totals["sources"], totals["sessions"], totals["errors"], output_dir)

    if getattr(args, "index", True):
        generate_index_manifest(output_dir, totals["sources"])


def _classify_session_source(filename: str) -> str:
    """Classify session source based on filename prefix."""
    if filename.startswith(CACHED_WSL_PREFIX):
        return "wsl"
    if filename.startswith(CACHED_WINDOWS_PREFIX):
        return "windows"
    if filename.startswith(CACHED_REMOTE_PREFIX):
        return "remote"
    return "local"


def _scan_workspace_directories(output_dir: Path) -> dict:
    """Scan workspace directories and group sessions by source."""
    workspaces = {}
    for item in output_dir.iterdir():
        if not item.is_dir():
            continue

        sessions = list(item.glob("*.md"))
        sources = {"local": 0, "wsl": 0, "windows": 0, "remote": 0}
        for session_file in sessions:
            sources[_classify_session_source(session_file.name)] += 1

        workspaces[item.name] = {"total": len(sessions), "sources": sources, "sessions": sessions}
    return workspaces


def _format_workspace_sources(sources: dict) -> list:
    """Format workspace sources as markdown lines."""
    lines = []
    for source, label in [
        ("local", "Local"),
        ("wsl", "WSL"),
        ("windows", "Windows"),
        ("remote", "Remote"),
    ]:
        if sources[source] > 0:
            lines.append(f"- **{label}:** {sources[source]} sessions")
    return lines


def generate_index_manifest(output_dir: Path, sources_info: list):
    """Generate index.md manifest file summarizing all exported sessions."""
    workspaces = _scan_workspace_directories(output_dir)

    lines = [
        "# Claude Conversation Export Index",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Total Workspaces:** {len(workspaces)}",
        f"**Total Sessions:** {sum(ws['total'] for ws in workspaces.values())}",
        "",
        "## Sources",
        "",
    ]

    for source_info in sources_info:
        lines.append(f"- **{source_info['source']}**: {source_info['sessions']} sessions")

    lines.extend(["", "## Workspaces", ""])

    for workspace_name in sorted(workspaces.keys()):
        ws_info = workspaces[workspace_name]
        lines.append(f"### {workspace_name} ({ws_info['total']} sessions)")
        lines.append("")
        lines.extend(_format_workspace_sources(ws_info["sources"]))
        lines.append("")

    index_path = output_dir / "index.md"
    index_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"\n[Index] Generated: {index_path}")


# ============================================================================
# Main
# ============================================================================

# Help text for argument parser
HELP_EPILOG = """
EXAMPLES:

  List workspaces:
    agent-history lsw                        # all local workspaces
    agent-history lsw myproject              # filter by pattern
    agent-history lsw -r user@server         # remote workspaces

  List sessions:
    agent-history lss                        # current workspace
    agent-history lss myproject              # specific workspace
    agent-history lss myproject -r user@server    # remote sessions

  Export (unified interface with orthogonal flags):
    agent-history export                     # current workspace, local home
    agent-history export --ah                # current workspace, all homes
    agent-history export --aw                # all workspaces, local home
    agent-history export --ah --aw           # all workspaces, all homes

    agent-history export myproject           # specific workspace, local
    agent-history export myproject --ah      # specific workspace, all homes
    agent-history export file.jsonl         # export single file

    agent-history export -o /tmp/backup      # current workspace, custom output
    agent-history export myproject -o ./out  # specific workspace, custom output

    agent-history export -r user@server      # current workspace, specific remote
    agent-history export --ah -r user@vm01   # current workspace, all homes + SSH

  Date filtering:
    agent-history lss myproject --since 2025-11-01
    agent-history export myproject --since 2025-11-01 --until 2025-11-30

  Export options:
    agent-history export myproject --minimal       # minimal mode
    agent-history export myproject --split 500     # split long conversations
    agent-history export myproject --flat          # flat structure (no subdirs)

  WSL access (Windows):
    agent-history lsh --wsl                        # list WSL distributions
    agent-history lsw --wsl                        # list WSL workspaces
    agent-history lsw --wsl Ubuntu                 # list from specific distro
    agent-history lss myproject --wsl              # list WSL sessions
    agent-history export myproject --wsl           # export from WSL

  Windows access (from WSL):
    agent-history lsh --windows                    # list Windows users with Claude
    agent-history lsw --windows                    # list Windows workspaces
    agent-history lss myproject --windows          # list Windows sessions
    agent-history export myproject --windows       # export from Windows

  Notes:
    - Outputs may show '[missing]' when a workspace directory no longer exists; the path
      is the closest match based on the stored workspace name.
"""


def _setup_windows_encoding():
    """Fix Unicode encoding for Windows console (emojis in output)."""
    if sys.platform == "win32":
        try:
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        except AttributeError:
            # Python < 3.7 doesn't have reconfigure
            import codecs

            sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
            sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")


def _validate_wsl_windows_mutex(args):
    """Validate that --wsl and --windows are not used together."""
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)

    if wsl_flag and windows_flag:
        sys.stderr.write("Error: --wsl and --windows cannot be used together\n")
        sys.stderr.write("  â€¢ Use --wsl to access WSL from Windows\n")
        sys.stderr.write("  â€¢ Use --windows to access Windows from WSL\n")
        sys.exit(1)


def _resolve_remote_flag(args):
    """Convert --wsl and --windows flags to remote format.

    Args:
        args: Parsed arguments with wsl, windows, and remotes attributes

    Returns:
        Remote string (e.g., 'wsl://Ubuntu', 'windows', 'user@host') or None
    """
    _validate_wsl_windows_mutex(args)

    remotes = getattr(args, "remotes", None)
    remote = remotes[0] if remotes else None
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)

    if wsl_flag:
        # If already in WSL, --wsl is a no-op (use local)
        if is_running_in_wsl():
            return None
        agent = getattr(args, "agent", None) or "auto"
        wsl_distros = get_wsl_distributions()
        if wsl_distros:
            selected = _select_wsl_distro_for_agent(wsl_distros, agent)
            if selected:
                return f"wsl://{selected['name']}"
        sys.stderr.write("Error: No WSL distributions with matching agent data found\n")
        sys.exit(1)

    if windows_flag:
        # If already on Windows, --windows is a no-op (use local)
        if platform.system() == "Windows":
            return None
        return "windows"

    return remote


def _get_wsl_remote_for_export(agent: Optional[str] = None) -> Optional[str]:
    """Get WSL remote string for export. Returns None if in WSL or no matching distros."""
    if is_running_in_wsl():
        return None
    wsl_distros = get_wsl_distributions()
    selected = _select_wsl_distro_for_agent(wsl_distros, agent or "auto")
    if selected:
        return f"wsl://{selected['name']}"
    return None


def _resolve_remote_list(args):
    """Convert --wsl and --windows flags to remote list for export command."""
    _validate_wsl_windows_mutex(args)

    remotes = list(getattr(args, "remotes", None) or [])

    if getattr(args, "wsl", False):
        wsl_remote = _get_wsl_remote_for_export(getattr(args, "agent", None) or "auto")
        if wsl_remote:
            remotes.append(wsl_remote)

    if getattr(args, "windows", False) and platform.system() != "Windows":
        remotes.append("windows")

    return remotes if remotes else None


def _add_lsw_parser(subparsers):
    """Add lsw (list workspaces) subparser."""
    parser = subparsers.add_parser(
        "lsw", help="List workspaces", description="List all workspaces (local or remote)"
    )

    parser.add_argument(
        "pattern",
        nargs="*",
        help="Optional pattern(s) to filter workspaces (multiple patterns supported)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List workspaces from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="List workspaces from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="List workspaces from Windows (auto-detects user)"
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local workspaces (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        dest="all_homes",
        action="store_true",
        help="List workspaces from all homes (local + WSL/Windows + remotes)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_lss_parser(subparsers):
    """Add lss (list sessions) subparser."""
    parser = subparsers.add_parser(
        "lss",
        help="List sessions",
        description="List sessions in a workspace (defaults to current workspace)",
    )

    parser.add_argument(
        "workspace",
        nargs="*",
        help="Workspace name(s) (default: current workspace, multiple patterns supported)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List sessions from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="List sessions from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="List sessions from Windows (auto-detects user)"
    )
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local sessions (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--no-wsl",
        action="store_true",
        help="Exclude WSL sessions (applies to --ah and mixed listings)",
    )
    parser.add_argument(
        "--no-windows",
        action="store_true",
        help="Exclude Windows sessions (applies to --ah and mixed listings)",
    )
    parser.add_argument(
        "--since",
        metavar="DATE",
        help="Only include sessions modified on or after this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--until",
        metavar="DATE",
        help="Only include sessions modified on or before this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        dest="all_homes",
        action="store_true",
        help="List sessions from all homes (local + WSL/Windows + remotes)",
    )
    parser.add_argument(
        "--alias",
        metavar="NAME",
        help="Use alias instead of workspace pattern (alternative to @name syntax)",
    )
    parser.add_argument(
        "--aw",
        "--all-workspaces",
        dest="all_workspaces",
        action="store_true",
        help="Include all workspaces for the selected homes (default: current workspace only)",
    )
    parser.add_argument(
        "--counts",
        action="store_true",
        help="Count messages (slower, required for WSL counts on Windows)",
    )
    parser.add_argument(
        "--wsl-counts",
        action="store_true",
        help="Count messages for WSL sessions on Windows (slower)",
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_lsh_parser(subparsers):
    """Add lsh (list homes) subparser."""
    parser = subparsers.add_parser(
        "lsh",
        help="List homes and manage SSH remotes",
        description="List all Claude Code installations (local, WSL, Windows, SSH remotes)",
    )

    lsh_subparsers = parser.add_subparsers(dest="lsh_action")

    # lsh add
    parser_add = lsh_subparsers.add_parser(
        "add", help="Add an SSH remote", description="Add an SSH remote (user@hostname)"
    )
    parser_add.add_argument("source", help="SSH remote (user@hostname)")

    # lsh remove
    parser_remove = lsh_subparsers.add_parser(
        "remove", help="Remove an SSH remote", description="Remove an SSH remote from configuration"
    )
    parser_remove.add_argument("source", help="SSH remote to remove")

    # lsh clear
    lsh_subparsers.add_parser(
        "clear", help="Clear all SSH remotes", description="Remove all saved SSH remotes"
    )

    # Filter flags for listing (default action)
    parser.add_argument("--wsl", action="store_true", help="Show WSL distributions only")
    parser.add_argument("--windows", action="store_true", help="Show Windows users only")
    parser.add_argument("--local", action="store_true", help="Show local home folder only")
    parser.add_argument("--remotes", action="store_true", help="Show SSH remotes only")
    return parser


def _add_export_parser(subparsers):
    """Add export subparser."""
    parser = subparsers.add_parser(
        "export",
        help="Export to markdown",
        description="Export workspace or single file to markdown",
    )

    parser.add_argument(
        "target",
        nargs="*",
        default=[],
        help="Workspace pattern(s) or file.jsonl (optional, multiple patterns supported)",
    )
    parser.add_argument(
        "output_dir",
        nargs="?",
        default="./ai-chats",
        help="Output directory (default: ./ai-chats)",
    )
    parser.add_argument(
        "-o",
        "--output",
        metavar="DIR",
        dest="output_override",
        help="Output directory (overrides positional output_dir)",
    )
    parser.add_argument(
        "-a",
        "--all",
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Export all workspaces (default: current workspace)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Export from all homes: local + WSL + Windows + remotes (default: local only)",
    )
    parser.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="Export from remote host via SSH (user@hostname) - can be used multiple times",
    )
    parser.add_argument(
        "--wsl", action="store_true", help="Export from WSL (auto-detects distribution)"
    )
    parser.add_argument(
        "--windows", action="store_true", help="Export from Windows (auto-detects user)"
    )
    parser.add_argument("--no-remote", action="store_true", help="Skip SSH remotes")
    parser.add_argument("--no-wsl", action="store_true", help="Skip WSL sources")
    parser.add_argument("--no-windows", action="store_true", help="Skip Windows sources")
    parser.add_argument(
        "--local",
        action="store_true",
        help="Include local (use with -r to combine local + remote)",
    )
    parser.add_argument(
        "--force", action="store_true", help="Force re-export (default: incremental)"
    )
    parser.add_argument(
        "--minimal",
        action="store_true",
        help="Minimal export: omit metadata, keep only conversation content",
    )
    parser.add_argument(
        "--split",
        metavar="LINES",
        type=validate_split_lines,
        help="Split long conversations into parts (e.g., --split 500)",
    )
    parser.add_argument(
        "--jobs",
        type=int,
        default=1,
        help="Parallelism for exports (default: 1)",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress per-file output (show summary only)",
    )
    parser.add_argument(
        "--since",
        metavar="DATE",
        help="Only include sessions modified on or after this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--until",
        metavar="DATE",
        help="Only include sessions modified on or before this date (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--flat",
        action="store_true",
        help="Use flat directory structure (default: organized by workspace with source tags)",
    )
    parser.add_argument(
        "--alias", metavar="NAME", help="Export using alias (alternative to @name syntax)"
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_alias_parser(subparsers):
    """Add alias subparser."""
    parser = subparsers.add_parser(
        "alias",
        help="Manage workspace aliases",
        description="Create and manage workspace aliases for consolidated exports",
    )

    alias_subparsers = parser.add_subparsers(dest="alias_command", help="Alias subcommand")

    # alias list
    parser_list = alias_subparsers.add_parser(
        "list",
        help="List all aliases",
        description="List all aliases with their workspaces",
    )
    parser_list.add_argument(
        "-c",
        "--counts",
        action="store_true",
        help="Show session counts (slower, accesses all sources)",
    )

    # alias show
    parser_show = alias_subparsers.add_parser(
        "show", help="Show alias details", description="Show details of a single alias"
    )
    parser_show.add_argument("name", help="Alias name")

    # alias create
    parser_create = alias_subparsers.add_parser(
        "create", help="Create a new alias", description="Create a new empty alias"
    )
    parser_create.add_argument("name", help="Alias name")

    # alias delete
    parser_delete = alias_subparsers.add_parser(
        "delete",
        help="Delete an alias",
        description="Delete an alias and all its workspace associations",
    )
    parser_delete.add_argument("name", help="Alias name")

    # alias add
    parser_add = alias_subparsers.add_parser(
        "add",
        help="Add workspace(s) to alias",
        description="Add one or more workspaces to an alias",
    )
    parser_add.add_argument(
        "name",
        nargs="?",
        default="",
        help="Alias name (default: auto-detect from current directory)",
    )
    parser_add.add_argument(
        "workspaces", nargs="*", help="Workspace names to add (encoded directory names)"
    )
    parser_add.add_argument("--pick", action="store_true", help="Interactive workspace picker")
    parser_add.add_argument(
        "-r", "--remote", metavar="HOST", help="Add workspace from remote host (user@hostname)"
    )
    parser_add.add_argument(
        "--wsl", action="store_true", help="Add workspace from WSL (auto-detects distribution)"
    )
    parser_add.add_argument(
        "--windows", action="store_true", help="Add workspace from Windows (auto-detects user)"
    )
    parser_add.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Add from all homes (local + WSL/Windows + remotes)",
    )

    # alias remove
    parser_remove = alias_subparsers.add_parser(
        "remove", help="Remove workspace from alias", description="Remove a workspace from an alias"
    )
    parser_remove.add_argument("name", help="Alias name")
    parser_remove.add_argument(
        "workspace", help="Workspace name to remove (encoded directory name)"
    )
    parser_remove.add_argument(
        "-r", "--remote", metavar="HOST", help="Remove workspace from remote source"
    )
    parser_remove.add_argument(
        "--wsl", action="store_true", help="Remove workspace from WSL source"
    )
    parser_remove.add_argument(
        "--windows", action="store_true", help="Remove workspace from Windows source"
    )

    # alias export
    alias_subparsers.add_parser(
        "export",
        help="Export aliases to JSON",
        description="Export aliases configuration to JSON (stdout)",
    )

    # alias import
    parser_import = alias_subparsers.add_parser(
        "import",
        help="Import aliases from JSON",
        description="Import aliases from JSON file or stdin",
    )
    parser_import.add_argument("file", nargs="?", help="JSON file to import (default: stdin)")
    parser_import.add_argument(
        "--replace", action="store_true", help="Replace all aliases instead of merging"
    )

    return parser


def _add_stats_parser(subparsers):
    """Add stats subparser."""
    parser = subparsers.add_parser(
        "stats",
        help="Show usage statistics and metrics",
        description="Display Claude Code usage statistics from synced metrics database",
    )

    parser.add_argument(
        "workspace", nargs="*", default=None, help="Filter by workspace pattern(s) or @alias"
    )
    parser.add_argument("--sync", action="store_true", help="Sync only (don't show stats)")
    parser.add_argument(
        "--no-sync",
        action="store_true",
        dest="no_sync",
        help="Skip auto-sync (faster, uses cached data)",
    )
    parser.add_argument(
        "--force", action="store_true", help="Force re-sync all files (ignore mtime)"
    )
    parser.add_argument("--tools", action="store_true", help="Show tool usage statistics")
    parser.add_argument("--models", action="store_true", help="Show model usage statistics")
    parser.add_argument(
        "--time", action="store_true", help="Show time tracking with daily breakdown"
    )
    parser.add_argument(
        "--by-workspace", action="store_true", help="Show statistics grouped by workspace"
    )
    parser.add_argument("--by-day", action="store_true", help="Show daily statistics")
    parser.add_argument(
        "--source",
        metavar="SOURCE",
        help="Filter by source (local, wsl:distro, windows, remote:host)",
    )
    parser.add_argument(
        "--jobs",
        type=int,
        default=1,
        help="Parallelism for remote sync operations (default: 1)",
    )
    parser.add_argument("--no-remote", action="store_true", help="Skip SSH remotes")
    parser.add_argument("--no-wsl", action="store_true", help="Skip WSL sources")
    parser.add_argument("--no-windows", action="store_true", help="Skip Windows sources")
    parser.add_argument(
        "--since", metavar="DATE", help="Filter sessions starting from this date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--until", metavar="DATE", help="Filter sessions until this date (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Show stats for all workspaces (default: current workspace)",
    )
    parser.add_argument(
        "--top-ws",
        type=int,
        default=None,
        help="Limit the number of workspaces shown per home (default: all)",
    )
    parser.add_argument(
        "--this",
        action="store_true",
        dest="this_only",
        help="Use current workspace only, not its alias (if aliased)",
    )
    parser.add_argument(
        "--ah",
        "--all-homes",
        action="store_true",
        dest="all_homes",
        help="Sync from all homes (local + WSL/Windows)",
    )
    parser.add_argument("--wsl", action="store_true", help="Include WSL source")
    parser.add_argument("--windows", action="store_true", help="Include Windows source")
    parser.add_argument(
        "-r",
        "--remote",
        action="append",
        dest="remotes",
        metavar="HOST",
        help="Include remote source (can be repeated)",
    )
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=argparse.SUPPRESS,
        help="Agent backend to use (default: auto-detect)",
    )
    return parser


def _add_reset_parser(subparsers):
    """Add reset subparser."""
    parser = subparsers.add_parser(
        "reset",
        help="Reset stored data (database, settings, aliases)",
        description="Delete metrics database, settings, and/or aliases.",
    )

    parser.add_argument(
        "what",
        nargs="?",
        choices=["db", "settings", "aliases", "all"],
        default="all",
        help="What to reset: db (metrics), settings (SSH remotes), aliases, or all (default: all)",
    )
    parser.add_argument("-y", "--yes", action="store_true", help="Skip confirmation prompt")
    return parser


def _create_argument_parser():
    """Create and configure the argument parser.

    Returns:
        Configured argparse.ArgumentParser instance
    """
    parser = argparse.ArgumentParser(
        prog="agent-history",
        description="Browse and export AI coding assistant conversation history (Claude Code, Codex CLI)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=HELP_EPILOG,
    )

    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    # Global agent selection flag (before subcommand)
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default="auto",
        help="Agent backend to use (default: auto-detect based on available data)",
    )

    # Create subparsers for commands
    subparsers = parser.add_subparsers(dest="command", help="Command to execute", required=False)

    # Add all command parsers
    _add_lsw_parser(subparsers)
    _add_lss_parser(subparsers)
    _add_lsh_parser(subparsers)
    _add_export_parser(subparsers)
    _add_alias_parser(subparsers)
    _add_stats_parser(subparsers)
    _add_reset_parser(subparsers)
    _add_install_parser(subparsers)
    _add_gemini_index_parser(subparsers)

    return parser


def _add_install_parser(subparsers):
    """Add install subparser."""
    parser = subparsers.add_parser(
        "install",
        help="Install CLI and Claude skill",
        description="Install the agent-history CLI to ~/.local/bin (default) and copy the skill files for Claude Code.",
    )

    parser.add_argument(
        "--bin-dir",
        default=str(DEFAULT_BIN_DIR),
        help="Destination directory for the CLI binary (default: ~/.local/bin)",
    )
    parser.add_argument(
        "--cli-name",
        default=DEFAULT_CLI_NAME,
        help="Binary name to use for the CLI install (default: agent-history)",
    )
    parser.add_argument(
        "--skill-dir",
        default=str(DEFAULT_SKILL_DIR),
        help="Destination directory for the Claude skill files (default: ~/.claude/skills/agent-history)",
    )
    parser.add_argument(
        "--skill-name",
        default=DEFAULT_SKILL_NAME,
        help="Binary name to use inside the Claude skill directory (default: agent-history)",
    )
    parser.add_argument(
        "--skip-cli",
        action="store_true",
        help="Skip installing the CLI binary (only update skill/settings)",
    )
    parser.add_argument(
        "--skip-skill",
        action="store_true",
        help="Skip installing the Claude skill files",
    )
    parser.add_argument(
        "--skip-settings",
        action="store_true",
        help="Skip updating ~/.claude/settings.json",
    )
    return parser


def _add_gemini_index_parser(subparsers):
    """Add gemini-index subparser."""
    parser = subparsers.add_parser(
        "gemini-index",
        help="Manage Gemini hashâ†’path index",
        description=(
            "Manage the Gemini hash-to-path index. By default, lists all mappings. "
            "Use --add to add project paths to the index. "
            "For each path added, computes its SHA-256 hash and checks if Gemini "
            "has sessions for that project. If sessions exist, adds the mapping "
            "so agent-history can display readable workspace paths instead of hashes."
        ),
    )

    parser.add_argument(
        "--add",
        "-a",
        nargs="*",
        dest="add_paths",
        metavar="PATH",
        help="Add project directories to index (default: current directory if no paths given)",
    )
    parser.add_argument(
        "--list",
        "-l",
        action="store_true",
        dest="list_index",
        help="List all mappings in the hash index (default if no options)",
    )
    parser.add_argument(
        "--full-hash",
        action="store_true",
        help="Show full SHA-256 hashes instead of truncated (with --list)",
    )
    return parser


# ============================================================================
# Command Dispatch
# ============================================================================


def _get_remote_workspaces_for_lsw(remote: str, patterns: list, agent: str = "auto") -> tuple:
    """Get filtered workspaces from a remote for lsw.

    Returns:
        Tuple of (hostname, sessions_list) or (None, []) on error
    """
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Cannot connect to remote: {remote}\n")
        return None, []

    hostname = get_remote_hostname(remote)
    sessions = []
    agent_key = agent or "auto"

    if agent_key in (AGENT_CLAUDE, "auto"):
        remote_workspaces = list_remote_workspaces(remote)

        # Filter by patterns
        if patterns and patterns != [""]:
            remote_workspaces = [
                ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)
            ]

        # Create session-like dicts for output
        sessions.extend(
            [
                {
                    "workspace": ws,
                    "workspace_readable": normalize_workspace_name(ws),
                    "agent": AGENT_CLAUDE,
                }
                for ws in remote_workspaces
            ]
        )

    if agent_key in (AGENT_CODEX, "auto"):
        for ws in _list_remote_codex_workspaces_only(remote, patterns):
            decoded = ws.get("decoded")
            if decoded:
                sessions.append(
                    {
                        "workspace": decoded,
                        "workspace_readable": decoded,
                        "agent": AGENT_CODEX,
                    }
                )

    if agent_key in (AGENT_GEMINI, "auto"):
        for ws in _list_remote_gemini_workspaces_only(remote, patterns):
            decoded = ws.get("decoded")
            if decoded:
                sessions.append(
                    {
                        "workspace": decoded,
                        "workspace_readable": decoded,
                        "agent": AGENT_GEMINI,
                    }
                )

    return hostname, sessions


def _print_lsw_results(all_results: list, workspaces_only: bool):
    """Print lsw results in the appropriate format."""
    for source_label, sessions in all_results:
        if workspaces_only:
            print(f"# {source_label}")
            seen_workspaces = set()
            for session in sessions:
                ws = session["workspace_readable"]
                # On Windows, only show paths that actually exist to avoid stale/invalid entries
                if sys.platform == "win32":
                    try:
                        p = Path(ws)
                        # Only filter for Windows and WSL sources
                        if (
                            source_label.startswith("Windows") or source_label.startswith("WSL ")
                        ) and not p.exists():
                            continue
                    except Exception:
                        pass
                if ws not in seen_workspaces:
                    seen_workspaces.add(ws)
                    print(f"  {ws}")
            print()
        else:
            for session in sessions:
                print(format_session_line(session, source_label))


def _dispatch_lsw_additive(args):
    """Handle --local with -r for additive lsw (local + remotes).

    Args:
        args: Arguments with patterns, remotes, and local_only flag
    """
    patterns = getattr(args, "patterns", [""])
    remotes = getattr(args, "remotes", [])
    workspaces_only = getattr(args, "workspaces_only", False)
    agent = getattr(args, "agent", None) or "auto"

    all_results = []

    # 1. Get local workspaces
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    try:
        local_sessions = collect_sessions_with_dedup(patterns, include_cached=False, agent=agent)
        if local_sessions:
            all_results.append((local_label, local_sessions))
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")

    # 2. Get from SSH remotes
    for remote in remotes:
        try:
            hostname, sessions = _get_remote_workspaces_for_lsw(remote, patterns, agent=agent)
            if hostname and sessions:
                all_results.append((f"Remote ({hostname})", sessions))
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")

    _print_lsw_results(all_results, workspaces_only)


def _filter_sessions_by_date(sessions: list, since_date, until_date) -> list:
    """Filter sessions by date range."""
    if not since_date and not until_date:
        return sessions
    return [s for s in sessions if is_date_in_range(s.get("modified"), since_date, until_date)]


def _collect_local_sessions_for_additive(
    patterns: list, since_date, until_date, agent: str = "auto"
) -> Optional[tuple]:
    """Collect local sessions for additive mode. Returns (label, sessions) or None."""
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    try:
        sessions = collect_sessions_with_dedup(
            patterns,
            since_date=since_date,
            until_date=until_date,
            include_cached=False,
            agent=agent,
        )
        if sessions:
            return (local_label, sessions)
    except (OSError, PermissionError) as e:
        sys.stderr.write(f"Error accessing local: {e}\n")
    return None


def _collect_remotes_for_additive(
    remotes: list, patterns: list, since_date, until_date, agent: str
) -> list:
    """Collect sessions from SSH remotes for additive mode."""
    results = []
    for remote in remotes:
        try:
            result = _collect_remote_sessions(remote, patterns, since_date, until_date, agent)
            if result:
                results.append(result)
        except (OSError, subprocess.SubprocessError) as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")
    return results


def _dispatch_lss_additive(args):
    """Handle --local with -r for additive lss (local + remotes)."""
    patterns = getattr(args, "patterns", [""])
    remotes = getattr(args, "remotes", [])
    since_date = getattr(args, "since_date", None)
    until_date = getattr(args, "until_date", None)
    agent = getattr(args, "agent", None) or "auto"

    all_results = []
    local_result = _collect_local_sessions_for_additive(patterns, since_date, until_date, agent)
    if local_result:
        all_results.append(local_result)
    all_results.extend(
        _collect_remotes_for_additive(remotes, patterns, since_date, until_date, agent)
    )

    if all_results:
        print("AGENT\tHOME\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for source_label, sessions in all_results:
            for session in sessions:
                print(format_session_line(session, source_label))


def _dispatch_lsw(args):
    """Dispatch lsw (list workspaces) command."""
    patterns = args.pattern if args.pattern else [""]
    local_flag = getattr(args, "local", False)
    remotes = getattr(args, "remotes", None) or []
    agent = getattr(args, "agent", None) or "auto"

    if getattr(args, "all_homes", False):
        list_args = ListCommandArgs(
            patterns=patterns,
            workspaces_only=True,
            remotes=remotes,
            agent=agent,
        )
        cmd_list_all_homes(list_args)
    elif local_flag and remotes:
        # --local with -r: show both local and remotes (additive)
        list_args = ListCommandArgs(
            patterns=patterns,
            workspaces_only=True,
            remotes=remotes,
            local_only=True,
            agent=agent,
        )
        _dispatch_lsw_additive(list_args)
    else:
        list_args = ListCommandArgs(
            patterns=patterns,
            workspaces_only=True,
            remote=_resolve_remote_flag(args),
            agent=agent,
        )
        cmd_list(list_args)


def _dispatch_lss_to_alias(alias_name: str, args):
    """Handle lss dispatch to alias command."""
    since_date = parse_date_with_validation(args.since, "--since")
    until_date = parse_date_with_validation(args.until, "--until")
    agent = getattr(args, "agent", None) or "auto"
    cmd_alias_lss(alias_name, since_date=since_date, until_date=until_date, agent=agent, args=args)


def _dispatch_lss_local_additive(args, workspace_list: list, remotes: list):
    """Handle lss with --local and -r flags (additive mode)."""
    agent = getattr(args, "agent", None) or "auto"
    source_hint = _get_source_hint_from_args(args)
    patterns, _ = resolve_patterns_for_command(workspace_list, agent=agent, source_hint=source_hint)

    list_args = ListCommandArgs(
        patterns=patterns,
        since=args.since,
        until=args.until,
        remotes=remotes,
        local_only=True,
        agent=agent,
    )
    _dispatch_lss_additive(list_args)


def _dispatch_lss_all_homes(args, workspace_list: list):
    """Handle lss with --all-homes flag."""
    this_only = getattr(args, "this_only", False)
    all_workspaces_flag = getattr(args, "all_workspaces", False)
    since_date = parse_date_with_validation(args.since, "--since")
    until_date = parse_date_with_validation(args.until, "--until")
    lss_remotes = getattr(args, "remotes", None) or []
    agent = getattr(args, "agent", None) or "auto"
    source_hint = _get_source_hint_from_args(args)

    if all_workspaces_flag and not workspace_list:
        patterns = [""]
        alias_for_ws = None
    else:
        patterns, alias_for_ws = resolve_patterns_for_command(
            workspace_list, this_only, agent=agent, source_hint=source_hint
        )
    if alias_for_ws:
        sys.stderr.write(f"Using alias @{alias_for_ws} (use --this for current workspace only)\n")
        cmd_alias_lss(alias_for_ws, since_date=since_date, until_date=until_date, agent=agent)
        return

    list_args = ListCommandArgs(
        patterns=patterns,
        since=args.since,
        until=args.until,
        remotes=lss_remotes,
        agent=agent,
        counts=getattr(args, "counts", False),
        wsl_counts=getattr(args, "wsl_counts", False),
        no_wsl=getattr(args, "no_wsl", False),
        no_windows=getattr(args, "no_windows", False),
    )
    cmd_list_all_homes(list_args)


def _dispatch_lss_local(args, workspace_list: list):
    """Handle lss for local sessions only."""
    projects_dir_override = getattr(args, "projects_dir_override", None)

    this_only = getattr(args, "this_only", False)
    all_workspaces_flag = getattr(args, "all_workspaces", False)
    remote_value = _resolve_remote_flag(args)
    needs_remote_scope = bool(
        remote_value or getattr(args, "windows", False) or getattr(args, "wsl", False)
    )
    error_msg = (
        "Not in a Claude Code workspace.\n\n"
        "To list sessions, either:\n"
        "  â€¢ Run from within a workspace directory\n"
        "  â€¢ Specify a workspace pattern: lss <pattern>\n"
        "  â€¢ Use --ah to list from all homes: lss --ah"
    )
    agent = getattr(args, "agent", None) or "auto"
    source_hint = _get_source_hint_from_args(args)
    if all_workspaces_flag and not workspace_list:
        patterns = [""]
        alias_for_ws = None
    else:
        require_ws = not needs_remote_scope
        err = error_msg if require_ws else None
        patterns, alias_for_ws = resolve_patterns_for_command(
            workspace_list,
            this_only,
            require_workspace=require_ws,
            error_message=err,
            agent=agent,
            source_hint=source_hint,
        )
    if alias_for_ws:
        sys.stderr.write(f"Using alias @{alias_for_ws} (use --this for current workspace only)\n")
        _dispatch_lss_to_alias(alias_for_ws, args)
        return

    list_args = ListCommandArgs(
        patterns=patterns,
        since=args.since,
        until=args.until,
        remote=remote_value,
        projects_dir=projects_dir_override,
        agent=getattr(args, "agent", None) or "auto",
        counts=getattr(args, "counts", False),
        wsl_counts=getattr(args, "wsl_counts", False),
        no_wsl=getattr(args, "no_wsl", False),
        no_windows=getattr(args, "no_windows", False),
    )
    cmd_list(list_args)


def _detect_projects_dir_override(workspace_inputs: list[str]) -> tuple[Path | None, bool]:
    """Return (projects_dir_override, saw_wsl_unc_input)."""
    projects_dir_override = None
    wsl_unc_in_input = False
    for raw in workspace_inputs:
        normalized = raw.replace("\\", "/")
        lower_norm = normalized.lower()
        if lower_norm.startswith("//wsl.localhost/") or lower_norm.startswith("//wsl$/"):
            projects_dir_override = _projects_dir_from_wsl_unc(normalized)
            wsl_unc_in_input = True
            break
        if projects_dir_override is None:
            try:
                path_obj = Path(raw)
                if path_obj.is_absolute():
                    parent = path_obj.parent
                    if parent.exists():
                        projects_dir_override = parent
            except OSError:
                continue
    return projects_dir_override, wsl_unc_in_input


def _ensure_workspace_default_for_remote(args, workspace_list: list[str]) -> list[str]:
    """Default to all workspaces when remote flags are set without explicit patterns."""
    if workspace_list:
        return workspace_list

    if getattr(args, "all_workspaces", False):
        return [""]

    selected_remote = None
    try:
        selected_remote = _resolve_remote_flag(args)
    except SystemExit:
        selected_remote = None

    windows_flag = getattr(args, "windows", False)
    wsl_flag = getattr(args, "wsl", False)

    if selected_remote or windows_flag or wsl_flag:
        # Keep list empty so downstream defaults to current workspace/alias
        return workspace_list

    return workspace_list


def _get_source_hint_from_args(args) -> Optional[str]:
    """Return source hint for workspace inference."""
    if getattr(args, "windows", False):
        return "windows"
    if getattr(args, "wsl", False):
        return "wsl"
    return None


def _apply_wsl_unc_override(args, saw_wsl_unc: bool):
    """Force local mode when user passed a WSL UNC path."""
    if saw_wsl_unc and not getattr(args, "wsl", False):
        args.wsl = False
        args.remotes = []


def _dispatch_lss(args):
    """Dispatch lss (list sessions) command."""
    workspace_inputs = args.workspace if args.workspace else []
    projects_dir_override, wsl_unc_in_input = _detect_projects_dir_override(workspace_inputs)
    workspace_list = [_coerce_target_to_workspace_pattern(w) for w in workspace_inputs]
    workspace_list = _ensure_workspace_default_for_remote(args, workspace_list)
    _apply_wsl_unc_override(args, wsl_unc_in_input)
    args.projects_dir_override = projects_dir_override

    if getattr(args, "wsl", False) and getattr(args, "no_wsl", False):
        exit_with_error("Conflicting flags: --wsl and --no-wsl")
    if getattr(args, "windows", False) and getattr(args, "no_windows", False):
        exit_with_error("Conflicting flags: --windows and --no-windows")

    local_flag = getattr(args, "local", False)
    remotes = getattr(args, "remotes", None) or []

    # Check for alias (explicit flag or @ prefix)
    alias_name = detect_alias_from_args(args, workspace_list)
    if alias_name:
        _dispatch_lss_to_alias(alias_name, args)
        return

    # Handle --local with -r (additive mode)
    if local_flag and remotes:
        _dispatch_lss_local_additive(args, workspace_list, remotes)
        return

    # Handle --all-homes
    if getattr(args, "all_homes", False):
        _dispatch_lss_all_homes(args, workspace_list)
    else:
        _dispatch_lss_local(args, workspace_list)


def _get_export_output_dir(args) -> str:
    """Get the final output directory from args."""
    if hasattr(args, "output_override") and args.output_override:
        return args.output_override
    return args.output_dir


def _build_export_config(
    args, output_dir: Union[str, Path], patterns: list, remote=None
) -> ExportConfig:
    """Build ExportConfig from args."""
    return ExportConfig(
        output_dir=str(output_dir),
        patterns=patterns,
        since=args.since,
        until=args.until,
        force=args.force,
        minimal=args.minimal,
        split=args.split,
        flat=args.flat,
        remote=remote,
        lenient=False,
        agent=getattr(args, "agent", None) or "auto",
        jobs=getattr(args, "jobs", 1),
        quiet=getattr(args, "quiet", False),
    )


def _prepare_single_export_output(args, jsonl_file: str) -> tuple[Path, bool]:
    """Return output file path and whether user provided a direct file."""
    output_target = Path(_get_export_output_dir(args)).expanduser()
    is_output_file = output_target.suffix.lower() == ".md"
    if not is_output_file:
        output_target.mkdir(parents=True, exist_ok=True)
        base_name = Path(Path(jsonl_file).name).with_suffix(".md").name
        output_file = output_target / base_name
    else:
        output_file = output_target
    return output_file, is_output_file


def _write_claude_single_file(
    local_jsonl: Path,
    output_file: Path,
    minimal: bool,
    split_lines: int,
    display_file: Optional[str],
    quiet: bool,
) -> None:
    """Write Claude markdown (with optional splitting) for a single session file."""
    messages = read_jsonl_messages(local_jsonl)
    parts = generate_markdown_parts(
        messages, local_jsonl, minimal, split_lines, display_file=display_file
    )
    if parts:
        base_stem = output_file.stem
        for part_num, _, part_md, _, _ in parts:
            part_path = output_file.parent / f"{base_stem}_part{part_num}.md"
            part_path.write_text(part_md, encoding="utf-8")
            if not quiet:
                print(part_path)
        return

    markdown = parse_jsonl_to_markdown(
        local_jsonl, minimal=minimal, messages=messages, display_file=display_file
    )
    output_file.write_text(markdown, encoding="utf-8")
    if not quiet:
        print(output_file)


def _export_remote_claude_single_file(
    remote: str,
    remote_path: str,
    output_file: Path,
    minimal: bool,
    split_lines: int,
    quiet: bool,
) -> None:
    """Download remote file, render markdown, and clean up temp file."""
    if not check_ssh_connection(remote):
        sys.stderr.write(f"Error: Cannot connect to {remote} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote}\n")
        sys.exit(1)

    with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as tmp:
        tmp_path = Path(tmp.name)
    try:
        if not _download_remote_file(remote, remote_path, tmp_path):
            tmp_path.unlink()
            sys.exit(1)
        display_file = f"{remote}:{remote_path}"
        _write_claude_single_file(tmp_path, output_file, minimal, split_lines, display_file, quiet)
    finally:
        if tmp_path.exists():
            tmp_path.unlink()


def _dispatch_export_single_file(args, jsonl_file: str):
    """Handle export of a single JSONL file."""
    final_remotes = _resolve_remote_list(args)
    final_remote = final_remotes[0] if final_remotes else None

    output_file, is_output_file = _prepare_single_export_output(args, jsonl_file)
    minimal = getattr(args, "minimal", False)
    split_lines = getattr(args, "split", None)
    agent_type = detect_agent_from_path(Path(jsonl_file))
    quiet = getattr(args, "quiet", False)

    # For Claude, support --minimal and --split in single-file export. For other agents, fall back
    # to the existing convert path (single output file).
    if agent_type != AGENT_CLAUDE or not split_lines:
        convert_args = ConvertCommandArgs(
            jsonl_file=jsonl_file,
            output=str(output_file),
            remote=final_remote,
            quiet=quiet,
        )
        cmd_convert(convert_args)
        return

    if is_output_file and split_lines:
        exit_with_error("Cannot use --split when -o points to a single file; use a directory.")

    if final_remote:
        _export_remote_claude_single_file(
            final_remote, jsonl_file, output_file, minimal, split_lines, quiet
        )
    else:
        _write_claude_single_file(
            Path(jsonl_file),
            output_file,
            minimal,
            split_lines,
            display_file=jsonl_file,
            quiet=quiet,
        )


def _dispatch_export_additive(args, output_dir: Union[str, Path], patterns: list, remotes: list):
    """Handle export with --local and -r flags (additive mode)."""
    # Export from local first
    local_label = "Local (WSL)" if is_running_in_wsl() else "Local"
    sys.stderr.write(f"# {local_label}\n")
    cmd_batch(_build_export_config(args, output_dir, patterns, remote=None))

    # Export from each remote
    for remote in remotes:
        hostname = get_remote_hostname(remote)
        sys.stderr.write(f"\n# Remote ({hostname})\n")
        cmd_batch(_build_export_config(args, output_dir, patterns, remote=remote))


def _dispatch_export_all_homes(args, output_dir: Union[str, Path], patterns: list, remotes: list):
    """Handle export with --all-homes flag."""
    export_config = ExportAllConfig(
        output_dir=str(output_dir),
        patterns=patterns,
        remotes=remotes,
        since=args.since,
        until=args.until,
        force=args.force,
        minimal=args.minimal,
        split=args.split,
        agent=getattr(args, "agent", None) or "auto",
        jobs=getattr(args, "jobs", 1),
        quiet=getattr(args, "quiet", False),
        no_wsl=getattr(args, "no_wsl", False),
        no_windows=getattr(args, "no_windows", False),
        no_remote=getattr(args, "no_remote", False),
    )
    cmd_export_all(export_config)


def _resolve_export_targets(args, targets: list, output_dir: Union[str, Path]) -> tuple:
    """Resolve export targets, handling workspace detection and aliases.

    Returns:
        Tuple of (resolved_targets, alias_handled) where alias_handled is True if exported via alias
    """
    if targets or args.all_workspaces:
        return targets, False

    this_only = getattr(args, "this_only", False)
    agent = getattr(args, "agent", None) or "auto"
    source_hint = _get_source_hint_from_args(args)

    # For non-Claude agents, use CWD name as pattern (matches against workspace_readable)
    if agent in ("gemini", "codex"):
        patterns, _ = resolve_patterns_for_command([], agent=agent)
        return patterns, False

    if this_only:
        source_patterns = _infer_this_only_patterns_for_source(source_hint, agent)
        if source_patterns:
            return source_patterns, False

    pattern, exists = check_current_workspace_exists()

    if not exists:
        if args.all_homes:
            args.all_workspaces = True
            return [], False
        exit_with_error(
            "Not in a Claude Code workspace.\n\n"
            "To export sessions, either:\n"
            "  â€¢ Run from within a workspace directory\n"
            "  â€¢ Specify a workspace pattern: export <pattern>\n"
            "  â€¢ Use --aw to export all workspaces: export --aw\n"
            "  â€¢ Use --ah to export from all homes: export --ah"
        )

    # Aliases are Claude-specific; skip for other agents
    if not this_only and agent in ("auto", "claude"):
        alias_for_ws = get_alias_for_workspace(pattern, "local")
        if alias_for_ws:
            sys.stderr.write(
                f"Using alias @{alias_for_ws} (use --this for current workspace only)\n"
            )
            cmd_alias_export(alias_for_ws, output_dir, args)
            return [], True

    return [pattern], False


def _route_export(args, output_dir: Union[str, Path], final_patterns: list, final_remotes):
    """Route export to appropriate handler based on flags."""
    local_flag = getattr(args, "local", False)
    if local_flag and final_remotes:
        _dispatch_export_additive(args, output_dir, final_patterns, final_remotes)
    elif args.all_homes:
        _dispatch_export_all_homes(args, output_dir, final_patterns, final_remotes)
    else:
        final_remote = final_remotes[0] if final_remotes else None
        if not cmd_batch(_build_export_config(args, output_dir, final_patterns, final_remote)):
            sys.exit(1)


def _dispatch_export(args):
    """Dispatch export command."""
    output_dir = _get_export_output_dir(args)
    targets = args.target if args.target else []
    targets = [_coerce_target_to_workspace_pattern(t) for t in targets]

    alias_name = detect_alias_from_args(args, targets)
    if alias_name:
        cmd_alias_export(alias_name, output_dir, args)
        return

    targets, alias_handled = _resolve_export_targets(args, targets, output_dir)
    if alias_handled:
        return

    if len(targets) == 1 and targets[0].endswith(".jsonl"):
        _dispatch_export_single_file(args, targets[0])
        return

    final_remotes = _resolve_remote_list(args)
    final_patterns = [] if args.all_workspaces else targets
    _route_export(args, output_dir, final_patterns, final_remotes)


def _dispatch_alias(args):
    """Dispatch alias command."""
    alias_cmd = getattr(args, "alias_command", None)

    if alias_cmd == "list" or alias_cmd is None:
        cmd_alias_list(args)
    elif alias_cmd == "show":
        cmd_alias_show(args)
    elif alias_cmd == "create":
        cmd_alias_create(args)
    elif alias_cmd == "delete":
        cmd_alias_delete(args)
    elif alias_cmd == "add":
        cmd_alias_add(args)
    elif alias_cmd == "remove":
        cmd_alias_remove(args)
    elif alias_cmd == "export":
        cmd_alias_config_export(args)
    elif alias_cmd == "import":
        cmd_alias_config_import(args)


def _get_stats_remotes(args) -> list:
    """Get remotes list for stats command, adding WSL/Windows if flagged."""
    remotes = getattr(args, "remotes", None) or []
    if args.wsl and not getattr(args, "no_wsl", False):
        wsl_distros = get_wsl_distributions()
        claude_distros = [d for d in wsl_distros if d.get("has_claude")]
        if claude_distros:
            remotes.append(f"wsl://{claude_distros[0]['name']}")
    if args.windows and not getattr(args, "no_windows", False):
        remotes.append("windows")
    return remotes


def _build_sync_args(args, remotes: list) -> SyncCommandArgs:
    """Build SyncCommandArgs for cmd_stats_sync."""
    return SyncCommandArgs(
        patterns=args.workspace if args.workspace else [""],
        remotes=remotes,
        force=args.force,
        all_homes=getattr(args, "all_homes", False),
        jobs=getattr(args, "jobs", 1),
        no_remote=getattr(args, "no_remote", False),
        no_wsl=getattr(args, "no_wsl", False),
        no_windows=getattr(args, "no_windows", False),
        show_progress=getattr(args, "sync", False),
    )


def _build_stats_args(args) -> StatsCommandArgs:
    """Build StatsCommandArgs for cmd_stats."""
    return StatsCommandArgs(
        workspace=args.workspace,
        source=args.source,
        since=args.since,
        until=args.until,
        tools=args.tools,
        models=args.models,
        time=getattr(args, "time", False),
        by_workspace=getattr(args, "by_workspace", False),
        by_day=getattr(args, "by_day", False),
        all_workspaces=getattr(args, "all_workspaces", False),
        top_ws=getattr(args, "top_ws", None),
        this_only=getattr(args, "this_only", False),
        agent=getattr(args, "agent", None) or "auto",
    )


def _dispatch_stats(args):
    """Dispatch stats command."""
    remotes = _get_stats_remotes(args)

    # Always auto-sync before showing stats (unless --no-sync)
    no_sync = getattr(args, "no_sync", False)
    if not no_sync:
        cmd_stats_sync(_build_sync_args(args, remotes))
        if not args.sync:
            # Only print separator if not explicit --sync (which already shows summary)
            print()

    # If explicit --sync, we're done (just sync, don't show stats)
    if args.sync:
        return

    cmd_stats(_build_stats_args(args))


def _dir_on_path(directory: Path) -> bool:
    """Return True if directory is on PATH."""
    path_env = os.environ.get("PATH", "")
    if not path_env:
        return False
    try:
        directory = directory.expanduser().resolve()
    except (OSError, RuntimeError):
        directory = directory.expanduser()
    for entry in path_env.split(os.pathsep):
        if not entry:
            continue
        try:
            entry_path = Path(entry).expanduser().resolve()
        except (OSError, RuntimeError):
            entry_path = Path(entry).expanduser()
        if entry_path == directory:
            return True
    return False


def _make_executable(path: Path):
    """Ensure target is executable on POSIX systems."""
    try:
        mode = path.stat().st_mode
        path.chmod(mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)
    except (PermissionError, OSError):
        pass


def _install_cli_binary(script_path: Path, bin_dir: str, binary_name: str) -> Path:
    """Install CLI binary to the specified directory."""
    target_dir = Path(bin_dir).expanduser()
    target_dir.mkdir(parents=True, exist_ok=True)
    target_path = target_dir / binary_name
    shutil.copy2(script_path, target_path)
    _make_executable(target_path)
    print(f"Installed CLI to {target_path}")
    if not _dir_on_path(target_dir):
        print(
            f'Note: {target_dir} is not currently on PATH. Add `export PATH="{target_dir}:$PATH"` to your shell profile.'
        )
    return target_path


def _install_skill_files(script_path: Path, skill_dir: str, skill_name: str):
    """Install skill binary and metadata."""
    target_dir = Path(skill_dir).expanduser()
    target_dir.mkdir(parents=True, exist_ok=True)
    skill_binary = target_dir / skill_name
    shutil.copy2(script_path, skill_binary)
    _make_executable(skill_binary)
    skill_metadata = script_path.with_name("SKILL.md")
    if skill_metadata.exists():
        shutil.copy2(skill_metadata, target_dir / "SKILL.md")
    else:
        print(f"Warning: {skill_metadata} not found; skipping SKILL metadata copy.")
    print(f"Installed Claude skill files to {target_dir}")


def _ensure_cleanup_settings():
    """Ensure cleanupPeriodDays is set to a high retention value."""
    settings_path = Path.home() / ".claude" / "settings.json"
    settings_path.parent.mkdir(parents=True, exist_ok=True)
    data = {}
    if settings_path.exists():
        try:
            data = json.loads(settings_path.read_text())
        except json.JSONDecodeError:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            backup_path = settings_path.with_name(f"{settings_path.name}.{timestamp}.bak")
            settings_path.rename(backup_path)
            print(f"Moved invalid settings file to {backup_path}")
            data = {}

    previous_value = data.get("cleanupPeriodDays")
    data["cleanupPeriodDays"] = DEFAULT_CLEANUP_DAYS
    settings_path.write_text(pretty_json(data) + "\n")

    if previous_value == DEFAULT_CLEANUP_DAYS:
        print(f"{settings_path} already had cleanupPeriodDays={DEFAULT_CLEANUP_DAYS}")
    else:
        print(f"Updated {settings_path} (cleanupPeriodDays={DEFAULT_CLEANUP_DAYS})")


def _dispatch_install(args):
    """Handle install command."""
    script_path = Path(__file__).resolve()
    did_work = False

    if not getattr(args, "skip_cli", False):
        _install_cli_binary(script_path, args.bin_dir, args.cli_name)
        did_work = True
    else:
        print("Skipped CLI installation (--skip-cli).")

    if not getattr(args, "skip_skill", False):
        _install_skill_files(script_path, args.skill_dir, args.skill_name)
        did_work = True
    else:
        print("Skipped Claude skill installation (--skip-skill).")

    if not getattr(args, "skip_settings", False):
        _ensure_cleanup_settings()
    else:
        print("Skipped settings update (--skip-settings).")

    if not did_work:
        print("No files were installed.")


def _dispatch_command(args):
    """Dispatch parsed arguments to appropriate command handler.

    Args:
        args: Parsed argparse.Namespace object
    """
    if args.command == "lsw":
        _dispatch_lsw(args)
    elif args.command == "lss":
        _dispatch_lss(args)
    elif args.command == "lsh":
        cmd_lsh(args)
    elif args.command == "export":
        _dispatch_export(args)
    elif args.command == "alias":
        _dispatch_alias(args)
    elif args.command == "stats":
        _dispatch_stats(args)
    elif args.command == "reset":
        cmd_reset(args)
    elif args.command == "install":
        _dispatch_install(args)
    elif args.command == "gemini-index":
        cmd_gemini_index(args)


def _extract_agent_override(argv: list) -> tuple:
    """Extract --agent from argv regardless of position.

    Returns:
        Tuple of (agent_override or None, remaining_args)
    """
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument(
        "--agent",
        choices=["auto", "claude", "codex", "gemini"],
        default=None,
    )
    args, remaining = parser.parse_known_args(argv)
    return args.agent, remaining


def main():
    """Main entry point."""
    _setup_windows_encoding()
    agent_override, remaining = _extract_agent_override(sys.argv[1:])
    parser = _create_argument_parser()
    args = parser.parse_args(remaining)
    if agent_override is not None:
        args.agent = agent_override
    if not hasattr(args, "command") or args.command is None:
        parser.print_help()
        sys.exit(0)

    # Progressively build Gemini hashâ†’path index from current directory
    # This learns the mapping as the user runs agent-history from different directories
    try:
        gemini_update_hash_index_from_cwd()
    except Exception:
        pass  # Non-critical, don't fail if index update fails

    _dispatch_command(args)


if __name__ == "__main__":
    try:
        main()
    except BrokenPipeError:
        # Downstream closed the pipe (for example: `... | head`); exit cleanly.
        sys.exit(0)
    except OSError as e:
        # Windows sometimes reports broken pipes as EINVAL during flush.
        if e.errno in (errno.EPIPE, errno.EINVAL):
            sys.exit(0)
        sys.stderr.write(f"\nError: {e}\n")
        sys.exit(1)
    except KeyboardInterrupt:
        sys.stderr.write("\\nInterrupted\\n")
        sys.exit(130)
    except Exception as e:
        sys.stderr.write(f"\\nError: {e}\\n")
        sys.exit(1)
