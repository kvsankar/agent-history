#!/usr/bin/env python3
"""
claude-sessions - Manage and export Claude Code conversation sessions by workspace

A tool to list, filter, and export Claude Code conversation sessions from specific
workspaces, bypassing the brittle session-ID-based approach of other tools.

Author: Built with Claude Code
Version: 1.0.0
License: MIT
"""

import argparse
import base64
import json
import sys
from pathlib import Path
from datetime import datetime

__version__ = "1.1.0"


# ============================================================================
# Date Parsing
# ============================================================================

def parse_date_string(date_str):
    """
    Parse ISO date string (YYYY-MM-DD) into datetime object.
    Returns datetime object or None if parsing fails.
    """
    if not date_str:
        return None

    try:
        return datetime.strptime(date_str.strip(), '%Y-%m-%d')
    except ValueError:
        return None


# ============================================================================
# JSONL Parsing and Markdown Conversion
# ============================================================================

def decode_content(encoded_str):
    """Decode base64 content."""
    try:
        return base64.b64decode(encoded_str).decode('utf-8')
    except Exception as e:
        return f"[Error decoding content: {e}]"


def extract_content(message_obj):
    """Extract text content from message object, preserving all information."""
    content_parts = []

    if 'content' in message_obj:
        content = message_obj['content']

        # User messages have simple string content
        if isinstance(content, str):
            return content

        # Assistant messages have array of content blocks
        if isinstance(content, list):
            for block in content:
                if block.get('type') == 'text':
                    content_parts.append(block.get('text', ''))

                elif block.get('type') == 'tool_use':
                    tool_name = block.get('name', 'unknown')
                    tool_id = block.get('id', '')
                    tool_input = block.get('input', {})

                    content_parts.append(f"\n**[Tool Use: {tool_name}]**")
                    if tool_id:
                        content_parts.append(f"Tool ID: `{tool_id}`")
                    content_parts.append("\nInput:")
                    content_parts.append("```json")
                    content_parts.append(json.dumps(tool_input, indent=2))
                    content_parts.append("```\n")

                elif block.get('type') == 'tool_result':
                    tool_use_id = block.get('tool_use_id', '')
                    is_error = block.get('is_error', False)
                    result_content = block.get('content', '')

                    # Handle both string and list content in tool results
                    if isinstance(result_content, list):
                        result_text = '\n'.join(
                            item.get('text', '') if isinstance(item, dict) else str(item)
                            for item in result_content
                        )
                    else:
                        result_text = result_content

                    status = "ERROR" if is_error else "Success"
                    content_parts.append(f"\n**[Tool Result: {status}]**")
                    if tool_use_id:
                        content_parts.append(f"Tool Use ID: `{tool_use_id}`")
                    content_parts.append("\n```")
                    content_parts.append(result_text)
                    content_parts.append("```\n")

    return '\n'.join(content_parts) if content_parts else '[No content]'


def get_first_timestamp(jsonl_file: Path) -> str:
    """Extract the first message timestamp from a .jsonl file."""
    try:
        with open(jsonl_file) as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    entry_type = entry.get('type')
                    if entry_type in ('user', 'assistant'):
                        timestamp = entry.get('timestamp', '')
                        if timestamp:
                            return timestamp
                except json.JSONDecodeError:
                    continue
    except Exception:
        pass
    return None


def estimate_message_lines(msg_content: str, has_metadata: bool) -> int:
    """Estimate number of lines a message will take in markdown."""
    lines = 0
    lines += 1  # Message header (## Message N)
    lines += 1  # Empty line
    lines += 1  # Timestamp
    lines += 1  # Empty line
    lines += len(msg_content.split('\n'))  # Content
    lines += 1  # Empty line
    if has_metadata:
        lines += 20  # Approximate metadata section
        lines += 1  # Empty line
    lines += 1  # Separator (---)
    lines += 1  # Empty line
    return lines


def is_tool_result_message(msg_content: str) -> bool:
    """Check if message content contains a tool result."""
    return '**[Tool Result:' in msg_content


def calculate_time_gap(msg1, msg2) -> float:
    """Calculate time gap in seconds between two messages."""
    try:
        from datetime import datetime
        ts1 = datetime.fromisoformat(msg1['timestamp'].replace('Z', '+00:00'))
        ts2 = datetime.fromisoformat(msg2['timestamp'].replace('Z', '+00:00'))
        return abs((ts2 - ts1).total_seconds())
    except:
        return 0


def find_best_split_point(messages, target_lines: int, minimal: bool) -> int:
    """Find the best message index to split at, near target_lines.

    Returns message index to start part 2 (exclusive end of part 1).
    Returns None if no split needed.
    """
    min_lines = int(target_lines * 0.8)  # 80% of target
    max_lines = int(target_lines * 1.3)  # 130% of target (more flexible)

    current_lines = 30  # Approximate header lines
    best_split = None
    best_score = -1

    for i in range(len(messages)):
        msg = messages[i]
        msg_lines = estimate_message_lines(msg['content'], not minimal)
        current_lines += msg_lines

        # If we're past max_lines, must break before this message
        if current_lines > max_lines:
            return i if i > 0 else 1

        # If we're in the sweet spot, evaluate this as a break point
        if current_lines >= min_lines:
            score = 0

            # Priority 1: Next message is User (best - starting new topic)
            if i + 1 < len(messages) and messages[i + 1]['role'] == 'user':
                score += 100

            # Priority 2: Current message is tool result (action complete)
            if is_tool_result_message(msg['content']):
                score += 50

            # Priority 3: Time gap > 5 minutes before next message
            if i + 1 < len(messages):
                time_gap = calculate_time_gap(msg, messages[i + 1])
                if time_gap > 300:  # 5 minutes
                    score += 30
                elif time_gap > 60:  # 1 minute
                    score += 10

            # Slight preference for being closer to target
            distance_from_target = abs(current_lines - target_lines)
            score -= distance_from_target * 0.05

            if score > best_score:
                best_score = score
                best_split = i + 1  # Split after this message

    return best_split


def read_jsonl_messages(jsonl_file: Path):
    """Read and parse messages from a JSONL file.

    Returns:
        List of message dictionaries with role, content, timestamp, and metadata.
    """
    messages = []

    with open(jsonl_file) as f:
        for line in f:
            try:
                entry = json.loads(line)

                # Look for user or assistant message types
                entry_type = entry.get('type')
                if entry_type in ('user', 'assistant'):
                    message_obj = entry.get('message', {})
                    role = message_obj.get('role', entry_type)
                    timestamp = entry.get('timestamp', '')
                    content = extract_content(message_obj)

                    # Preserve all metadata from the entry
                    messages.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp,
                        'uuid': entry.get('uuid', ''),
                        'parentUuid': entry.get('parentUuid'),
                        'sessionId': entry.get('sessionId', ''),
                        'agentId': entry.get('agentId'),
                        'requestId': entry.get('requestId'),
                        'cwd': entry.get('cwd', ''),
                        'version': entry.get('version', ''),
                        'gitBranch': entry.get('gitBranch'),
                        'isSidechain': entry.get('isSidechain'),
                        'userType': entry.get('userType'),
                        'model': message_obj.get('model'),
                        'usage': message_obj.get('usage'),
                        'stop_reason': message_obj.get('stop_reason'),
                        'stop_sequence': message_obj.get('stop_sequence'),
                    })

            except json.JSONDecodeError as e:
                print(f"Warning: Couldn't parse line: {e}", file=sys.stderr)
                continue

    return messages


def parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False) -> str:
    """Parse a .jsonl file and convert to markdown.

    Args:
        jsonl_file: Path to the JSONL file to convert
        minimal: If True, omit metadata and only include conversation content
    """
    messages = read_jsonl_messages(jsonl_file)

    # Check if this is an agent conversation
    is_agent = any(msg.get('isSidechain') for msg in messages)
    parent_session_id = None
    agent_id = None
    if is_agent and messages:
        parent_session_id = messages[0].get('sessionId')
        agent_id = messages[0].get('agentId')

    # Generate markdown
    md_lines = []

    # Add title with agent indicator
    if is_agent:
        md_lines.append(f"# Claude Conversation (Agent)")
    else:
        md_lines.append(f"# Claude Conversation")

    md_lines.append(f"")
    md_lines.append(f"**File:** {jsonl_file.name}")
    md_lines.append(f"**Messages:** {len(messages)}")

    if messages:
        first_ts = messages[0]['timestamp']
        last_ts = messages[-1]['timestamp']
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append(f"")
        md_lines.append("> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.")
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append(f"")
    md_lines.append("---")
    md_lines.append(f"")

    # Build UUID to message index map for internal linking
    uuid_to_index = {}
    for i, msg in enumerate(messages, 1):
        if msg.get('uuid'):
            uuid_to_index[msg['uuid']] = i

    for i, msg in enumerate(messages, 1):
        # Determine message label based on context
        if is_agent and i == 1 and msg['role'] == 'user':
            # First message in agent conversation is the task prompt from parent Claude
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg['role'] == 'user' else "ðŸ¤–"
            role_label = msg['role'].title()

        # Add HTML anchor for this message's UUID (skip in minimal mode)
        if not minimal and msg.get('uuid'):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append(f"")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append(f"")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append(f"")
        md_lines.append(msg['content'])
        md_lines.append(f"")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            # Core identifiers with navigation links
            if msg.get('uuid'):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get('parentUuid'):
                parent_uuid = msg['parentUuid']
                # Check if parent is in the same file
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*")
                else:
                    # Parent is in a different file (e.g., main session when this is an agent)
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get('sessionId'):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get('agentId'):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get('requestId'):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            # Environment info
            if msg.get('cwd'):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get('gitBranch'):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get('version'):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            # User/session info
            if msg.get('userType'):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get('isSidechain') is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            # Model info (for assistant messages)
            if msg.get('model'):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get('stop_reason'):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get('stop_sequence'):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            # Usage stats (for assistant messages)
            if msg.get('usage'):
                usage = msg['usage']
                md_lines.append(f"- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get('cache_creation_input_tokens'):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get('cache_read_input_tokens'):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append(f"")
        md_lines.append("---")
        md_lines.append(f"")

    return "\n".join(md_lines)


def generate_markdown_parts(messages, jsonl_file: Path, minimal: bool, split_lines: int):
    """Generate multiple markdown parts from messages, split at smart break points.

    Returns list of (part_num, total_parts, markdown_content, start_msg, end_msg) tuples.
    """
    if not split_lines or len(messages) == 0:
        return None

    # Find all split points
    split_points = [0]  # Start with message 0
    remaining_messages = messages

    while True:
        # Find next split point in remaining messages
        split_idx = find_best_split_point(remaining_messages, split_lines, minimal)

        if split_idx is None or split_idx >= len(remaining_messages):
            # No more splits needed
            break

        # Add this split point (adjusted for global message index)
        global_idx = split_points[-1] + split_idx
        split_points.append(global_idx)
        remaining_messages = messages[global_idx:]

    # Add end point
    split_points.append(len(messages))

    # If only one part, no splitting needed
    if len(split_points) <= 2:
        return None

    total_parts = len(split_points) - 1
    parts = []

    for part_num in range(total_parts):
        start_idx = split_points[part_num]
        end_idx = split_points[part_num + 1]
        part_messages = messages[start_idx:end_idx]

        # Generate markdown for this part
        part_md = generate_markdown_for_messages(
            part_messages, jsonl_file, minimal,
            part_num=part_num + 1, total_parts=total_parts,
            start_msg_num=start_idx + 1, end_msg_num=end_idx
        )

        parts.append((part_num + 1, total_parts, part_md, start_idx + 1, end_idx))

    return parts


def generate_markdown_for_messages(part_messages, jsonl_file, minimal, part_num=None, total_parts=None, start_msg_num=1, end_msg_num=None):
    """Generate markdown for a subset of messages (used for splitting)."""
    # Similar logic to parse_jsonl_to_markdown but for a message subset
    # This is a simplified version - we'll call the full function and then extract messages

    # Check if this is an agent conversation
    is_agent = any(msg.get('isSidechain') for msg in part_messages)
    parent_session_id = None
    agent_id = None
    if is_agent and part_messages:
        parent_session_id = part_messages[0].get('sessionId')
        agent_id = part_messages[0].get('agentId')

    md_lines = []

    # Add title with part indicator
    if part_num and total_parts:
        if is_agent:
            md_lines.append(f"# Claude Conversation (Agent) - Part {part_num} of {total_parts}")
        else:
            md_lines.append(f"# Claude Conversation - Part {part_num} of {total_parts}")
    else:
        if is_agent:
            md_lines.append(f"# Claude Conversation (Agent)")
        else:
            md_lines.append(f"# Claude Conversation")

    md_lines.append(f"")
    md_lines.append(f"**File:** {jsonl_file.name}")

    if part_num and total_parts:
        md_lines.append(f"**Part:** {part_num} of {total_parts}")
        md_lines.append(f"**Messages in this part:** {len(part_messages)} (#{start_msg_num}-#{end_msg_num})")
    else:
        md_lines.append(f"**Messages:** {len(part_messages)}")

    if part_messages:
        first_ts = part_messages[0]['timestamp']
        last_ts = part_messages[-1]['timestamp']
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append(f"")
        md_lines.append("> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.")
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append(f"")
    md_lines.append("---")
    md_lines.append(f"")

    # Build UUID to message index map
    uuid_to_index = {}
    for i, msg in enumerate(part_messages, start_msg_num):
        if msg.get('uuid'):
            uuid_to_index[msg['uuid']] = i

    # Generate messages
    for i, msg in enumerate(part_messages, start_msg_num):
        # Determine message label
        if is_agent and i == start_msg_num and msg['role'] == 'user':
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg['role'] == 'user' else "ðŸ¤–"
            role_label = msg['role'].title()

        # Add HTML anchor (skip in minimal mode)
        if not minimal and msg.get('uuid'):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append(f"")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append(f"")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append(f"")
        md_lines.append(msg['content'])
        md_lines.append(f"")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            if msg.get('uuid'):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get('parentUuid'):
                parent_uuid = msg['parentUuid']
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*")
                else:
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get('sessionId'):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get('agentId'):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get('requestId'):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            if msg.get('cwd'):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get('gitBranch'):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get('version'):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            if msg.get('userType'):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get('isSidechain') is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            if msg.get('model'):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get('stop_reason'):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get('stop_sequence'):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            if msg.get('usage'):
                usage = msg['usage']
                md_lines.append(f"- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get('cache_creation_input_tokens'):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get('cache_read_input_tokens'):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append(f"")
        md_lines.append("---")
        md_lines.append(f"")

    return "\n".join(md_lines)


# ============================================================================
# Workspace Scanning
# ============================================================================

def get_claude_projects_dir():
    """Get the Claude projects directory, with error handling."""
    projects_dir = Path.home() / ".claude" / "projects"

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found at {projects_dir}\n")
        sys.exit(1)

    return projects_dir


def normalize_workspace_name(workspace_dir_name: str, verify_local: bool = True) -> str:
    """Convert workspace directory name to readable path.

    Args:
        workspace_dir_name: Encoded workspace name (e.g., '-home-user-my-project')
        verify_local: If True, verify against local filesystem to handle dashes correctly

    Returns:
        Decoded path (e.g., 'home/user/my-project')
    """
    # Remove leading dash
    if workspace_dir_name.startswith('-'):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # If not verifying, just replace all dashes (legacy behavior)
    if not verify_local:
        return '/' + encoded.replace('-', '/')

    # Try to reconstruct the original path by checking filesystem
    # Split by dashes and try to build the path, preferring longer matches
    parts = encoded.split('-')
    path_segments = []
    i = 0

    while i < len(parts):
        # Try progressively longer combinations
        best_match = None
        best_match_len = 0

        for j in range(i + 1, len(parts) + 1):
            # Try combining parts[i:j] with dashes
            candidate_segment = '-'.join(parts[i:j])
            test_path = '/' + '/'.join(path_segments + [candidate_segment])

            if Path(test_path).exists():
                # This path exists - remember it as a candidate
                best_match = candidate_segment
                best_match_len = j - i
                # Keep trying longer matches

        if best_match:
            # Use the longest match that exists
            path_segments.append(best_match)
            i += best_match_len
        else:
            # No match found, use single part and move on
            path_segments.append(parts[i])
            i += 1

    # If we couldn't build a valid path, fall back to simple replacement
    if len(path_segments) == 0:
        return '/' + encoded.replace('-', '/')

    return '/' + '/'.join(path_segments)


def get_current_workspace_pattern():
    """
    Detect the current workspace based on the current working directory.
    Returns a pattern that can be used to match the workspace directory.
    """
    cwd = Path.cwd()
    # Convert path to the format used in workspace directory names
    # e.g., /home/user/projects/myapp -> home-user-projects-myapp
    workspace_pattern = str(cwd).lstrip('/').replace('/', '-')
    return workspace_pattern


def get_workspace_sessions(workspace_pattern: str, quiet: bool = False, since_date=None, until_date=None):
    """Find all sessions in workspaces matching the pattern.

    Args:
        workspace_pattern: Pattern to match workspace names
        quiet: Suppress output if True
        since_date: Only include sessions modified on or after this date (datetime object)
        until_date: Only include sessions modified on or before this date (datetime object)
    """
    projects_dir = get_claude_projects_dir()
    sessions = []

    # If pattern is empty, "*", or "all", match everything
    match_all = workspace_pattern in ("", "*", "all")

    # Scan each workspace directory
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue

        # Check if workspace path matches our pattern
        if match_all or workspace_pattern in workspace_dir.name:
            readable_name = normalize_workspace_name(workspace_dir.name)

            # Get all .jsonl files in this workspace
            for jsonl_file in workspace_dir.glob("*.jsonl"):
                # Get file stats
                stat = jsonl_file.stat()
                size_kb = stat.st_size / 1024
                modified = datetime.fromtimestamp(stat.st_mtime)
                modified_date = modified.replace(hour=0, minute=0, second=0, microsecond=0)

                # Check date range filter
                if since_date and modified_date < since_date:
                    continue
                if until_date and modified_date > until_date:
                    continue

                # Count messages (each line is a message)
                try:
                    with open(jsonl_file) as f:
                        message_count = sum(1 for _ in f)
                except Exception as e:
                    message_count = 0

                sessions.append({
                    'workspace': workspace_dir.name,
                    'workspace_readable': readable_name,
                    'file': jsonl_file,
                    'filename': jsonl_file.name,
                    'size_kb': size_kb,
                    'modified': modified,
                    'message_count': message_count
                })

    # Sort by modification time
    sessions.sort(key=lambda s: s['modified'])

    return sessions


# ============================================================================
# Remote Fetching
# ============================================================================

def parse_remote_host(remote_spec: str) -> tuple:
    """Parse remote host specification.

    Args:
        remote_spec: Remote host in format 'user@hostname' or 'hostname'

    Returns:
        Tuple of (user, hostname, full_spec) or (None, hostname, hostname)
    """
    if '@' in remote_spec:
        user, hostname = remote_spec.split('@', 1)
        return (user, hostname, remote_spec)
    else:
        # Just hostname, SSH will use current user
        return (None, remote_spec, remote_spec)


def check_ssh_connection(remote_host: str) -> bool:
    """Check if passwordless SSH connection is possible.

    Args:
        remote_host: Full remote spec (user@hostname or hostname)

    Returns:
        True if connection successful, False otherwise
    """
    import subprocess

    try:
        # Try SSH with BatchMode (no password prompts) and short timeout
        result = subprocess.run(
            ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5',
             remote_host, 'echo ok'],
            capture_output=True,
            text=True,
            timeout=10
        )
        return result.returncode == 0 and result.stdout.strip() == 'ok'
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def get_remote_hostname(remote_host: str) -> str:
    """Extract hostname from remote spec for use in directory prefix.

    Args:
        remote_host: Remote spec (user@hostname or hostname)

    Returns:
        Hostname portion only
    """
    _, hostname, _ = parse_remote_host(remote_host)
    # Clean hostname for use in directory names (remove dots, etc.)
    return hostname.replace('.', '-')


def get_remote_workspaces_batch(remote_host: str) -> list:
    """Get all workspace info from remote in one batch operation.

    Creates and runs a script on remote to gather all info efficiently.

    Returns:
        List of dicts with workspace info including decoded paths
    """
    import subprocess
    import json

    # Create a shell script that will run on remote
    script = '''#!/bin/bash
cd ~/.claude/projects/ 2>/dev/null || exit 1

for dir in -*/ ; do
    dir=${dir%/}  # Remove trailing slash
    [ -d "$dir" ] || continue

    # Decode the workspace name by testing paths
    encoded="${dir#-}"  # Remove leading dash

    # Try to find the actual path
    IFS='-' read -ra PARTS <<< "$encoded"
    best_path=""

    # Build path by testing combinations
    current=""
    for part in "${PARTS[@]}"; do
        if [ -z "$current" ]; then
            current="$part"
        else
            # Try with dash
            test_with_dash="$current-$part"
            # Try as separate segment
            test_separate="$current/$part"

            if [ -d "/$test_with_dash" ]; then
                current="$test_with_dash"
            elif [ -d "/$current/$part" ]; then
                current="$current/$part"
            else
                current="$test_separate"
            fi
        fi
    done

    # Get session count
    session_count=$(find "$dir" -maxdepth 1 -name "*.jsonl" 2>/dev/null | wc -l)

    # Output: encoded_name|decoded_path|session_count
    echo "$dir|/$current|$session_count"
done
'''

    try:
        # Run the script on remote via SSH
        result = subprocess.run(
            ['ssh', remote_host, 'bash -s'],
            input=script,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        # Parse results
        workspaces = []
        for line in result.stdout.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|')
            if len(parts) >= 3:
                workspaces.append({
                    'encoded': parts[0],
                    'decoded': parts[1],
                    'session_count': int(parts[2]) if parts[2].isdigit() else 0
                })

        return workspaces

    except Exception as e:
        return []


def normalize_remote_workspace_name(remote_host: str, workspace_dir_name: str) -> str:
    """Convert remote workspace directory name to readable path by verifying via SSH.

    Args:
        remote_host: Remote host specification
        workspace_dir_name: Encoded workspace name

    Returns:
        Decoded path
    """
    import subprocess

    # Remove leading dash
    if workspace_dir_name.startswith('-'):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # Generate possible decodings by trying different dash positions
    parts = encoded.split('-')

    # Generate all reasonable path combinations (limit to avoid exponential explosion)
    # We'll try keeping dashes together in common patterns
    candidates = []

    # Strategy: Build path greedily, trying longer segments first
    def generate_paths(parts, start_idx, current_path):
        if start_idx >= len(parts):
            candidates.append('/'.join(current_path))
            return

        # Try combining 1 to 3 parts (limit to keep it reasonable)
        for length in range(1, min(4, len(parts) - start_idx + 1)):
            segment = '-'.join(parts[start_idx:start_idx + length])
            generate_paths(parts, start_idx + length, current_path + [segment])

    generate_paths(parts, 0, [])

    # Limit candidates to reasonable number
    candidates = candidates[:20]

    # Test all candidates in ONE SSH call
    test_commands = ' || '.join([f'test -e "/{path}" && echo "/{path}"' for path in candidates])

    try:
        result = subprocess.run(
            ['ssh', remote_host, test_commands],
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.stdout.strip():
            # Return the first (longest) match
            return result.stdout.strip().split('\n')[0]
    except:
        pass

    # Fallback to simple replacement
    return '/' + encoded.replace('-', '/')


def list_remote_workspaces(remote_host: str) -> list:
    """List workspace directories on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of workspace directory names (e.g., ['-home-user-project', ...])
    """
    import subprocess

    try:
        # List directories in remote ~/.claude/projects/ (simple and fast)
        result = subprocess.run(
            ['ssh', remote_host,
             'ls -1 ~/.claude/projects/ | grep "^-"'],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        # Parse output - one directory name per line
        workspaces = [line.strip() for line in result.stdout.strip().split('\n') if line.strip()]
        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_remote_session_info(remote_host: str, remote_workspace: str) -> list:
    """Get session file information from remote workspace without downloading.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count
    """
    import subprocess
    import json

    try:
        # Get file stats from remote using find and stat
        # Output format: filename|size_bytes|mtime_epoch|line_count
        cmd = f'''cd ~/.claude/projects/{remote_workspace} && \
                  for f in *.jsonl; do \
                      [ -f "$f" ] || continue; \
                      size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null); \
                      mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null); \
                      lines=$(wc -l < "$f"); \
                      echo "$f|$size|$mtime|$lines"; \
                  done'''

        result = subprocess.run(
            ['ssh', remote_host, cmd],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split('\n'):
            if not line or '|' not in line:
                continue

            parts = line.split('|')
            if len(parts) != 4:
                continue

            filename, size_bytes, mtime_epoch, line_count = parts

            try:
                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)

                sessions.append({
                    'filename': filename,
                    'size_kb': size_kb,
                    'modified': modified,
                    'message_count': message_count
                })
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def fetch_workspace_files(remote_host: str, remote_workspace: str,
                          local_projects_dir: Path, hostname: str) -> dict:
    """Fetch all files from a remote workspace using rsync.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name (e.g., '-home-user-project')
        local_projects_dir: Local ~/.claude/projects/ directory
        hostname: Clean hostname for prefix

    Returns:
        Dict with stats: {'success': bool, 'files_copied': int, 'bytes': int}
    """
    import subprocess

    # Generate local directory name with remote prefix
    local_workspace = f"-remote-{hostname}{remote_workspace}"
    local_dir = local_projects_dir / local_workspace

    # Create local directory
    local_dir.mkdir(parents=True, exist_ok=True)

    # Build rsync command
    # -a: archive mode (preserves timestamps, permissions, etc.)
    # -v: verbose
    # -h: human-readable
    # --include='*.jsonl': only sync .jsonl files
    # --exclude='*': exclude everything else
    remote_path = f"{remote_host}:~/.claude/projects/{remote_workspace}/"

    try:
        result = subprocess.run(
            ['rsync', '-avh',
             '--include=*.jsonl', '--exclude=*',
             remote_path, str(local_dir) + '/'],
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )

        if result.returncode != 0:
            return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': result.stderr}

        # Parse rsync output to count files
        # Look for lines that don't start with special characters
        lines = result.stdout.split('\n')
        files_copied = sum(1 for line in lines if line.strip() and
                          not line.startswith(('sending', 'sent', 'total', 'building')))

        return {
            'success': True,
            'files_copied': files_copied,
            'local_dir': local_workspace,
            'output': result.stdout
        }

    except subprocess.TimeoutExpired:
        return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': 'Timeout'}
    except FileNotFoundError:
        return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': 'rsync not found'}


# ============================================================================
# Commands
# ============================================================================

def cmd_list(args):
    """List sessions for a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, 'remote', None)

    # Parse date filters
    since_str = getattr(args, 'since', None)
    until_str = getattr(args, 'until', None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    # Handle remote listing
    if remote_host:
        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        workspace_pattern = args.workspace
        workspaces_only = getattr(args, 'workspaces_only', False)

        # Use fast batch approach for workspaces-only mode
        if workspaces_only:
            batch_workspaces = get_remote_workspaces_batch(remote_host)

            if not batch_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by pattern
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                batch_workspaces = [ws for ws in batch_workspaces if workspace_pattern in ws['encoded']]

            if not batch_workspaces:
                sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
                sys.exit(1)

            # Just print workspace names, one per line
            for ws_info in sorted(batch_workspaces, key=lambda x: x['decoded']):
                print(ws_info['decoded'])

        else:
            # Detailed mode - get full session info
            remote_workspaces = list_remote_workspaces(remote_host)

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by pattern
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                remote_workspaces = [ws for ws in remote_workspaces if workspace_pattern in ws]

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
                sys.exit(1)

            # Get session info for each workspace
            sessions = []

            for remote_workspace in remote_workspaces:
                readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)

                # Get session files from this workspace
                workspace_sessions = get_remote_session_info(remote_host, remote_workspace)

                for session_info in workspace_sessions:
                    # Apply date filtering
                    modified_date = session_info['modified'].replace(hour=0, minute=0, second=0, microsecond=0)
                    if since_date and modified_date < since_date:
                        continue
                    if until_date and modified_date > until_date:
                        continue

                    sessions.append({
                        'workspace': remote_workspace,
                        'workspace_readable': readable_name,
                        'filename': session_info['filename'],
                        'size_kb': session_info['size_kb'],
                        'modified': session_info['modified'],
                        'message_count': session_info['message_count']
                    })

            if not sessions:
                sys.stderr.write(f"Error: No sessions found\n")
                sys.exit(1)

            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in sessions:
                print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")

    else:
        # Local listing
        sessions = get_workspace_sessions(args.workspace, since_date=since_date, until_date=until_date)

        if not sessions:
            sys.stderr.write(f"Error: No sessions found matching '{args.workspace}'\n")
            sys.exit(1)

        # Check if workspaces-only mode
        workspaces_only = getattr(args, 'workspaces_only', False)

        if workspaces_only:
            # Just print workspace names, one per line
            workspaces = set()
            for session in sessions:
                workspaces.add(session['workspace_readable'])

            for ws in sorted(workspaces):
                print(ws)

        else:
            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in sessions:
                print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")


def cmd_convert(args):
    """Convert a single .jsonl file to markdown."""
    remote_host = getattr(args, 'remote', None)

    if remote_host:
        # Handle remote file conversion
        import subprocess
        import tempfile

        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Download file to temporary location
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            # Use scp to download the file
            result = subprocess.run(
                ['scp', f"{remote_host}:{args.jsonl_file}", str(tmp_path)],
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode != 0:
                sys.stderr.write(f"Error downloading file: {result.stderr}\n")
                tmp_path.unlink()
                sys.exit(1)

            # Convert the downloaded file
            jsonl_file = tmp_path
            filename = Path(args.jsonl_file).name
            output_file = Path(args.output) if args.output else Path(filename).with_suffix('.md')

            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown)

            print(output_file)

        finally:
            # Clean up temporary file
            if tmp_path.exists():
                tmp_path.unlink()

    else:
        # Handle local file conversion
        jsonl_file = Path(args.jsonl_file)

        if not jsonl_file.exists():
            sys.stderr.write(f"Error: {jsonl_file} not found\n")
            sys.exit(1)

        output_file = Path(args.output) if args.output else jsonl_file.with_suffix('.md')

        try:
            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown)

            print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting file: {e}\n")
            sys.exit(1)


def cmd_batch(args):
    """Batch convert all sessions from a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, 'remote', None)

    # Parse date filters
    since_str = getattr(args, 'since', None)
    until_str = getattr(args, 'until', None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    output_dir = Path(args.output_dir)

    # Handle remote export (fetch first, then export)
    if remote_host:
        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Get remote workspaces
        remote_workspaces = list_remote_workspaces(remote_host)

        if not remote_workspaces:
            sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
            sys.exit(1)

        # Filter by pattern
        workspace_pattern = args.workspace
        if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
            remote_workspaces = [ws for ws in remote_workspaces if workspace_pattern in ws]

        if not remote_workspaces:
            sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
            sys.exit(1)

        # Fetch each workspace to local cache (silent)
        local_projects_dir = get_claude_projects_dir()
        hostname = get_remote_hostname(remote_host)

        for remote_workspace in remote_workspaces:
            result = fetch_workspace_files(remote_host, remote_workspace,
                                          local_projects_dir, hostname)

            if not result['success']:
                sys.stderr.write(f"Error fetching {remote_workspace}: {result.get('error', 'Unknown error')}\n")

        # Now export from cached location
        cached_pattern = f"remote-{hostname}"
        if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
            cached_pattern += f"-{workspace_pattern}"

        sessions = get_workspace_sessions(cached_pattern, quiet=True, since_date=since_date, until_date=until_date)

    else:
        # Local export
        sessions = get_workspace_sessions(args.workspace, quiet=True, since_date=since_date, until_date=until_date)

    if not sessions:
        sys.stderr.write(f"Error: No sessions found matching '{args.workspace}'\n")
        sys.exit(1)

    output_dir.mkdir(parents=True, exist_ok=True)

    # Convert each session
    force = getattr(args, 'force', False)

    for session in sessions:
        jsonl_file = session['file']

        # Extract first timestamp and create timestamped filename
        first_ts = get_first_timestamp(jsonl_file)
        if first_ts:
            try:
                dt = datetime.fromisoformat(first_ts.replace('Z', '+00:00'))
                ts_prefix = dt.strftime('%Y%m%d%H%M%S')
                output_name = f"{ts_prefix}_{jsonl_file.stem}.md"
            except Exception:
                output_name = f"{jsonl_file.stem}.md"
        else:
            output_name = f"{jsonl_file.stem}.md"

        output_file = output_dir / output_name

        # Check if we should skip (incremental export)
        if not force and output_file.exists():
            source_mtime = jsonl_file.stat().st_mtime
            output_mtime = output_file.stat().st_mtime

            if output_mtime >= source_mtime:
                continue

        try:
            minimal = getattr(args, 'minimal', False)
            split_lines = getattr(args, 'split', None)

            # Read messages from JSONL
            messages = read_jsonl_messages(jsonl_file)

            # Check if we should split into multiple parts
            if split_lines and len(messages) > 0:
                parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)

                if parts:
                    # Save multiple part files
                    for part_num, total_parts, part_md, start_msg, end_msg in parts:
                        base_name = output_name.rsplit('.md', 1)[0]
                        part_filename = f"{base_name}_part{part_num}.md"
                        part_file = output_dir / part_filename

                        # Add navigation footer (remove emoji)
                        nav_lines = ["\n---\n"]
                        nav_parts = [f"**Part {part_num} of {total_parts}**"]

                        if part_num > 1:
                            prev_filename = f"{base_name}_part{part_num - 1}.md"
                            nav_parts.append(f"[â† Part {part_num - 1}]({prev_filename})")

                        if part_num < total_parts:
                            next_filename = f"{base_name}_part{part_num + 1}.md"
                            nav_parts.append(f"[Part {part_num + 1} â†’]({next_filename})")

                        nav_lines.append("> " + " | ".join(nav_parts))

                        # Write part file with navigation
                        part_file.write_text(part_md + "\n".join(nav_lines))

                    # Print all part files
                    for part_num in range(1, total_parts + 1):
                        part_filename = f"{base_name}_part{part_num}.md"
                        print(output_dir / part_filename)
                else:
                    # Splitting not needed, use single file
                    markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                    output_file.write_text(markdown)
                    print(output_file)
            else:
                # No splitting, use single file
                markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                output_file.write_text(markdown)
                print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting {jsonl_file.name}: {e}\n")


def cmd_version(args):
    """Show version information."""
    print(f"claude-sessions {__version__}")


def cmd_fetch(args):
    """Fetch remote conversation sessions via SSH."""
    remote_host = args.remote_host
    workspace_pattern = getattr(args, 'pattern', None) or ''

    # Check SSH connectivity
    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    # Get hostname for directory prefix
    hostname = get_remote_hostname(remote_host)

    # List remote workspaces
    remote_workspaces = list_remote_workspaces(remote_host)

    if not remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    # Filter by pattern if specified
    if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
        filtered = [ws for ws in remote_workspaces if workspace_pattern in ws]
        if not filtered:
            sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
            sys.exit(1)
        remote_workspaces = filtered

    # Get local projects directory
    local_projects_dir = get_claude_projects_dir()

    # Fetch each workspace (silent on success)
    for remote_workspace in remote_workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        local_name = f"remote-{hostname}{remote_workspace}"

        result = fetch_workspace_files(remote_host, remote_workspace,
                                       local_projects_dir, hostname)

        if result['success']:
            # Print the local path where it was cached
            print(normalize_workspace_name(local_name, verify_local=False))
        else:
            error = result.get('error', 'Unknown error')
            sys.stderr.write(f"Error fetching {readable_name}: {error}\n")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        prog='claude-history',
        description='Browse and export Claude Code conversation history',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
EXAMPLES:

  List workspaces:
    claude-history lsw                        # all local workspaces
    claude-history lsw myproject              # filter by pattern
    claude-history lsw -r user@server         # remote workspaces

  List sessions:
    claude-history lss                        # current workspace
    claude-history lss myproject              # specific workspace
    claude-history lss myproject -r user@server    # remote sessions

  Export:
    claude-history export myproject           # export workspace
    claude-history export -a                  # export all workspaces
    claude-history export file.jsonl         # export single file
    claude-history export myproject -r user@server  # remote export

  Date filtering:
    claude-history lss myproject --since 2025-11-01
    claude-history export myproject --since 2025-11-01 --until 2025-11-30

  Export options:
    claude-history export myproject --minimal       # minimal mode
    claude-history export myproject --split 500     # split long conversations
    claude-history export myproject ./output        # custom output directory
        '''
    )

    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')

    # Create subparsers for commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
    subparsers.required = True

    # ========== lsw - list workspaces ==========
    parser_lsw = subparsers.add_parser('lsw',
        help='List workspaces',
        description='List all workspaces (local or remote)')

    parser_lsw.add_argument('pattern',
        nargs='?',
        help='Optional pattern to filter workspaces')

    parser_lsw.add_argument('-r', '--remote',
        metavar='HOST',
        help='List workspaces from remote host via SSH (user@hostname or hostname)')

    # ========== lss - list sessions ==========
    parser_lss = subparsers.add_parser('lss',
        help='List sessions',
        description='List sessions in a workspace (defaults to current workspace)')

    parser_lss.add_argument('workspace',
        nargs='?',
        help='Workspace name (default: current workspace)')

    parser_lss.add_argument('-r', '--remote',
        metavar='HOST',
        help='List sessions from remote host via SSH (user@hostname or hostname)')

    parser_lss.add_argument('--since',
        metavar='DATE',
        help='Only include sessions modified on or after this date (YYYY-MM-DD)')

    parser_lss.add_argument('--until',
        metavar='DATE',
        help='Only include sessions modified on or before this date (YYYY-MM-DD)')

    # ========== export command ==========
    parser_export = subparsers.add_parser('export',
        help='Export to markdown',
        description='Export workspace or single file to markdown')

    parser_export.add_argument('target',
        help='Workspace name or file.jsonl (detects file by .jsonl extension)')

    parser_export.add_argument('output_dir',
        nargs='?',
        default='./claude-conversations',
        help='Output directory (default: ./claude-conversations)')

    parser_export.add_argument('-a', '--all',
        action='store_true',
        help='Export all workspaces (target is ignored)')

    parser_export.add_argument('-r', '--remote',
        metavar='HOST',
        help='Export from remote host via SSH (user@hostname or hostname)')

    parser_export.add_argument('--force',
        action='store_true',
        help='Force re-export (default: incremental)')

    parser_export.add_argument('--minimal',
        action='store_true',
        help='Minimal export: omit metadata, keep only conversation content')

    parser_export.add_argument('--split',
        metavar='LINES',
        type=int,
        help='Split long conversations into parts (e.g., --split 500)')

    parser_export.add_argument('--since',
        metavar='DATE',
        help='Only include sessions modified on or after this date (YYYY-MM-DD)')

    parser_export.add_argument('--until',
        metavar='DATE',
        help='Only include sessions modified on or before this date (YYYY-MM-DD)')

    # Parse arguments
    args = parser.parse_args()

    # Dispatch to command handlers
    if args.command == 'lsw':
        # List workspaces
        pattern = args.pattern if args.pattern else ''
        class LswArgs:
            workspace = pattern
            remote = getattr(args, 'remote', None)
            workspaces_only = True  # lsw always lists workspaces
        cmd_list(LswArgs())

    elif args.command == 'lss':
        # List sessions
        workspace_val = args.workspace if args.workspace else get_current_workspace_pattern()
        class LssArgs:
            workspace = workspace_val
            since = args.since
            until = args.until
            remote = getattr(args, 'remote', None)
            workspaces_only = False  # lss always lists sessions
        cmd_list(LssArgs())

    elif args.command == 'export':
        # Export - handle file vs workspace
        if args.all:
            # Export all workspaces
            class ExportArgs:
                workspace = ''  # empty means all
                output_dir = args.output_dir
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split
                remote = getattr(args, 'remote', None)
            cmd_batch(ExportArgs())
        elif args.target.endswith('.jsonl'):
            # Single file export
            class ConvertArgs:
                jsonl_file = args.target
                output = None  # Use default
                remote = getattr(args, 'remote', None)
            cmd_convert(ConvertArgs())
        else:
            # Workspace export
            class ExportArgs:
                workspace = args.target
                output_dir = args.output_dir
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split
                remote = getattr(args, 'remote', None)
            cmd_batch(ExportArgs())


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.stderr.write("\nInterrupted\n")
        sys.exit(130)
    except Exception as e:
        sys.stderr.write(f"\nError: {e}\n")
        sys.exit(1)
