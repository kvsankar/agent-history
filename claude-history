#!/usr/bin/env python3
"""
claude-sessions - Manage and export Claude Code conversation sessions by workspace

A tool to list, filter, and export Claude Code conversation sessions from specific
workspaces, bypassing the brittle session-ID-based approach of other tools.

Author: Built with Claude Code
Version: 1.0.0
License: MIT
"""

import argparse
import base64
import json
import re
import shlex
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path

__version__ = "1.4.1"


# ============================================================================
# Date Parsing
# ============================================================================


def parse_date_string(date_str):
    """
    Parse ISO date string (YYYY-MM-DD) into datetime object.
    Returns datetime object or None if parsing fails.
    """
    if not date_str:
        return None

    try:
        return datetime.strptime(date_str.strip(), "%Y-%m-%d")
    except ValueError:
        return None


def validate_split_lines(value):
    """
    Validate split lines argument for argparse.
    Ensures value is a positive integer.
    """
    try:
        ivalue = int(value)
        if ivalue <= 0:
            raise argparse.ArgumentTypeError(f"split value must be a positive integer, got '{value}'")
        return ivalue
    except ValueError as e:
        raise argparse.ArgumentTypeError(f"split value must be an integer, got '{value}'") from e


# ============================================================================
# Input Validation (Security)
# ============================================================================

# Pattern for valid workspace directory names (Claude Code encoded paths)
# Allow: alphanumeric, dashes, underscores, dots
# Workspace names always start with dash (Unix) or letter (Windows drive)
WORKSPACE_NAME_PATTERN = re.compile(r"^[-a-zA-Z0-9_.]+$")

# Pattern for valid SSH remote host specifications
# Allow: user@hostname, hostname, user@hostname:port, IPv4/IPv6
REMOTE_HOST_PATTERN = re.compile(
    r"^(?:[\w.-]+@)?"  # Optional user@
    r"(?:[\w.-]+|\[[0-9a-fA-F:]+\])"  # Hostname or IPv6 in brackets
    r"(?::\d+)?$"  # Optional :port
)


def validate_workspace_name(workspace_name: str) -> bool:
    """Validate workspace name to prevent command injection.

    Workspace names from Claude Code are encoded paths like:
    - '-home-user-project' (Unix)
    - 'C--Users-name-project' (Windows)
    - 'remote_host_home-user-project' (cached remote)
    - 'wsl_Ubuntu_home-user-project' (cached WSL)

    Returns True if valid, False otherwise.
    """
    if not workspace_name:
        return False
    if len(workspace_name) > 1000:  # Reasonable limit
        return False
    return bool(WORKSPACE_NAME_PATTERN.match(workspace_name))


def validate_remote_host(remote_host: str) -> bool:
    """Validate remote host specification to prevent command injection.

    Valid formats:
    - hostname
    - user@hostname
    - user@hostname:port
    - IPv4 address
    - user@IPv6 (with brackets)

    Returns True if valid, False otherwise.
    """
    if not remote_host:
        return False
    if len(remote_host) > 255:  # DNS limit
        return False
    return bool(REMOTE_HOST_PATTERN.match(remote_host))


def sanitize_for_shell(value: str) -> str:
    """Sanitize a value for safe use in shell commands.

    Uses shlex.quote for proper escaping.
    """
    return shlex.quote(value)


# ============================================================================
# JSONL Parsing and Markdown Conversion
# ============================================================================


def decode_content(encoded_str):
    """Decode base64 content."""
    try:
        return base64.b64decode(encoded_str).decode("utf-8")
    except Exception as e:
        return f"[Error decoding content: {e}]"


def extract_content(message_obj):
    """Extract text content from message object, preserving all information."""
    content_parts = []

    if "content" in message_obj:
        content = message_obj["content"]

        # User messages have simple string content
        if isinstance(content, str):
            return content

        # Assistant messages have array of content blocks
        if isinstance(content, list):
            for block in content:
                if block.get("type") == "text":
                    content_parts.append(block.get("text", ""))

                elif block.get("type") == "tool_use":
                    tool_name = block.get("name", "unknown")
                    tool_id = block.get("id", "")
                    tool_input = block.get("input", {})

                    content_parts.append(f"\n**[Tool Use: {tool_name}]**")
                    if tool_id:
                        content_parts.append(f"Tool ID: `{tool_id}`")
                    content_parts.append("\nInput:")
                    content_parts.append("```json")
                    content_parts.append(json.dumps(tool_input, indent=2))
                    content_parts.append("```\n")

                elif block.get("type") == "tool_result":
                    tool_use_id = block.get("tool_use_id", "")
                    is_error = block.get("is_error", False)
                    result_content = block.get("content", "")

                    # Handle both string and list content in tool results
                    if isinstance(result_content, list):
                        result_text = "\n".join(
                            item.get("text", "") if isinstance(item, dict) else str(item) for item in result_content
                        )
                    else:
                        result_text = result_content

                    status = "ERROR" if is_error else "Success"
                    content_parts.append(f"\n**[Tool Result: {status}]**")
                    if tool_use_id:
                        content_parts.append(f"Tool Use ID: `{tool_use_id}`")
                    content_parts.append("\n```")
                    content_parts.append(result_text)
                    content_parts.append("```\n")

    return "\n".join(content_parts) if content_parts else "[No content]"


def get_first_timestamp(jsonl_file: Path) -> str:
    """Extract the first message timestamp from a .jsonl file."""
    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    entry_type = entry.get("type")
                    if entry_type in ("user", "assistant"):
                        timestamp = entry.get("timestamp", "")
                        if timestamp:
                            return timestamp
                except json.JSONDecodeError:
                    continue
    except Exception:
        pass
    return None


def estimate_message_lines(msg_content: str, has_metadata: bool) -> int:
    """Estimate number of lines a message will take in markdown."""
    lines = 0
    lines += 1  # Message header (## Message N)
    lines += 1  # Empty line
    lines += 1  # Timestamp
    lines += 1  # Empty line
    lines += len(msg_content.split("\n"))  # Content
    lines += 1  # Empty line
    if has_metadata:
        lines += 20  # Approximate metadata section
        lines += 1  # Empty line
    lines += 1  # Separator (---)
    lines += 1  # Empty line
    return lines


def is_tool_result_message(msg_content: str) -> bool:
    """Check if message content contains a tool result."""
    return "**[Tool Result:" in msg_content


def calculate_time_gap(msg1, msg2) -> float:
    """Calculate time gap in seconds between two messages."""
    try:
        ts1 = datetime.fromisoformat(msg1["timestamp"].replace("Z", "+00:00"))
        ts2 = datetime.fromisoformat(msg2["timestamp"].replace("Z", "+00:00"))
        return abs((ts2 - ts1).total_seconds())
    except (ValueError, KeyError, TypeError):
        return 0


def find_best_split_point(messages, target_lines: int, minimal: bool) -> int:
    """Find the best message index to split at, near target_lines.

    Returns message index to start part 2 (exclusive end of part 1).
    Returns None if no split needed.
    """
    min_lines = int(target_lines * 0.8)  # 80% of target
    max_lines = int(target_lines * 1.3)  # 130% of target (more flexible)

    current_lines = 30  # Approximate header lines
    best_split = None
    best_score = -1

    for i in range(len(messages)):
        msg = messages[i]
        msg_lines = estimate_message_lines(msg["content"], not minimal)
        current_lines += msg_lines

        # If we're past max_lines, must break before this message
        if current_lines > max_lines:
            return i if i > 0 else 1

        # If we're in the sweet spot, evaluate this as a break point
        if current_lines >= min_lines:
            score = 0

            # Priority 1: Next message is User (best - starting new topic)
            if i + 1 < len(messages) and messages[i + 1]["role"] == "user":
                score += 100

            # Priority 2: Current message is tool result (action complete)
            if is_tool_result_message(msg["content"]):
                score += 50

            # Priority 3: Time gap > 5 minutes before next message
            if i + 1 < len(messages):
                time_gap = calculate_time_gap(msg, messages[i + 1])
                if time_gap > 300:  # 5 minutes
                    score += 30
                elif time_gap > 60:  # 1 minute
                    score += 10

            # Slight preference for being closer to target
            distance_from_target = abs(current_lines - target_lines)
            score -= distance_from_target * 0.05

            if score > best_score:
                best_score = score
                best_split = i + 1  # Split after this message

    return best_split


def read_jsonl_messages(jsonl_file: Path):
    """Read and parse messages from a JSONL file.

    Returns:
        List of message dictionaries with role, content, timestamp, and metadata.
    """
    messages = []

    with open(jsonl_file, encoding="utf-8") as f:
        for line in f:
            try:
                entry = json.loads(line)

                # Look for user or assistant message types
                entry_type = entry.get("type")
                if entry_type in ("user", "assistant"):
                    message_obj = entry.get("message", {})
                    role = message_obj.get("role", entry_type)
                    timestamp = entry.get("timestamp", "")
                    content = extract_content(message_obj)

                    # Preserve all metadata from the entry
                    messages.append(
                        {
                            "role": role,
                            "content": content,
                            "timestamp": timestamp,
                            "uuid": entry.get("uuid", ""),
                            "parentUuid": entry.get("parentUuid"),
                            "sessionId": entry.get("sessionId", ""),
                            "agentId": entry.get("agentId"),
                            "requestId": entry.get("requestId"),
                            "cwd": entry.get("cwd", ""),
                            "version": entry.get("version", ""),
                            "gitBranch": entry.get("gitBranch"),
                            "isSidechain": entry.get("isSidechain"),
                            "userType": entry.get("userType"),
                            "model": message_obj.get("model"),
                            "usage": message_obj.get("usage"),
                            "stop_reason": message_obj.get("stop_reason"),
                            "stop_sequence": message_obj.get("stop_sequence"),
                        }
                    )

            except json.JSONDecodeError as e:
                print(f"Warning: Couldn't parse line: {e}", file=sys.stderr)
                continue

    return messages


def parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False) -> str:
    """Parse a .jsonl file and convert to markdown.

    Args:
        jsonl_file: Path to the JSONL file to convert
        minimal: If True, omit metadata and only include conversation content
    """
    messages = read_jsonl_messages(jsonl_file)

    # Check if this is an agent conversation
    is_agent = any(msg.get("isSidechain") for msg in messages)
    parent_session_id = None
    agent_id = None
    if is_agent and messages:
        parent_session_id = messages[0].get("sessionId")
        agent_id = messages[0].get("agentId")

    # Generate markdown
    md_lines = []

    # Add title with agent indicator
    if is_agent:
        md_lines.append("# Claude Conversation (Agent)")
    else:
        md_lines.append("# Claude Conversation")

    md_lines.append("")
    md_lines.append(f"**File:** {jsonl_file.name}")
    md_lines.append(f"**Messages:** {len(messages)}")

    if messages:
        first_ts = messages[0]["timestamp"]
        last_ts = messages[-1]["timestamp"]
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append("")
        md_lines.append(
            "> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation."
        )
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")

    # Build UUID to message index map for internal linking
    uuid_to_index = {}
    for i, msg in enumerate(messages, 1):
        if msg.get("uuid"):
            uuid_to_index[msg["uuid"]] = i

    for i, msg in enumerate(messages, 1):
        # Determine message label based on context
        if is_agent and i == 1 and msg["role"] == "user":
            # First message in agent conversation is the task prompt from parent Claude
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg["role"] == "user" else "ðŸ¤–"
            role_label = msg["role"].title()

        # Add HTML anchor for this message's UUID (skip in minimal mode)
        if not minimal and msg.get("uuid"):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append("")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append("")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append("")
        md_lines.append(msg["content"])
        md_lines.append("")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            # Core identifiers with navigation links
            if msg.get("uuid"):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get("parentUuid"):
                parent_uuid = msg["parentUuid"]
                # Check if parent is in the same file
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(
                        f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*"
                    )
                else:
                    # Parent is in a different file (e.g., main session when this is an agent)
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get("sessionId"):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get("agentId"):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get("requestId"):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            # Environment info
            if msg.get("cwd"):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get("gitBranch"):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get("version"):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            # User/session info
            if msg.get("userType"):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get("isSidechain") is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            # Model info (for assistant messages)
            if msg.get("model"):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get("stop_reason"):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get("stop_sequence"):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            # Usage stats (for assistant messages)
            if msg.get("usage"):
                usage = msg["usage"]
                md_lines.append("- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get("cache_creation_input_tokens"):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get("cache_read_input_tokens"):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append("")
        md_lines.append("---")
        md_lines.append("")

    return "\n".join(md_lines)


def generate_markdown_parts(messages, jsonl_file: Path, minimal: bool, split_lines: int):
    """Generate multiple markdown parts from messages, split at smart break points.

    Returns list of (part_num, total_parts, markdown_content, start_msg, end_msg) tuples.
    """
    if not split_lines or len(messages) == 0:
        return None

    # Find all split points
    split_points = [0]  # Start with message 0
    remaining_messages = messages

    while True:
        # Find next split point in remaining messages
        split_idx = find_best_split_point(remaining_messages, split_lines, minimal)

        if split_idx is None or split_idx >= len(remaining_messages):
            # No more splits needed
            break

        # Add this split point (adjusted for global message index)
        global_idx = split_points[-1] + split_idx
        split_points.append(global_idx)
        remaining_messages = messages[global_idx:]

    # Add end point
    split_points.append(len(messages))

    # If only one part, no splitting needed
    if len(split_points) <= 2:
        return None

    total_parts = len(split_points) - 1
    parts = []

    for part_num in range(total_parts):
        start_idx = split_points[part_num]
        end_idx = split_points[part_num + 1]
        part_messages = messages[start_idx:end_idx]

        # Generate markdown for this part
        part_md = generate_markdown_for_messages(
            part_messages,
            jsonl_file,
            minimal,
            part_num=part_num + 1,
            total_parts=total_parts,
            start_msg_num=start_idx + 1,
            end_msg_num=end_idx,
        )

        parts.append((part_num + 1, total_parts, part_md, start_idx + 1, end_idx))

    return parts


def generate_markdown_for_messages(
    part_messages, jsonl_file, minimal, part_num=None, total_parts=None, start_msg_num=1, end_msg_num=None
):
    """Generate markdown for a subset of messages (used for splitting)."""
    # Similar logic to parse_jsonl_to_markdown but for a message subset
    # This is a simplified version - we'll call the full function and then extract messages

    # Check if this is an agent conversation
    is_agent = any(msg.get("isSidechain") for msg in part_messages)
    parent_session_id = None
    agent_id = None
    if is_agent and part_messages:
        parent_session_id = part_messages[0].get("sessionId")
        agent_id = part_messages[0].get("agentId")

    md_lines = []

    # Add title with part indicator
    if part_num and total_parts:
        if is_agent:
            md_lines.append(f"# Claude Conversation (Agent) - Part {part_num} of {total_parts}")
        else:
            md_lines.append(f"# Claude Conversation - Part {part_num} of {total_parts}")
    else:
        if is_agent:
            md_lines.append("# Claude Conversation (Agent)")
        else:
            md_lines.append("# Claude Conversation")

    md_lines.append("")
    md_lines.append(f"**File:** {jsonl_file.name}")

    if part_num and total_parts:
        md_lines.append(f"**Part:** {part_num} of {total_parts}")
        md_lines.append(f"**Messages in this part:** {len(part_messages)} (#{start_msg_num}-#{end_msg_num})")
    else:
        md_lines.append(f"**Messages:** {len(part_messages)}")

    if part_messages:
        first_ts = part_messages[0]["timestamp"]
        last_ts = part_messages[-1]["timestamp"]
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append("")
        md_lines.append(
            "> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation."
        )
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")

    # Build UUID to message index map
    uuid_to_index = {}
    for i, msg in enumerate(part_messages, start_msg_num):
        if msg.get("uuid"):
            uuid_to_index[msg["uuid"]] = i

    # Generate messages
    for i, msg in enumerate(part_messages, start_msg_num):
        # Determine message label
        if is_agent and i == start_msg_num and msg["role"] == "user":
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg["role"] == "user" else "ðŸ¤–"
            role_label = msg["role"].title()

        # Add HTML anchor (skip in minimal mode)
        if not minimal and msg.get("uuid"):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append("")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append("")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append("")
        md_lines.append(msg["content"])
        md_lines.append("")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            if msg.get("uuid"):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get("parentUuid"):
                parent_uuid = msg["parentUuid"]
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(
                        f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*"
                    )
                else:
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get("sessionId"):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get("agentId"):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get("requestId"):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            if msg.get("cwd"):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get("gitBranch"):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get("version"):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            if msg.get("userType"):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get("isSidechain") is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            if msg.get("model"):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get("stop_reason"):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get("stop_sequence"):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            if msg.get("usage"):
                usage = msg["usage"]
                md_lines.append("- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get("cache_creation_input_tokens"):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get("cache_read_input_tokens"):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append("")
        md_lines.append("---")
        md_lines.append("")

    return "\n".join(md_lines)


# ============================================================================
# Workspace Scanning
# ============================================================================


def get_claude_projects_dir():
    """Get the Claude projects directory, with error handling."""
    projects_dir = Path.home() / ".claude" / "projects"

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found at {projects_dir}\n")
        sys.exit(1)

    return projects_dir


def path_to_encoded_workspace(path: str) -> str:
    """Convert an absolute path to Claude's encoded workspace directory name.

    Args:
        path: Absolute path (e.g., '/home/user/my-project')

    Returns:
        Encoded workspace name (e.g., '-home-user-my-project')
    """
    # Remove trailing slash if present
    path = path.rstrip("/")
    # Replace / with - and add leading -
    if path.startswith("/"):
        return "-" + path[1:].replace("/", "-")
    else:
        return "-" + path.replace("/", "-")


def resolve_workspace_input(workspace_input: str, source_key: str = "local", args=None) -> list:
    """Resolve a workspace input (pattern, path, or encoded name) to actual encoded workspace names.

    Args:
        workspace_input: Can be:
            - Encoded name: '-home-user-project'
            - Absolute path: '/home/user/project'
            - Pattern: 'project' or 'my-project'
        source_key: Source to search in ('local', 'remote:host', 'wsl:distro', 'windows')
        args: Arguments object for remote access

    Returns:
        List of matching encoded workspace names
    """
    # Case 1: Already an encoded workspace name (starts with -)
    if workspace_input.startswith("-"):
        return [workspace_input]

    # Case 2: Absolute path - convert to encoded name
    if workspace_input.startswith("/"):
        return [path_to_encoded_workspace(workspace_input)]

    # Case 3: Windows absolute path (C:\... or C:/...)
    if len(workspace_input) > 2 and workspace_input[1] == ":":
        # Convert C:\path or C:/path to encoded format
        drive = workspace_input[0].upper()
        rest = workspace_input[2:].lstrip("/\\").replace("\\", "/").replace("/", "-")
        return [f"{drive}-{rest}"]

    # Case 4: Pattern - search for matching workspaces
    # Get list of available workspaces for this source
    try:
        if source_key.startswith("wsl:"):
            distro = source_key[4:]
            projects_dir = get_wsl_projects_dir(distro)
            if not projects_dir:
                return []
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
        elif source_key.startswith("windows"):
            # source_key is 'windows' or 'windows:username'
            user = source_key[8:] if source_key.startswith("windows:") else None
            projects_dir = get_windows_projects_dir(user)
            if not projects_dir:
                return []
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
        elif source_key.startswith("remote:"):
            hostname = source_key[7:]
            remote_host = getattr(args, "remote", hostname) if args else hostname
            batch_ws = get_remote_workspaces_batch(remote_host)
            workspaces = [ws["encoded"] for ws in batch_ws] if batch_ws else []
        else:
            # Local
            projects_dir = get_claude_projects_dir()
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
    except Exception:
        return []

    # Find workspaces matching the pattern
    matches = [ws for ws in workspaces if workspace_input in ws]
    return matches


def normalize_workspace_name(workspace_dir_name: str, verify_local: bool = True, base_path: Path = None) -> str:
    """Convert workspace directory name to readable path.

    Args:
        workspace_dir_name: Encoded workspace name (e.g., '-home-user-my-project')
        verify_local: If True, verify against local filesystem to handle dashes correctly
        base_path: Base path to prepend when verifying (for WSL: //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., 'home/user/my-project')
    """
    # Handle Windows-style paths (e.g., 'C--sankar-projects-claude-history')
    # These start with a drive letter followed by double dash
    if len(workspace_dir_name) >= 3 and workspace_dir_name[1:3] == "--":
        drive_letter = workspace_dir_name[0].upper()
        rest = workspace_dir_name[3:]  # Skip 'C--'

        # For Windows paths accessed from WSL, verify against /mnt/<drive>/
        if verify_local:
            mnt_base = Path(f"/mnt/{drive_letter.lower()}")
            if mnt_base.exists():
                # Verify path segments against filesystem
                parts = rest.split("-")
                path_segments = []
                i = 0

                while i < len(parts):
                    current_path = mnt_base / "/".join(path_segments) if path_segments else mnt_base

                    # Try progressively longer combinations (longest first)
                    best_match = None
                    best_match_len = 0

                    for j in range(len(parts), i, -1):
                        candidate_segment = "-".join(parts[i:j])
                        candidate_path = current_path / candidate_segment

                        if candidate_path.exists() and candidate_path.is_dir():
                            best_match = candidate_segment
                            best_match_len = j - i
                            break

                    if best_match:
                        path_segments.append(best_match)
                        i += best_match_len
                    else:
                        path_segments.append(parts[i])
                        i += 1

                if path_segments:
                    return f"/{drive_letter}/" + "/".join(path_segments)

        # Fallback: simple replacement (may break hyphenated names)
        return f"/{drive_letter}/" + rest.replace("-", "/")

    # Remove leading dash for Unix paths
    if workspace_dir_name.startswith("-"):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # If not verifying, just replace all dashes (legacy behavior)
    if not verify_local:
        return "/" + encoded.replace("-", "/")

    # Try to reconstruct the original path by checking filesystem
    # Split by dashes and try to build the path, preferring longer matches
    parts = encoded.split("-")
    path_segments = []
    i = 0

    while i < len(parts):
        # Build current path to check children against
        if path_segments:
            current_path_str = "/" + "/".join(path_segments)
            if base_path:
                current_path = base_path / current_path_str.lstrip("/")
            else:
                current_path = Path(current_path_str)
        else:
            # Starting from root
            if base_path:
                current_path = base_path
            else:
                current_path = Path("/")

        # Try progressively longer combinations (longest first for better matching)
        best_match = None
        best_match_len = 0

        # Try from longest to shortest to prefer longer directory names with dashes
        for j in range(len(parts), i, -1):
            # Try combining parts[i:j] with dashes
            candidate_segment = "-".join(parts[i:j])

            # Check if this directory name exists as a child of current_path
            candidate_path = current_path / candidate_segment

            if candidate_path.exists() and candidate_path.is_dir():
                # This exact directory exists - use it
                best_match = candidate_segment
                best_match_len = j - i
                break  # Found the longest match, stop searching

        if best_match:
            # Use the match we found
            path_segments.append(best_match)
            i += best_match_len
        else:
            # No match found, use single part and move on
            path_segments.append(parts[i])
            i += 1

    # If we couldn't build a valid path, fall back to simple replacement
    if len(path_segments) == 0:
        return "/" + encoded.replace("-", "/")

    return "/" + "/".join(path_segments)


def get_current_workspace_pattern():
    """
    Detect the current workspace based on the current working directory.
    Returns a pattern that can be used to match the workspace directory.
    """
    cwd = Path.cwd()
    cwd_str = str(cwd)

    # Handle Windows paths (C:\path\to\project -> C--path-to-project)
    if sys.platform == "win32" and len(cwd_str) >= 2 and cwd_str[1] == ":":
        # Extract drive letter and path
        drive = cwd_str[0]
        path_part = cwd_str[2:].lstrip("\\").lstrip("/")
        # Convert backslashes and forward slashes to dashes
        path_part = path_part.replace("\\", "-").replace("/", "-")
        workspace_pattern = f"{drive}--{path_part}"
    else:
        # Unix/Linux paths (/home/user/projects/myapp -> home-user-projects-myapp)
        workspace_pattern = cwd_str.lstrip("/").replace("/", "-")

    return workspace_pattern


def check_current_workspace_exists():
    """
    Check if the current directory corresponds to a known Claude Code workspace.
    Returns (pattern, exists) tuple.
    """
    pattern = get_current_workspace_pattern()
    projects_dir = get_claude_projects_dir()

    if not projects_dir or not projects_dir.exists():
        return pattern, False

    # Check if any workspace directory matches the pattern
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue
        # Skip cached remote/wsl directories
        dirname = workspace_dir.name
        if dirname.startswith("remote_") or dirname.startswith("wsl_"):
            continue
        if pattern in dirname:
            return pattern, True

    return pattern, False


def get_workspace_sessions(
    workspace_pattern: str, quiet: bool = False, since_date=None, until_date=None, include_cached: bool = False
):
    """Find all sessions in workspaces matching the pattern.

    Args:
        workspace_pattern: Pattern to match workspace names
        quiet: Suppress output if True
        since_date: Only include sessions modified on or after this date (datetime object)
        until_date: Only include sessions modified on or before this date (datetime object)
        include_cached: If True, include remote_* and wsl_* cached workspaces (default: False)
    """
    projects_dir = get_claude_projects_dir()
    sessions = []

    # If pattern is empty, "*", or "all", match everything
    match_all = workspace_pattern in ("", "*", "all")

    # Detect if using WSL path (for proper dash handling in workspace names)
    projects_dir_str = str(projects_dir)
    is_wsl = projects_dir_str.startswith("\\\\wsl.localhost\\") or projects_dir_str.startswith("//wsl.localhost/")

    # For WSL, extract the base path for filesystem verification
    if is_wsl:
        # Extract distro name and construct base path for verification
        # Path format: //wsl.localhost/Ubuntu/home/user/.claude/projects
        # For UNC paths, parts[0] contains '\\wsl.localhost\Ubuntu\' (includes distro)
        parts = projects_dir.parts
        if len(parts) >= 1:
            # parts[0] is '\\wsl.localhost\Ubuntu\' - this is our base
            wsl_base = Path(parts[0].rstrip("\\").rstrip("/"))
        else:
            wsl_base = None
    else:
        wsl_base = None

    # Scan each workspace directory
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue

        # Skip cached remote/WSL workspaces to prevent circular fetching
        # These are already fetched data from other sources
        # Unless include_cached is True (for exporting from cached remote data)
        dir_name = workspace_dir.name
        if not include_cached:
            if (
                dir_name.startswith("remote_")
                or dir_name.startswith("wsl_")
                or dir_name.startswith("-remote-")
                or dir_name.startswith("--wsl-")
            ):
                continue

        # Check if workspace path matches our pattern
        if match_all or workspace_pattern in workspace_dir.name:
            readable_name = normalize_workspace_name(workspace_dir.name, base_path=wsl_base)

            # Get all .jsonl files in this workspace
            for jsonl_file in workspace_dir.glob("*.jsonl"):
                # Get file stats
                stat = jsonl_file.stat()
                size_kb = stat.st_size / 1024
                modified = datetime.fromtimestamp(stat.st_mtime)
                modified_date = modified.replace(hour=0, minute=0, second=0, microsecond=0)

                # Check date range filter
                if since_date and modified_date < since_date:
                    continue
                if until_date and modified_date > until_date:
                    continue

                # Count messages (each line is a message)
                try:
                    with open(jsonl_file, encoding="utf-8") as f:
                        message_count = sum(1 for _ in f)
                except Exception:
                    message_count = 0

                sessions.append(
                    {
                        "workspace": workspace_dir.name,
                        "workspace_readable": readable_name,
                        "file": jsonl_file,
                        "filename": jsonl_file.name,
                        "size_kb": size_kb,
                        "modified": modified,
                        "message_count": message_count,
                    }
                )

    # Sort by modification time
    sessions.sort(key=lambda s: s["modified"])

    return sessions


# ============================================================================
# WSL Access
# ============================================================================


def is_running_in_wsl() -> bool:
    """Detect if we're running inside WSL."""
    try:
        with open("/proc/version") as f:
            content = f.read().lower()
            return "microsoft" in content or "wsl" in content
    except OSError:
        return False


def get_windows_home_from_wsl(username: str = None):
    """
    Get Windows user home directory from WSL.

    Args:
        username: Optional Windows username. If None, uses USERPROFILE.

    Returns:
        Path to Windows home directory, or None if not found.
    """
    import subprocess

    if username:
        # Try specific username across all drives
        mnt = Path("/mnt")
        if mnt.exists():
            for drive in sorted(mnt.iterdir()):
                if drive.is_dir() and drive.name not in ["wsl", "wslg"]:
                    user_path = drive / "Users" / username
                    if user_path.exists() and (user_path / ".claude" / "projects").exists():
                        return user_path
        return None

    # Primary approach: use Windows USERPROFILE
    try:
        result = subprocess.run(
            ["cmd.exe", "/c", "echo %USERPROFILE%"], capture_output=True, text=True, check=True, timeout=5
        )
        win_path = result.stdout.strip()

        if win_path and win_path != "%USERPROFILE%":
            # Convert Windows path to WSL path
            result = subprocess.run(["wslpath", win_path], capture_output=True, text=True, check=True, timeout=5)
            wsl_path = Path(result.stdout.strip())

            if wsl_path.exists() and (wsl_path / ".claude" / "projects").exists():
                return wsl_path
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass

    # Fallback: scan all drives
    mnt = Path("/mnt")
    if mnt.exists():
        for drive in sorted(mnt.iterdir()):
            if drive.is_dir() and drive.name not in ["wsl", "wslg"]:
                users_dir = drive / "Users"
                if users_dir.exists():
                    for user_dir in users_dir.iterdir():
                        if user_dir.is_dir() and not user_dir.is_symlink():
                            claude_dir = user_dir / ".claude" / "projects"
                            if claude_dir.exists():
                                return user_dir

    return None


def get_windows_users_with_claude():
    """
    Get list of all Windows users with Claude Code installed.

    Returns:
        List of dicts with 'username', 'path', and 'workspace_count'.
    """
    results = []
    mnt = Path("/mnt")

    if not mnt.exists():
        return results

    for drive in sorted(mnt.iterdir()):
        if drive.is_dir() and drive.name not in ["wsl", "wslg"]:
            users_dir = drive / "Users"
            if users_dir.exists():
                for user_dir in users_dir.iterdir():
                    if user_dir.is_dir() and not user_dir.is_symlink():
                        claude_dir = user_dir / ".claude" / "projects"
                        if claude_dir.exists():
                            workspace_count = len([d for d in claude_dir.iterdir() if d.is_dir()])
                            results.append(
                                {
                                    "username": user_dir.name,
                                    "drive": drive.name,
                                    "path": user_dir,
                                    "claude_dir": claude_dir,
                                    "workspace_count": workspace_count,
                                }
                            )

    return results


def is_wsl_remote(remote_spec: str) -> bool:
    """Check if remote spec is a WSL distribution (wsl://DistroName)."""
    return remote_spec.startswith("wsl://")


def is_windows_remote(remote_spec: str) -> bool:
    """Check if remote spec is Windows from WSL (windows or windows://username)."""
    return remote_spec == "windows" or remote_spec.startswith("windows://")


def get_source_tag(remote_spec: str = None) -> str:
    """
    Generate source tag for filename/directory prefixes.

    Args:
        remote_spec: Remote specification (None for local, 'wsl://Ubuntu', 'windows', 'user@host')

    Returns:
        Source tag string:
        - '' for local (no tag)
        - 'wsl_{distro}_' for WSL
        - 'windows_' for Windows from WSL
        - 'remote_{hostname}_' for SSH remotes
    """
    if not remote_spec:
        # Local - no tag
        return ""

    if is_wsl_remote(remote_spec):
        # WSL: wsl_ubuntu_
        distro = remote_spec[6:]  # Remove 'wsl://' prefix
        return f"wsl_{distro.lower()}_"

    if is_windows_remote(remote_spec):
        # Windows from WSL: windows_ or windows_username_
        if remote_spec.startswith("windows://"):
            username = remote_spec[10:]  # Remove 'windows://' prefix
            return f"windows_{username.lower()}_"
        else:
            return "windows_"

    # SSH Remote: remote_hostname_
    # Extract hostname from user@hostname or hostname
    if "@" in remote_spec:
        hostname = remote_spec.split("@")[1]
    else:
        hostname = remote_spec

    # Take first part if FQDN (e.g., dev.example.com -> dev)
    hostname = hostname.split(".")[0].lower()

    return f"remote_{hostname}_"


def get_workspace_name_from_path(workspace_dir_name: str) -> str:
    """
    Extract clean workspace name from directory name.
    Removes source tags and normalizes path.

    Examples:
        'C--sankar-projects-claude-sessions' -> 'claude-sessions'
        'remote_ubuntuvm01_home-sankar-projects-claude-skills' -> 'claude-skills'
        'wsl_ubuntu_home-sankar-projects-auth' -> 'auth'
    """
    # Remove source tags if present
    if workspace_dir_name.startswith("remote_"):
        # remote_hostname_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= 3:
            workspace_dir_name = parts[2]
    elif workspace_dir_name.startswith("wsl_"):
        # wsl_distro_path -> path
        parts = workspace_dir_name.split("_", 2)
        if len(parts) >= 3:
            workspace_dir_name = parts[2]

    # Get the last path component (workspace name)
    # home-sankar-projects-claude-sessions -> claude-sessions
    # C--sankar-projects-claude-sessions -> claude-sessions
    path_parts = workspace_dir_name.split("-")

    # Find the workspace name (usually last component or last few)
    # Simple heuristic: take last part, or last 2 if hyphenated (e.g., claude-sessions)
    if len(path_parts) >= 2:
        # Check if last 2 parts form a common pattern
        last_two = "-".join(path_parts[-2:])
        # If the second-to-last part is short (likely part of name like "claude-sessions")
        if len(path_parts[-2]) <= 10:
            return last_two

    return path_parts[-1] if path_parts else workspace_dir_name


def get_wsl_distributions() -> list:
    """Get list of available WSL distributions.

    Returns:
        List of dicts with distro info: {'name': str, 'username': str, 'has_claude': bool}
    """
    import platform
    import subprocess

    # WSL only available on Windows
    if platform.system() != "Windows":
        return []

    try:
        # Get list of WSL distributions
        result = subprocess.run(["wsl", "--list", "--quiet"], capture_output=True, timeout=5)

        if result.returncode != 0:
            return []

        # Parse distribution names (output is UTF-16 LE with BOM and null terminators)
        output = result.stdout.decode("utf-16-le", errors="ignore")
        distros_raw = [line.strip().rstrip("\x00") for line in output.split("\n") if line.strip()]

        distributions = []
        for distro_name in distros_raw:
            if not distro_name:
                continue

            # Get username for this distro
            try:
                user_result = subprocess.run(
                    ["wsl", "-d", distro_name, "whoami"], capture_output=True, text=True, timeout=5
                )

                if user_result.returncode != 0:
                    continue

                username = user_result.stdout.strip()

                # Check if .claude/projects exists
                claude_path = Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects")
                has_claude = claude_path.exists()

                distributions.append(
                    {
                        "name": distro_name,
                        "username": username,
                        "has_claude": has_claude,
                        "path": str(claude_path) if has_claude else None,
                    }
                )
            except (subprocess.TimeoutExpired, FileNotFoundError):
                continue

        return distributions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_wsl_projects_dir(distro_name: str) -> Path:
    """Get Claude projects directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .claude/projects in WSL, accessible from Windows
    """
    import subprocess

    # Get username from WSL
    try:
        result = subprocess.run(["wsl", "-d", distro_name, "whoami"], capture_output=True, text=True, timeout=5)

        if result.returncode != 0:
            sys.stderr.write(f"Error: Could not get username for WSL distribution '{distro_name}'\n")
            sys.exit(1)

        username = result.stdout.strip()

        # Construct Windows path to WSL filesystem
        projects_dir = Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects")

        if not projects_dir.exists():
            sys.stderr.write(f"Error: Claude projects directory not found in WSL '{distro_name}'\n")
            sys.stderr.write(f"Expected path: {projects_dir}\n")
            sys.exit(1)

        return projects_dir

    except subprocess.TimeoutExpired:
        sys.stderr.write(f"Error: Timeout accessing WSL distribution '{distro_name}'\n")
        sys.exit(1)
    except FileNotFoundError:
        sys.stderr.write("Error: WSL not found. Is it installed?\n")
        sys.exit(1)


def get_windows_projects_dir(username: str = None) -> Path:
    """Get Claude projects directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .claude/projects in Windows, accessible from WSL
    """
    if not is_running_in_wsl():
        sys.stderr.write("Error: Not running in WSL. Windows access is only available from WSL.\n")
        sys.exit(1)

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        if username:
            sys.stderr.write(f"Error: Could not find Windows user '{username}' with Claude Code\n")
        else:
            sys.stderr.write("Error: Could not find Windows home directory with Claude Code\n")
        sys.stderr.write("\nTips:\n")
        sys.stderr.write("  â€¢ Make sure Claude Code is installed on Windows\n")
        sys.stderr.write("  â€¢ Try: ./claude-history --list-windows\n")
        if not username:
            sys.stderr.write("  â€¢ Or specify username: -r windows://username\n")
        sys.exit(1)

    projects_dir = windows_home / ".claude" / "projects"

    if not projects_dir.exists():
        sys.stderr.write("Error: Claude projects directory not found in Windows\n")
        sys.stderr.write(f"Expected path: {projects_dir}\n")
        sys.exit(1)

    return projects_dir


# ============================================================================
# Remote Fetching
# ============================================================================


def parse_remote_host(remote_spec: str) -> tuple:
    """Parse remote host specification.

    Args:
        remote_spec: Remote host in format 'user@hostname' or 'hostname'

    Returns:
        Tuple of (user, hostname, full_spec) or (None, hostname, hostname)
    """
    if "@" in remote_spec:
        user, hostname = remote_spec.split("@", 1)
        return (user, hostname, remote_spec)
    else:
        # Just hostname, SSH will use current user
        return (None, remote_spec, remote_spec)


def check_ssh_connection(remote_host: str) -> bool:
    """Check if passwordless SSH connection is possible.

    Args:
        remote_host: Full remote spec (user@hostname or hostname)

    Returns:
        True if connection successful, False otherwise
    """
    import subprocess

    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return False

    try:
        # Try SSH with BatchMode (no password prompts) and short timeout
        result = subprocess.run(
            ["ssh", "-o", "BatchMode=yes", "-o", "ConnectTimeout=5", remote_host, "echo ok"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        return result.returncode == 0 and result.stdout.strip() == "ok"
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def get_remote_hostname(remote_host: str) -> str:
    """Extract hostname from remote spec for use in directory prefix.

    Args:
        remote_host: Remote spec (user@hostname or hostname)

    Returns:
        Hostname portion only
    """
    _, hostname, _ = parse_remote_host(remote_host)
    # Clean hostname for use in directory names (remove dots, etc.)
    return hostname.replace(".", "-")


def get_remote_workspaces_batch(remote_host: str) -> list:
    """Get all workspace info from remote in one batch operation.

    Creates and runs a script on remote to gather all info efficiently.

    Returns:
        List of dicts with workspace info including decoded paths
    """
    import subprocess

    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    # Create a shell script that will run on remote
    script = """#!/bin/bash
cd ~/.claude/projects/ 2>/dev/null || exit 1

for dir in -*/ ; do
    dir=${dir%/}  # Remove trailing slash
    [ -d "$dir" ] || continue

    # Decode the workspace name by testing paths
    encoded="${dir#-}"  # Remove leading dash

    # Try to find the actual path
    IFS='-' read -ra PARTS <<< "$encoded"
    best_path=""

    # Build path by testing combinations
    current=""
    for part in "${PARTS[@]}"; do
        if [ -z "$current" ]; then
            current="$part"
        else
            # Try with dash
            test_with_dash="$current-$part"
            # Try as separate segment
            test_separate="$current/$part"

            if [ -d "/$test_with_dash" ]; then
                current="$test_with_dash"
            elif [ -d "/$current/$part" ]; then
                current="$current/$part"
            else
                current="$test_separate"
            fi
        fi
    done

    # Get session count
    session_count=$(find "$dir" -maxdepth 1 -name "*.jsonl" 2>/dev/null | wc -l)

    # Output: encoded_name|decoded_path|session_count
    echo "$dir|/$current|$session_count"
done
"""

    try:
        # Run the script on remote via SSH
        # Ensure Unix line endings (important on Windows)
        # Replace both \r\n and standalone \r with \n
        script_unix = script.replace("\r\n", "\n").replace("\r", "\n")
        script_bytes = script_unix.encode("utf-8")

        result = subprocess.run(["ssh", remote_host, "bash -s"], input=script_bytes, capture_output=True, timeout=30)

        if result.returncode != 0:
            return []

        stdout_text = result.stdout.decode("utf-8", errors="replace")
        # Parse results
        workspaces = []
        for line in stdout_text.strip().split("\n"):
            if not line:
                continue
            parts = line.split("|")
            if len(parts) >= 3:
                workspaces.append(
                    {
                        "encoded": parts[0],
                        "decoded": parts[1],
                        "session_count": int(parts[2]) if parts[2].isdigit() else 0,
                    }
                )

        return workspaces

    except Exception:
        return []


def normalize_remote_workspace_name(remote_host: str, workspace_dir_name: str) -> str:
    """Convert remote workspace directory name to readable path by verifying via SSH.

    Args:
        remote_host: Remote host specification
        workspace_dir_name: Encoded workspace name

    Returns:
        Decoded path
    """
    import subprocess

    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        # Fall back to simple replacement without remote verification
        encoded = workspace_dir_name[1:] if workspace_dir_name.startswith("-") else workspace_dir_name
        return "/" + encoded.replace("-", "/")
    if not validate_workspace_name(workspace_dir_name):
        return workspace_dir_name  # Return as-is if invalid

    # Remove leading dash
    if workspace_dir_name.startswith("-"):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # Generate possible decodings by trying different dash positions
    parts = encoded.split("-")

    # Generate all reasonable path combinations (limit to avoid exponential explosion)
    # We'll try keeping dashes together in common patterns
    candidates = []

    # Strategy: Build path greedily, trying longer segments first
    def generate_paths(parts, start_idx, current_path):
        if start_idx >= len(parts):
            candidates.append("/".join(current_path))
            return

        # Try combining 1 to 3 parts (limit to keep it reasonable)
        for length in range(1, min(4, len(parts) - start_idx + 1)):
            segment = "-".join(parts[start_idx : start_idx + length])
            generate_paths(parts, start_idx + length, current_path + [segment])

    generate_paths(parts, 0, [])

    # Limit candidates to reasonable number
    candidates = candidates[:20]

    # Test all candidates in ONE SSH call
    test_commands = " || ".join([f'test -e "/{path}" && echo "/{path}"' for path in candidates])

    try:
        result = subprocess.run(["ssh", remote_host, test_commands], capture_output=True, text=True, timeout=10)

        if result.stdout.strip():
            # Return the first (longest) match
            return result.stdout.strip().split("\n")[0]
    except (subprocess.SubprocessError, subprocess.TimeoutExpired, FileNotFoundError, OSError):
        pass

    # Fallback to simple replacement
    return "/" + encoded.replace("-", "/")


def list_remote_workspaces(remote_host: str) -> list:
    """List workspace directories on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of workspace directory names (e.g., ['-home-user-project', ...])
        Excludes remote caches (remote_* and wsl_*) to prevent circular fetching
    """
    import subprocess

    # Validate remote host to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []

    try:
        # List directories in remote ~/.claude/projects/ (simple and fast)
        result = subprocess.run(
            ["ssh", remote_host, 'ls -1 ~/.claude/projects/ | grep "^-"'], capture_output=True, text=True, timeout=30
        )

        if result.returncode != 0:
            return []

        # Parse output - one directory name per line
        all_workspaces = [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]

        # Filter out remote caches to prevent circular fetching:
        # - remote_* = SSH remote caches
        # - wsl_* = WSL caches
        # These are already fetched data and shouldn't be re-fetched
        workspaces = [ws for ws in all_workspaces if not ws.startswith("remote_") and not ws.startswith("wsl_")]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_remote_session_info(remote_host: str, remote_workspace: str) -> list:
    """Get session file information from remote workspace without downloading.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count
    """
    import subprocess

    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        sys.stderr.write(f"Error: Invalid remote host specification: {remote_host}\n")
        return []
    if not validate_workspace_name(remote_workspace):
        sys.stderr.write(f"Error: Invalid workspace name: {remote_workspace}\n")
        return []

    try:
        # Get file stats from remote using find and stat
        # Output format: filename|size_bytes|mtime_epoch|line_count
        # Use sanitize_for_shell to safely quote the workspace name
        safe_workspace = sanitize_for_shell(remote_workspace)
        cmd = f"""cd ~/.claude/projects/{safe_workspace} && \
                  for f in *.jsonl; do \
                      [ -f "$f" ] || continue; \
                      size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null); \
                      mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null); \
                      lines=$(wc -l < "$f"); \
                      echo "$f|$size|$mtime|$lines"; \
                  done"""

        result = subprocess.run(["ssh", remote_host, cmd], capture_output=True, text=True, timeout=30)

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split("\n"):
            if not line or "|" not in line:
                continue

            parts = line.split("|")
            if len(parts) != 4:
                continue

            filename, size_bytes, mtime_epoch, line_count = parts

            try:
                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)

                sessions.append(
                    {"filename": filename, "size_kb": size_kb, "modified": modified, "message_count": message_count}
                )
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def fetch_workspace_files(remote_host: str, remote_workspace: str, local_projects_dir: Path, hostname: str) -> dict:
    """Fetch all files from a remote workspace using rsync.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name (e.g., '-home-user-project')
        local_projects_dir: Local ~/.claude/projects/ directory
        hostname: Clean hostname for prefix

    Returns:
        Dict with stats: {'success': bool, 'files_copied': int, 'bytes': int}
    """
    import subprocess

    # Validate inputs to prevent command injection (security fix)
    if not validate_remote_host(remote_host):
        return {"success": False, "files_copied": 0, "bytes": 0, "error": f"Invalid remote host: {remote_host}"}
    if not validate_workspace_name(remote_workspace):
        return {"success": False, "files_copied": 0, "bytes": 0, "error": f"Invalid workspace name: {remote_workspace}"}

    # Generate local directory name with remote prefix
    # New naming: remote_hostname_path (consistent with export tags)
    # Strip leading dash from remote_workspace if present
    workspace_path = remote_workspace.lstrip("-")
    local_workspace = f"remote_{hostname}_{workspace_path}"
    local_dir = local_projects_dir / local_workspace

    # Create local directory
    local_dir.mkdir(parents=True, exist_ok=True)

    # Build rsync command
    # -a: archive mode (preserves timestamps, permissions, etc.)
    # -v: verbose
    # -h: human-readable
    # --include='*.jsonl': only sync .jsonl files
    # --exclude='*': exclude everything else
    # Note: rsync handles quoting internally when using list arguments
    remote_path = f"{remote_host}:~/.claude/projects/{remote_workspace}/"

    try:
        # Convert Windows path to Unix-style for rsync (even on Windows)
        # rsync expects forward slashes
        local_path_posix = str(local_dir).replace("\\", "/")

        # On Windows, rsync interprets "C:" as a remote host
        # Convert to /cygdrive/c/ notation which rsync understands
        import re

        if ":" in local_path_posix and not local_path_posix.startswith("/"):
            # Windows absolute path with drive letter (e.g., C:/Users/...)
            # Convert to Cygwin-style: /cygdrive/c/Users/...
            drive_match = re.match(r"([A-Za-z]):(.*)", local_path_posix)
            if drive_match:
                drive_letter = drive_match.group(1).lower()
                path_part = drive_match.group(2)
                local_path_str = f"/cygdrive/{drive_letter}{path_part}/"
            else:
                local_path_str = local_path_posix + "/"
        else:
            local_path_str = local_path_posix + "/"

        rsync_cmd = ["rsync", "-avh", "--include=*.jsonl", "--exclude=*", remote_path, local_path_str]

        result = subprocess.run(
            rsync_cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minute timeout
        )

        if result.returncode != 0:
            return {"success": False, "files_copied": 0, "bytes": 0, "error": result.stderr}

        # Parse rsync output to count files
        # Look for lines that don't start with special characters
        lines = result.stdout.split("\n")
        files_copied = sum(
            1 for line in lines if line.strip() and not line.startswith(("sending", "sent", "total", "building"))
        )

        return {"success": True, "files_copied": files_copied, "local_dir": local_workspace, "output": result.stdout}

    except subprocess.TimeoutExpired:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "Timeout"}
    except FileNotFoundError:
        return {"success": False, "files_copied": 0, "bytes": 0, "error": "rsync not found"}


# ============================================================================
# Alias Storage
# ============================================================================


def get_config_dir() -> Path:
    """Get the config storage directory (~/.claude-history/)."""
    return Path.home() / ".claude-history"


def get_aliases_dir() -> Path:
    """Get the aliases storage directory (~/.claude-history/)."""
    return get_config_dir()


def get_aliases_file() -> Path:
    """Get the aliases storage file path."""
    return get_aliases_dir() / "aliases.json"


def get_config_file() -> Path:
    """Get the config file path."""
    return get_config_dir() / "config.json"


def load_config() -> dict:
    """Load config from storage file. Returns empty structure if not found."""
    config_file = get_config_file()
    if not config_file.exists():
        return {"version": 1, "sources": []}

    try:
        with open(config_file, encoding="utf-8") as f:
            data = json.load(f)
            if "sources" not in data:
                data["sources"] = []
            if "version" not in data:
                data["version"] = 1
            return data
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Warning: Could not load config file: {e}\n")
        return {"version": 1, "sources": []}


def save_config(data: dict) -> bool:
    """Save config to storage file. Returns True on success."""
    config_dir = get_config_dir()
    config_file = get_config_file()

    try:
        config_dir.mkdir(parents=True, exist_ok=True)
        if "version" not in data:
            data["version"] = 1
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        return True
    except OSError as e:
        sys.stderr.write(f"Error saving config: {e}\n")
        return False


def get_saved_sources() -> list:
    """Get list of saved remote sources."""
    config = load_config()
    return config.get("sources", [])


def load_aliases() -> dict:
    """Load aliases from storage file. Returns empty structure if not found."""
    aliases_file = get_aliases_file()
    if not aliases_file.exists():
        return {"version": 1, "aliases": {}}

    try:
        with open(aliases_file, encoding="utf-8") as f:
            data = json.load(f)
            # Ensure structure is valid
            if "aliases" not in data:
                data["aliases"] = {}
            if "version" not in data:
                data["version"] = 1
            return data
    except (OSError, json.JSONDecodeError) as e:
        sys.stderr.write(f"Warning: Could not load aliases file: {e}\n")
        return {"version": 1, "aliases": {}}


def save_aliases(data: dict) -> bool:
    """Save aliases to storage file. Returns True on success."""
    aliases_dir = get_aliases_dir()
    aliases_file = get_aliases_file()

    try:
        # Create directory if needed
        aliases_dir.mkdir(parents=True, exist_ok=True)

        # Ensure version is set
        if "version" not in data:
            data["version"] = 1

        with open(aliases_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        return True
    except OSError as e:
        sys.stderr.write(f"Error: Could not save aliases: {e}\n")
        return False


def get_source_key(remote_host=None, wsl_distro=None, windows_user=None) -> str:
    """Get the source key for aliases based on current context."""
    if remote_host:
        if remote_host.startswith("wsl://"):
            return f"wsl:{remote_host[6:]}"
        elif remote_host.startswith("windows:"):
            return f"windows:{remote_host[8:]}" if len(remote_host) > 8 else "windows"
        elif remote_host == "windows":
            return "windows"
        else:
            hostname = remote_host.split("@")[-1] if "@" in remote_host else remote_host
            return f"remote:{hostname}"
    elif wsl_distro:
        return f"wsl:{wsl_distro}"
    elif windows_user is not None:
        return f"windows:{windows_user}" if windows_user else "windows"
    else:
        return "local"


def get_alias_session_count(alias_name: str, aliases_data: dict) -> int:
    """Count total sessions across all sources for an alias."""
    if alias_name not in aliases_data.get("aliases", {}):
        return 0

    alias_config = aliases_data["aliases"][alias_name]
    total = 0

    for _source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            try:
                # Try to get session count for this workspace
                sessions = get_workspace_sessions(workspace)
                total += len(sessions)
            except (OSError, PermissionError):
                pass  # Workspace may not exist or be accessible

    return total


def resolve_alias_workspaces(alias_name: str, source_filter: str = None) -> list:
    """
    Resolve an alias to a list of (source_key, workspace, sessions) tuples.
    If source_filter is provided, only include matching sources.
    """
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        return []

    alias_config = aliases_data["aliases"][alias_name]
    results = []

    for source_key, workspaces in alias_config.items():
        if source_filter and source_key != source_filter:
            continue

        for workspace in workspaces:
            results.append((source_key, workspace))

    return results


def get_alias_for_workspace(workspace: str, source_key: str = "local") -> str:
    """
    Find the alias that contains a given workspace, if any.

    Args:
        workspace: Encoded workspace name (e.g., '-home-user-project')
        source_key: Source identifier like 'local', 'windows', 'wsl:distro', 'remote:hostname'

    Returns:
        Alias name if found, None otherwise
    """
    aliases_data = load_aliases()

    for alias_name, alias_config in aliases_data.get("aliases", {}).items():
        # Check each source in the alias
        for src_key, workspaces in alias_config.items():
            # Handle source key matching (e.g., 'local' matches 'local', 'wsl:Ubuntu' matches 'wsl')
            if src_key == source_key or source_key.startswith(src_key + ":") or src_key.startswith(source_key + ":"):
                if workspace in workspaces:
                    return alias_name
            # Also check if workspace name (without source prefix) matches
            ws_name = get_workspace_name_from_path(workspace)
            for ws in workspaces:
                if get_workspace_name_from_path(ws) == ws_name:
                    return alias_name

    return None


def get_sessions_for_source(
    source_key: str, workspace: str, since_date=None, until_date=None, fetch_remote: bool = False
) -> list:
    """
    Get sessions from a workspace on a specific source.

    Args:
        source_key: Source identifier like 'local', 'windows:username', 'wsl:distro', 'remote:hostname'
        workspace: Encoded workspace name (e.g., '-home-user-project' or 'C--user-project')
        since_date: Optional start date filter
        until_date: Optional end date filter
        fetch_remote: If True, fetch remote sessions via SSH if not cached

    Returns:
        List of session dicts, or empty list if not accessible
    """
    try:
        if source_key == "local":
            # Local sessions - use default get_workspace_sessions
            return get_workspace_sessions(workspace, quiet=True, since_date=since_date, until_date=until_date)

        elif source_key.startswith("windows:"):
            # Windows sessions (from WSL)
            username = source_key.split(":", 1)[1]
            projects_dir = get_windows_projects_dir(username)

            # Temporarily override get_claude_projects_dir
            original_get_claude = get_claude_projects_dir

            def temp_override():
                return projects_dir

            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = temp_override

            try:
                sessions = get_workspace_sessions(workspace, quiet=True, since_date=since_date, until_date=until_date)
                return sessions
            finally:
                current_module.get_claude_projects_dir = original_get_claude

        elif source_key.startswith("wsl:"):
            # WSL sessions (from Windows)
            distro = source_key.split(":", 1)[1]
            projects_dir = get_wsl_projects_dir(distro)

            if projects_dir is None:
                return []

            # Temporarily override get_claude_projects_dir
            original_get_claude = get_claude_projects_dir

            def temp_override():
                return projects_dir

            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = temp_override

            try:
                sessions = get_workspace_sessions(workspace, quiet=True, since_date=since_date, until_date=until_date)
                return sessions
            finally:
                current_module.get_claude_projects_dir = original_get_claude

        elif source_key.startswith("remote:"):
            # Remote sessions via SSH
            hostname = source_key.split(":", 1)[1]

            # Get the local cache directory
            try:
                local_projects = get_claude_projects_dir()
            except (OSError, SystemExit):
                return []

            # Build the cached workspace name
            cached_workspace = f"remote_{hostname}_{workspace.lstrip('-')}"

            # Check if cached workspace exists
            cached_path = local_projects / cached_workspace
            if cached_path.exists():
                # Use the cached workspace (include_cached=True to allow remote_* pattern)
                return get_workspace_sessions(
                    cached_workspace, quiet=True, since_date=since_date, until_date=until_date, include_cached=True
                )
            elif fetch_remote:
                # Fetch from remote via SSH
                # Build remote host string (assume same username as local or use hostname directly)
                # The hostname stored might be just hostname or user@hostname
                remote_host = hostname if "@" in hostname else hostname

                # Check SSH connection first
                if not check_ssh_connection(remote_host):
                    sys.stderr.write(f"âš  Cannot connect to remote '{hostname}' - skipping\n")
                    return []

                # Fetch the workspace
                sys.stderr.write(f"ðŸ“¥ Fetching from {hostname}:{workspace}...\n")
                result = fetch_workspace_files(remote_host, workspace, local_projects, hostname)

                if result.get("success"):
                    # Now get sessions from the cached workspace (include_cached=True)
                    return get_workspace_sessions(
                        cached_workspace, quiet=True, since_date=since_date, until_date=until_date, include_cached=True
                    )
                else:
                    return []
            else:
                # Remote not cached and fetch not requested
                return []

        else:
            # Unknown source type
            return []

    except Exception:
        return []


# ============================================================================
# Alias Commands
# ============================================================================


def cmd_alias_list(args):
    """List all aliases with their workspaces and session counts."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    if not aliases:
        print("No aliases defined.")
        print("\nCreate an alias with:")
        print("  claude-history alias create <name>")
        print("  claude-history alias add <name> <workspace>")
        return

    for alias_name, sources in sorted(aliases.items()):
        total_sessions = 0
        print(f"\n{alias_name}:")

        for source_key, workspaces in sorted(sources.items()):
            if workspaces:
                print(f"  {source_key}:")
                for workspace in workspaces:
                    # Try to get session count using source-aware function
                    sessions = get_sessions_for_source(source_key, workspace)
                    count = len(sessions)
                    total_sessions += count
                    ws_readable = normalize_workspace_name(workspace)
                    if count > 0 or source_key == "local":
                        print(f"    {ws_readable}\t{count} sessions")
                    else:
                        print(f"    {ws_readable}\t{count} sessions (not cached)")

        print(f"  Total: {total_sessions} sessions")


def cmd_alias_show(args):
    """Show details of a single alias."""
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    alias_name = args.name
    if alias_name not in aliases:
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    sources = aliases[alias_name]
    total_sessions = 0

    print(f"{alias_name}:")

    for source_key, workspaces in sorted(sources.items()):
        if workspaces:
            print(f"  {source_key}:")
            for workspace in workspaces:
                # Use source-aware function to get sessions
                sessions = get_sessions_for_source(source_key, workspace)
                count = len(sessions)
                total_sessions += count
                ws_readable = normalize_workspace_name(workspace)
                if count > 0 or source_key == "local":
                    print(f"    {ws_readable}\t{count} sessions")
                else:
                    print(f"    {ws_readable}\t{count} sessions (not cached)")

    print(f"  Total: {total_sessions} sessions")


def cmd_alias_create(args):
    """Create a new empty alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' already exists\n")
        sys.exit(1)

    aliases_data["aliases"][alias_name] = {}

    if save_aliases(aliases_data):
        print(f"âœ… Created alias '{alias_name}'")
    else:
        sys.exit(1)


def cmd_alias_delete(args):
    """Delete an alias."""
    aliases_data = load_aliases()

    alias_name = args.name

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    del aliases_data["aliases"][alias_name]

    if save_aliases(aliases_data):
        print(f"âœ… Deleted alias '{alias_name}'")
    else:
        sys.exit(1)


def cmd_alias_add(args):
    """Add workspace(s) to an alias."""
    aliases_data = load_aliases()

    # Determine alias name (auto-detect from cwd if not provided)
    alias_name = args.name
    if not alias_name:
        # Auto-detect from current directory name
        alias_name = Path.cwd().name

    # Get workspace patterns
    raw_workspaces = args.workspaces if args.workspaces else []
    use_picker = getattr(args, "pick", False)

    if not raw_workspaces and not use_picker:
        sys.stderr.write("Error: No workspaces specified. Use --pick for interactive mode.\n")
        sys.exit(1)

    # Ensure alias exists
    if alias_name not in aliases_data.get("aliases", {}):
        aliases_data["aliases"][alias_name] = {}

    # Handle --as (all sources) flag
    if getattr(args, "all_sources", False):
        # Build list of all sources to check
        sources_to_check = []

        # 1. Local
        sources_to_check.append(("local", None))

        # 2. WSL distributions (if on Windows) or Windows users (if on WSL/Linux)
        if sys.platform == "win32":
            # On Windows, check WSL distributions
            try:
                wsl_distros = get_wsl_distributions()
                for distro in wsl_distros:
                    sources_to_check.append((f"wsl:{distro}", distro))
            except (OSError, subprocess.SubprocessError):
                pass
        else:
            # On Linux/WSL, check Windows users
            try:
                windows_users = get_windows_users_with_claude()
                for user_info in windows_users:
                    username = user_info.get("username", "")
                    sources_to_check.append((f"windows:{username}" if username else "windows", username))
            except (OSError, subprocess.SubprocessError):
                pass

        # 3. SSH remotes (if provided)
        remotes = getattr(args, "remotes", None) or []
        if getattr(args, "remote", None):
            remotes.append(args.remote)
        for remote in remotes:
            hostname = get_remote_hostname(remote)
            sources_to_check.append((f"remote:{hostname}", remote))

        # Add workspaces from each source
        total_added = 0
        for source_key, source_param in sources_to_check:
            if use_picker:
                workspaces_to_add = interactive_workspace_picker(source_key, args)
            else:
                workspaces_to_add = []
                for ws_input in raw_workspaces:
                    # Create a mock args object with the right remote parameter
                    class MockArgs:
                        remote = source_param if source_key.startswith("remote:") else None

                    resolved = resolve_workspace_input(ws_input, source_key, MockArgs())
                    if resolved:
                        workspaces_to_add.extend(resolved)

            if not workspaces_to_add:
                continue

            # Ensure source key exists
            if source_key not in aliases_data["aliases"][alias_name]:
                aliases_data["aliases"][alias_name][source_key] = []

            # Add workspaces (avoid duplicates)
            added = 0
            for workspace in workspaces_to_add:
                if workspace not in aliases_data["aliases"][alias_name][source_key]:
                    aliases_data["aliases"][alias_name][source_key].append(workspace)
                    added += 1

            if added > 0:
                print(f"[{source_key}] Added {added} workspace(s)")
                total_added += added

        if save_aliases(aliases_data):
            print(f"âœ… Added {total_added} workspace(s) to alias '{alias_name}' from all sources")
        else:
            sys.exit(1)
        return

    # Single source mode (original behavior)
    # For alias add, --wsl and --windows are boolean flags (store_true)
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,  # Empty string means auto-detect
        windows_user="" if windows_flag else None,  # Empty string means auto-detect
    )

    # Handle interactive picker mode
    if use_picker:
        workspaces_to_add = interactive_workspace_picker(source_key, args)
        if not workspaces_to_add:
            print("No workspaces selected.")
            return
    else:
        # Resolve each input to encoded workspace names
        workspaces_to_add = []
        for ws_input in raw_workspaces:
            resolved = resolve_workspace_input(ws_input, source_key, args)
            if resolved:
                workspaces_to_add.extend(resolved)
            else:
                sys.stderr.write(f"Warning: No workspace found matching '{ws_input}'\n")

        if not workspaces_to_add:
            sys.stderr.write("Error: No valid workspaces found.\n")
            sys.exit(1)

    # Ensure source key exists in alias
    if source_key not in aliases_data["aliases"][alias_name]:
        aliases_data["aliases"][alias_name][source_key] = []

    # Add workspaces (avoid duplicates)
    added = 0
    for workspace in workspaces_to_add:
        if workspace not in aliases_data["aliases"][alias_name][source_key]:
            aliases_data["aliases"][alias_name][source_key].append(workspace)
            added += 1

    if save_aliases(aliases_data):
        print(f"âœ… Added {added} workspace(s) to alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def interactive_workspace_picker(source_key: str, args) -> list:
    """Interactive workspace picker. Returns list of selected workspace names."""
    # Get list of available workspaces
    try:
        if source_key.startswith("wsl:"):
            distro = source_key[4:]
            projects_dir = get_wsl_projects_dir(distro)
            if not projects_dir:
                sys.stderr.write(f"Error: Cannot access WSL '{distro}'\n")
                return []
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
        elif source_key.startswith("windows"):
            # source_key is 'windows' or 'windows:username'
            user = source_key[8:] if source_key.startswith("windows:") else None
            projects_dir = get_windows_projects_dir(user)
            if not projects_dir:
                sys.stderr.write("Error: Cannot access Windows\n")
                return []
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
        elif source_key.startswith("remote:"):
            hostname = source_key[7:]
            # Find original remote format
            remote_host = getattr(args, "remote", hostname)
            batch_ws = get_remote_workspaces_batch(remote_host)
            workspaces = [ws["encoded"] for ws in batch_ws] if batch_ws else []
        else:
            # Local
            projects_dir = get_claude_projects_dir()
            workspaces = [
                d.name
                for d in projects_dir.iterdir()
                if d.is_dir() and not d.name.startswith("remote_") and not d.name.startswith("wsl_")
            ]
    except Exception as e:
        sys.stderr.write(f"Error listing workspaces: {e}\n")
        return []

    if not workspaces:
        print("No workspaces found.")
        return []

    # Display numbered list
    print(f"\nAvailable workspaces [{source_key}]:")
    for i, ws in enumerate(sorted(workspaces), 1):
        ws_readable = normalize_workspace_name(ws)
        print(f"  {i}. {ws_readable}")

    # Get user selection
    print("\nEnter workspace numbers (space-separated), or 'q' to quit:")
    try:
        selection = input("> ").strip()
    except (EOFError, KeyboardInterrupt):
        return []

    if selection.lower() == "q":
        return []

    # Parse selection
    selected = []
    sorted_ws = sorted(workspaces)
    for part in selection.split():
        try:
            idx = int(part) - 1
            if 0 <= idx < len(sorted_ws):
                selected.append(sorted_ws[idx])
        except ValueError:
            continue

    return selected


def cmd_alias_remove(args):
    """Remove workspace from an alias."""
    aliases_data = load_aliases()

    alias_name = args.name
    workspace = args.workspace

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    # Determine source key
    # For alias remove, --wsl and --windows are boolean flags
    wsl_flag = getattr(args, "wsl", False)
    windows_flag = getattr(args, "windows", False)
    source_key = get_source_key(
        remote_host=getattr(args, "remote", None),
        wsl_distro="" if wsl_flag else None,  # Empty string means auto-detect
        windows_user="" if windows_flag else None,  # Empty string means auto-detect
    )

    alias_config = aliases_data["aliases"][alias_name]

    if source_key not in alias_config:
        sys.stderr.write(f"Error: No workspaces from '{source_key}' in alias '{alias_name}'\n")
        sys.exit(1)

    if workspace not in alias_config[source_key]:
        sys.stderr.write(f"Error: Workspace '{workspace}' not in alias '{alias_name}' [{source_key}]\n")
        sys.exit(1)

    alias_config[source_key].remove(workspace)

    # Clean up empty source
    if not alias_config[source_key]:
        del alias_config[source_key]

    if save_aliases(aliases_data):
        print(f"âœ… Removed workspace from alias '{alias_name}' [{source_key}]")
    else:
        sys.exit(1)


def cmd_alias_config_export(args):
    """Export aliases configuration to JSON (stdout)."""
    aliases_data = load_aliases()
    print(json.dumps(aliases_data, indent=2))


def cmd_alias_lss(alias_name: str, since_date=None, until_date=None):
    """List sessions across all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    alias_config = aliases_data["aliases"][alias_name]
    all_sessions = []

    for source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            # Use source-aware function to get sessions
            sessions = get_sessions_for_source(source_key, workspace, since_date=since_date, until_date=until_date)
            for session in sessions:
                session["source"] = source_key
                all_sessions.append(session)

    if not all_sessions:
        sys.stderr.write(f"Error: No sessions found in alias '{alias_name}'\n")
        sys.exit(1)

    # Print header and sessions
    print("SOURCE\tWORKSPACE\tFILE\tMESSAGES\tDATE")
    for session in all_sessions:
        print(
            f"{session['source']}\t{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
        )


def cmd_alias_export(alias_name: str, output_dir: str, args):
    """Export sessions from all workspaces in an alias."""
    aliases_data = load_aliases()

    if alias_name not in aliases_data.get("aliases", {}):
        sys.stderr.write(f"Error: Alias '{alias_name}' not found\n")
        sys.exit(1)

    alias_config = aliases_data["aliases"][alias_name]

    # Parse options
    since_date = parse_date_string(getattr(args, "since", None))
    until_date = parse_date_string(getattr(args, "until", None))
    force = getattr(args, "force", False)
    minimal = getattr(args, "minimal", False)
    split_lines = getattr(args, "split", None)
    flat = getattr(args, "flat", False)

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    total_exported = 0
    total_skipped = 0
    total_failed = 0

    for source_key, workspaces in alias_config.items():
        for workspace in workspaces:
            # Use source-aware function to get sessions (auto-fetch remote if needed)
            sessions = get_sessions_for_source(
                source_key, workspace, since_date=since_date, until_date=until_date, fetch_remote=True
            )

            if not sessions:
                continue

            # Determine source tag for filenames
            source_tag = ""
            if source_key != "local":
                source_tag = source_key.replace(":", "_") + "_"

            # Determine workspace name for directory
            ws_name = normalize_workspace_name(workspace).replace("/", "-").replace("\\", "-")
            if ws_name.startswith("-"):
                ws_name = ws_name[1:]

            # Create workspace directory (unless flat mode)
            if flat:
                ws_output_path = output_path
            else:
                ws_output_path = output_path / ws_name
                ws_output_path.mkdir(parents=True, exist_ok=True)

            for session in sessions:
                jsonl_file = session["file"]

                # Generate output filename with source tag
                first_ts = get_first_timestamp(jsonl_file)
                if first_ts:
                    try:
                        dt = datetime.fromisoformat(first_ts.replace("Z", "+00:00"))
                        ts_prefix = dt.strftime("%Y%m%d%H%M%S")
                        output_name = f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
                    except (ValueError, AttributeError):
                        output_name = f"{source_tag}{jsonl_file.stem}.md"
                else:
                    output_name = f"{source_tag}{jsonl_file.stem}.md"

                output_file = ws_output_path / output_name

                # Check if incremental skip
                if not force and output_file.exists():
                    source_mtime = jsonl_file.stat().st_mtime
                    output_mtime = output_file.stat().st_mtime
                    if output_mtime >= source_mtime:
                        total_skipped += 1
                        continue

                try:
                    # Handle splitting
                    if split_lines:
                        messages = read_jsonl_messages(jsonl_file)
                        if messages and len(messages) > 3:
                            parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)
                            if parts:
                                base_name = output_name[:-3]  # Remove .md
                                for part_num, total_parts, part_md, start_msg, end_msg in parts:
                                    part_filename = f"{base_name}_part{part_num}.md"
                                    part_file = ws_output_path / part_filename
                                    part_file.write_text(part_md, encoding="utf-8")
                                    print(part_file)
                                total_exported += 1
                                continue

                    # Regular export (no splitting)
                    markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                    output_file.write_text(markdown, encoding="utf-8")
                    print(output_file)
                    total_exported += 1

                except Exception as e:
                    sys.stderr.write(f"Error exporting {jsonl_file}: {e}\n")
                    total_failed += 1

    # Print summary
    if total_exported > 0 or total_skipped > 0:
        print(f"\nâœ… Alias '{alias_name}': {total_exported} exported, {total_skipped} skipped, {total_failed} failed")


def cmd_alias_config_import(args):
    """Import aliases from JSON file or stdin."""
    import_file = getattr(args, "file", None)
    replace_mode = getattr(args, "replace", False)

    try:
        if import_file:
            with open(import_file, encoding="utf-8") as f:
                import_data = json.load(f)
        else:
            # Read from stdin
            import_data = json.load(sys.stdin)
    except json.JSONDecodeError as e:
        sys.stderr.write(f"Error: Invalid JSON: {e}\n")
        sys.exit(1)
    except OSError as e:
        sys.stderr.write(f"Error reading file: {e}\n")
        sys.exit(1)

    if "aliases" not in import_data:
        sys.stderr.write("Error: Invalid aliases format (missing 'aliases' key)\n")
        sys.exit(1)

    if replace_mode:
        # Replace all aliases
        new_data = import_data
    else:
        # Merge with existing
        existing_data = load_aliases()

        for alias_name, sources in import_data["aliases"].items():
            if alias_name not in existing_data["aliases"]:
                existing_data["aliases"][alias_name] = sources
            else:
                # Merge sources
                for source_key, workspaces in sources.items():
                    if source_key not in existing_data["aliases"][alias_name]:
                        existing_data["aliases"][alias_name][source_key] = workspaces
                    else:
                        # Merge workspaces (avoid duplicates)
                        for ws in workspaces:
                            if ws not in existing_data["aliases"][alias_name][source_key]:
                                existing_data["aliases"][alias_name][source_key].append(ws)

        new_data = existing_data

    if save_aliases(new_data):
        alias_count = len(new_data.get("aliases", {}))
        mode_str = "Replaced" if replace_mode else "Merged"
        print(f"âœ… {mode_str} aliases ({alias_count} aliases)")
    else:
        sys.exit(1)


# ============================================================================
# Metrics Database (SQLite)
# ============================================================================

import sqlite3

METRICS_DB_VERSION = 3

# Gap threshold for work period detection (30 minutes in seconds)
WORK_PERIOD_GAP_THRESHOLD = 30 * 60


def get_metrics_db_path() -> Path:
    """Get the metrics database file path."""
    return get_aliases_dir() / "metrics.db"


def init_metrics_db(db_path: Path = None) -> sqlite3.Connection:
    """Initialize the metrics database, creating tables if needed.

    Returns an open connection to the database.
    """
    if db_path is None:
        db_path = get_metrics_db_path()

    # Ensure directory exists
    db_path.parent.mkdir(parents=True, exist_ok=True)

    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row  # Enable column access by name

    # Create tables
    # Note: file_path is the primary key since multiple files can share the same session_id
    # (e.g., main session + agent files spawned from it)
    conn.executescript("""
        -- Schema version tracking
        CREATE TABLE IF NOT EXISTS schema_version (
            version INTEGER PRIMARY KEY
        );

        -- Sessions table (one row per JSONL file)
        CREATE TABLE IF NOT EXISTS sessions (
            file_path TEXT PRIMARY KEY,
            session_id TEXT,
            workspace TEXT NOT NULL,
            source TEXT NOT NULL DEFAULT 'local',
            file_mtime REAL,
            is_agent INTEGER DEFAULT 0,
            parent_session_id TEXT,
            start_time TEXT,
            end_time TEXT,
            message_count INTEGER DEFAULT 0,
            git_branch TEXT,
            claude_version TEXT,
            cwd TEXT,
            work_period_seconds REAL DEFAULT 0,
            num_work_periods INTEGER DEFAULT 1
        );

        -- Messages table (aggregated stats per message)
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            parent_uuid TEXT,
            type TEXT NOT NULL,
            timestamp TEXT NOT NULL,
            model TEXT,
            stop_reason TEXT,
            input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0,
            cache_creation_tokens INTEGER DEFAULT 0,
            cache_read_tokens INTEGER DEFAULT 0,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Tool uses table
        CREATE TABLE IF NOT EXISTS tool_uses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tool_use_id TEXT,
            message_uuid TEXT,
            file_path TEXT NOT NULL,
            session_id TEXT,
            tool_name TEXT NOT NULL,
            is_error INTEGER DEFAULT 0,
            timestamp TEXT,
            FOREIGN KEY (file_path) REFERENCES sessions(file_path)
        );

        -- Synced files tracking (for incremental sync)
        CREATE TABLE IF NOT EXISTS synced_files (
            file_path TEXT PRIMARY KEY,
            mtime REAL NOT NULL,
            synced_at TEXT NOT NULL
        );

        -- Create indexes for common queries
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace ON sessions(workspace);
        CREATE INDEX IF NOT EXISTS idx_sessions_source ON sessions(source);
        CREATE INDEX IF NOT EXISTS idx_sessions_start_time ON sessions(start_time);
        CREATE INDEX IF NOT EXISTS idx_sessions_session_id ON sessions(session_id);
        CREATE INDEX IF NOT EXISTS idx_messages_file_path ON messages(file_path);
        CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp);
        CREATE INDEX IF NOT EXISTS idx_messages_model ON messages(model);
        CREATE INDEX IF NOT EXISTS idx_messages_session ON messages(session_id);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_file_path ON tool_uses(file_path);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_tool_name ON tool_uses(tool_name);
        CREATE INDEX IF NOT EXISTS idx_tool_uses_session ON tool_uses(session_id);

        -- Composite indexes for common filter combinations
        CREATE INDEX IF NOT EXISTS idx_sessions_workspace_source ON sessions(workspace, source);
        CREATE INDEX IF NOT EXISTS idx_sessions_source_time ON sessions(source, start_time);
    """)

    # Check/set schema version and handle migrations
    cursor = conn.execute("SELECT version FROM schema_version LIMIT 1")
    row = cursor.fetchone()
    current_version = row["version"] if row else 0

    if current_version < METRICS_DB_VERSION:
        # Migrate from version 2 to 3: add time tracking columns
        if current_version < 3:
            try:
                conn.execute("ALTER TABLE sessions ADD COLUMN work_period_seconds REAL DEFAULT 0")
                conn.execute("ALTER TABLE sessions ADD COLUMN num_work_periods INTEGER DEFAULT 1")
            except sqlite3.OperationalError:
                pass  # Columns already exist

        # Update version
        if row is None:
            conn.execute("INSERT INTO schema_version (version) VALUES (?)", (METRICS_DB_VERSION,))
        else:
            conn.execute("UPDATE schema_version SET version = ?", (METRICS_DB_VERSION,))

    conn.commit()
    return conn


def calculate_work_periods(timestamps: list, gap_threshold: float = WORK_PERIOD_GAP_THRESHOLD) -> tuple:
    """Calculate work period time from a list of timestamps.

    Args:
        timestamps: List of ISO 8601 timestamp strings
        gap_threshold: Gap in seconds that marks end of a work period

    Returns:
        (work_period_seconds, num_work_periods, start_time, end_time)

    Note: Timestamps are sorted first since JSONL files may have
    out-of-order timestamps due to context restoration.
    """
    if not timestamps:
        return 0.0, 0, None, None

    if len(timestamps) == 1:
        return 0.0, 1, timestamps[0], timestamps[0]

    # Sort timestamps chronologically
    sorted_ts = sorted(timestamps)

    # Parse timestamps
    def parse_ts(ts):
        return datetime.fromisoformat(ts.replace("Z", "+00:00"))

    total_duration = 0.0
    num_periods = 1
    period_start = parse_ts(sorted_ts[0])
    prev_ts = period_start

    for ts_str in sorted_ts[1:]:
        ts = parse_ts(ts_str)
        gap = (ts - prev_ts).total_seconds()

        if gap > gap_threshold:
            # End current period, start new one
            total_duration += (prev_ts - period_start).total_seconds()
            period_start = ts
            num_periods += 1

        prev_ts = ts

    # Add final period
    total_duration += (prev_ts - period_start).total_seconds()

    return total_duration, num_periods, sorted_ts[0], sorted_ts[-1]


def extract_metrics_from_jsonl(jsonl_file: Path, source: str = "local") -> dict:
    """Extract metrics data from a JSONL file.

    Returns a dict with:
        - session: session metadata
        - messages: list of message stats
        - tool_uses: list of tool use records
    """
    messages = []
    tool_uses = []
    all_timestamps = []  # Collect all timestamps for work period calculation
    session_info = {
        "session_id": None,
        "workspace": None,
        "source": source,
        "file_path": str(jsonl_file),
        "file_mtime": jsonl_file.stat().st_mtime if jsonl_file.exists() else None,
        "is_agent": False,
        "parent_session_id": None,
        "start_time": None,
        "end_time": None,
        "message_count": 0,
        "git_branch": None,
        "claude_version": None,
        "cwd": None,
        "work_period_seconds": 0.0,
        "num_work_periods": 0,
    }

    try:
        with open(jsonl_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                try:
                    entry = json.loads(line)
                    entry_type = entry.get("type")

                    # Skip non-message records
                    if entry_type not in ("user", "assistant"):
                        continue

                    timestamp = entry.get("timestamp", "")

                    # Update session info from first message
                    if session_info["session_id"] is None:
                        session_info["session_id"] = entry.get("sessionId")
                        session_info["git_branch"] = entry.get("gitBranch")
                        session_info["claude_version"] = entry.get("version")
                        session_info["cwd"] = entry.get("cwd")
                        session_info["is_agent"] = entry.get("isSidechain", False)
                        if session_info["is_agent"]:
                            session_info["parent_session_id"] = entry.get("sessionId")

                    # Collect timestamps for work period calculation
                    if timestamp:
                        all_timestamps.append(timestamp)

                    session_info["message_count"] += 1

                    # Extract message metrics
                    message_obj = entry.get("message", {})
                    usage = message_obj.get("usage", {})

                    msg_record = {
                        "uuid": entry.get("uuid"),
                        "session_id": entry.get("sessionId"),
                        "parent_uuid": entry.get("parentUuid"),
                        "type": entry_type,
                        "timestamp": timestamp,
                        "model": message_obj.get("model"),
                        "stop_reason": message_obj.get("stop_reason"),
                        "input_tokens": usage.get("input_tokens", 0),
                        "output_tokens": usage.get("output_tokens", 0),
                        "cache_creation_tokens": usage.get("cache_creation_input_tokens", 0),
                        "cache_read_tokens": usage.get("cache_read_input_tokens", 0),
                    }
                    messages.append(msg_record)

                    # Extract tool uses from assistant messages
                    if entry_type == "assistant":
                        content = message_obj.get("content", [])
                        if isinstance(content, list):
                            for block in content:
                                if isinstance(block, dict) and block.get("type") == "tool_use":
                                    tool_uses.append(
                                        {
                                            "tool_use_id": block.get("id"),
                                            "message_uuid": entry.get("uuid"),
                                            "session_id": entry.get("sessionId"),
                                            "tool_name": block.get("name", "unknown"),
                                            "is_error": False,  # Will be updated from tool_result
                                            "timestamp": timestamp,
                                        }
                                    )

                    # Check for tool errors in user messages (tool results)
                    if entry_type == "user":
                        content = message_obj.get("content", [])
                        if isinstance(content, list):
                            for block in content:
                                if isinstance(block, dict) and block.get("type") == "tool_result":
                                    tool_use_id = block.get("tool_use_id")
                                    is_error = block.get("is_error", False)
                                    # Update corresponding tool_use record
                                    for tu in tool_uses:
                                        if tu["tool_use_id"] == tool_use_id:
                                            tu["is_error"] = is_error
                                            break

                except json.JSONDecodeError:
                    continue

    except Exception as e:
        sys.stderr.write(f"Warning: Error reading {jsonl_file}: {e}\n")

    # Calculate work periods from collected timestamps
    work_secs, num_periods, start_time, end_time = calculate_work_periods(all_timestamps)
    session_info["work_period_seconds"] = work_secs
    session_info["num_work_periods"] = num_periods
    session_info["start_time"] = start_time
    session_info["end_time"] = end_time

    # Derive workspace from file path
    workspace_dir = jsonl_file.parent.name
    session_info["workspace"] = get_workspace_name_from_path(workspace_dir)

    return {
        "session": session_info,
        "messages": messages,
        "tool_uses": tool_uses,
    }


def sync_file_to_db(conn: sqlite3.Connection, jsonl_file: Path, source: str = "local", force: bool = False) -> bool:
    """Sync a single JSONL file to the database.

    Returns True if file was synced, False if skipped (already up to date).
    """
    file_path = str(jsonl_file)
    current_mtime = jsonl_file.stat().st_mtime if jsonl_file.exists() else 0

    # Check if already synced and up to date
    if not force:
        cursor = conn.execute("SELECT mtime FROM synced_files WHERE file_path = ?", (file_path,))
        row = cursor.fetchone()
        if row and row["mtime"] >= current_mtime:
            return False  # Already up to date

    # Extract metrics
    metrics = extract_metrics_from_jsonl(jsonl_file, source)
    session = metrics["session"]

    if session["message_count"] == 0:
        return False  # No messages in file

    # Delete existing data for this file (for re-sync)
    conn.execute("DELETE FROM tool_uses WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM messages WHERE file_path = ?", (file_path,))
    conn.execute("DELETE FROM sessions WHERE file_path = ?", (file_path,))

    # Insert session
    conn.execute(
        """
        INSERT INTO sessions (
            file_path, session_id, workspace, source, file_mtime,
            is_agent, parent_session_id, start_time, end_time,
            message_count, git_branch, claude_version, cwd,
            work_period_seconds, num_work_periods
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """,
        (
            file_path,
            session["session_id"],
            session["workspace"],
            session["source"],
            session["file_mtime"],
            1 if session["is_agent"] else 0,
            session["parent_session_id"],
            session["start_time"],
            session["end_time"],
            session["message_count"],
            session["git_branch"],
            session["claude_version"],
            session["cwd"],
            session["work_period_seconds"],
            session["num_work_periods"],
        ),
    )

    # Insert messages
    for msg in metrics["messages"]:
        conn.execute(
            """
            INSERT INTO messages (
                uuid, file_path, session_id, parent_uuid, type, timestamp,
                model, stop_reason, input_tokens, output_tokens,
                cache_creation_tokens, cache_read_tokens
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                msg["uuid"],
                file_path,
                msg["session_id"],
                msg["parent_uuid"],
                msg["type"],
                msg["timestamp"],
                msg["model"],
                msg["stop_reason"],
                msg["input_tokens"],
                msg["output_tokens"],
                msg["cache_creation_tokens"],
                msg["cache_read_tokens"],
            ),
        )

    # Insert tool uses
    for tu in metrics["tool_uses"]:
        conn.execute(
            """
            INSERT INTO tool_uses (
                tool_use_id, message_uuid, file_path, session_id, tool_name, is_error, timestamp
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                tu["tool_use_id"],
                tu["message_uuid"],
                file_path,
                tu["session_id"],
                tu["tool_name"],
                1 if tu["is_error"] else 0,
                tu["timestamp"],
            ),
        )

    # Update synced files tracking
    conn.execute(
        """
        INSERT OR REPLACE INTO synced_files (file_path, mtime, synced_at)
        VALUES (?, ?, ?)
    """,
        (file_path, current_mtime, datetime.now().isoformat()),
    )

    conn.commit()
    return True


def cmd_stats_sync(args):
    """Sync JSONL files to metrics database."""
    force = getattr(args, "force", False)
    all_sources = getattr(args, "all_sources", False)
    remotes = getattr(args, "remotes", None) or []
    patterns = getattr(args, "patterns", [""])

    # Include saved sources when --as is used
    if all_sources:
        saved_sources = get_saved_sources()
        for src in saved_sources:
            if src not in remotes:
                remotes.append(src)

    conn = init_metrics_db()

    synced = 0
    skipped = 0
    errors = 0

    def sync_source(projects_dir: Path, source: str, source_label: str):
        nonlocal synced, skipped, errors

        if not projects_dir or not projects_dir.exists():
            return

        print(f"ðŸ” Scanning {source_label}...")

        for workspace_dir in projects_dir.iterdir():
            if not workspace_dir.is_dir():
                continue

            # Skip cached remote/WSL directories
            if workspace_dir.name.startswith("remote_") or workspace_dir.name.startswith("wsl_"):
                continue

            # Check pattern match
            if patterns and patterns[0]:
                if not any(p in workspace_dir.name for p in patterns):
                    continue

            # Find all JSONL files
            for jsonl_file in workspace_dir.glob("*.jsonl"):
                try:
                    if sync_file_to_db(conn, jsonl_file, source, force):
                        synced += 1
                    else:
                        skipped += 1
                except Exception as e:
                    sys.stderr.write(f"  âŒ Error syncing {jsonl_file.name}: {e}\n")
                    errors += 1

    # Sync local
    projects_dir = get_claude_projects_dir()
    sync_source(projects_dir, "local", "local")

    # Sync WSL/Windows if --as flag or running from Windows/WSL
    if all_sources:
        # WSL distributions
        wsl_distros = get_wsl_distributions()
        for distro in wsl_distros:
            if distro.get("has_claude"):
                wsl_projects = get_wsl_projects_dir(distro["name"])
                if wsl_projects:
                    sync_source(wsl_projects, f"wsl:{distro['name']}", f"WSL ({distro['name']})")

        # Windows (if accessible)
        try:
            win_projects = get_windows_projects_dir()
            if win_projects:
                sync_source(win_projects, "windows", "Windows")
        except Exception:
            pass

    # Sync remotes
    for remote in remotes:
        if remote.startswith("wsl://"):
            distro = remote[6:]
            wsl_projects = get_wsl_projects_dir(distro)
            if wsl_projects:
                sync_source(wsl_projects, f"wsl:{distro}", f"WSL ({distro})")
        elif remote == "windows" or remote.startswith("windows:"):
            win_projects = get_windows_projects_dir()
            if win_projects:
                sync_source(win_projects, "windows", "Windows")
        else:
            # SSH remote - fetch files first, then sync
            hostname = get_remote_hostname(remote)
            print(f"ðŸ” Fetching from remote {remote}...")

            # Check SSH connectivity
            if not check_ssh_connection(remote):
                sys.stderr.write(f"  âŒ Cannot connect to {remote}\n")
                errors += 1
                continue

            # Get list of remote workspaces
            remote_workspaces = list_remote_workspaces(remote)
            if not remote_workspaces:
                print(f"  No workspaces found on {remote}")
                continue

            # Filter by patterns if specified
            if patterns and patterns[0]:
                remote_workspaces = [ws for ws in remote_workspaces if any(p in ws for p in patterns)]

            # Fetch each workspace and sync
            local_projects_dir = get_claude_projects_dir()
            for ws in remote_workspaces:
                try:
                    # Fetch files using rsync
                    fetch_result = fetch_workspace_files(remote, ws, local_projects_dir, hostname)
                    if not fetch_result.get("success"):
                        continue

                    # Now sync the cached files
                    workspace_path = ws.lstrip("-")
                    local_workspace = f"remote_{hostname}_{workspace_path}"
                    local_dir = local_projects_dir / local_workspace

                    if local_dir.exists():
                        for jsonl_file in local_dir.glob("*.jsonl"):
                            try:
                                if sync_file_to_db(conn, jsonl_file, f"remote:{hostname}", force):
                                    synced += 1
                                else:
                                    skipped += 1
                            except Exception as e:
                                sys.stderr.write(f"  âŒ Error syncing {jsonl_file.name}: {e}\n")
                                errors += 1
                except Exception as e:
                    sys.stderr.write(f"  âŒ Error fetching {ws}: {e}\n")
                    errors += 1

    conn.close()

    print(f"\nâœ… Sync complete: {synced} synced, {skipped} up-to-date, {errors} errors")


def cmd_stats(args):
    """Display metrics statistics."""
    db_path = get_metrics_db_path()

    if not db_path.exists():
        print("âŒ No metrics database found. Run 'stats --sync' first.")
        print("\nUsage:")
        print("  claude-history stats --sync        # Sync local sessions")
        print("  claude-history stats --sync --as   # Sync from all sources")
        sys.exit(1)

    conn = init_metrics_db(db_path)

    # Get filter parameters
    workspace_patterns = getattr(args, "workspace", None) or []
    source_filter = getattr(args, "source", None)
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)
    all_workspaces = getattr(args, "all_workspaces", False)
    this_only = getattr(args, "this_only", False)

    # Default to current workspace if no patterns specified (consistent with lss/export)
    if not workspace_patterns and not all_workspaces:
        # Use workspace name (final component) since that's what's stored in the DB
        full_pattern = get_current_workspace_pattern()
        ws_name = get_workspace_name_from_path(full_pattern)

        # Check if current workspace belongs to an alias (unless --this is used)
        if not this_only:
            alias_name = get_alias_for_workspace(full_pattern, "local")
            if alias_name:
                # Use the alias instead of just the current workspace
                workspace_patterns = [f"@{alias_name}"]
                sys.stderr.write(f"ðŸ“Ž Using alias @{alias_name} (use --this for current workspace only)\n")
            else:
                workspace_patterns = [ws_name]
        else:
            workspace_patterns = [ws_name]

    # Build WHERE clause
    where_clauses = []
    params = []

    if workspace_patterns:
        # Handle multiple workspace patterns (including aliases)
        ws_conditions = []
        aliases_data = load_aliases()
        aliases = aliases_data.get("aliases", {})

        for pattern in workspace_patterns:
            if pattern.startswith("@"):
                # Alias reference
                alias_name = pattern[1:]
                if alias_name in aliases:
                    # Expand alias to workspace names
                    for source_key, workspaces in aliases[alias_name].items():
                        if isinstance(workspaces, list):
                            for ws in workspaces:
                                ws_name = get_workspace_name_from_path(ws)
                                ws_conditions.append("s.workspace = ?")
                                params.append(ws_name)
                else:
                    print(f"âš ï¸  Alias '{alias_name}' not found. Treating as workspace pattern.")
                    ws_conditions.append("s.workspace LIKE ?")
                    params.append(f"%{alias_name}%")
            else:
                # Regular pattern
                ws_conditions.append("s.workspace LIKE ?")
                params.append(f"%{pattern}%")

        if ws_conditions:
            where_clauses.append(f"({' OR '.join(ws_conditions)})")

    if source_filter:
        where_clauses.append("s.source = ?")
        params.append(source_filter)

    if since_str:
        where_clauses.append("s.start_time >= ?")
        params.append(since_str)

    if until_str:
        where_clauses.append("s.start_time <= ?")
        params.append(until_str + "T23:59:59")

    where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

    # Check what stats to show
    show_tools = getattr(args, "tools", False)
    show_models = getattr(args, "models", False)
    show_time = getattr(args, "time", False)
    by_workspace = getattr(args, "by_workspace", False)
    by_day = getattr(args, "by_day", False)

    if show_tools:
        display_tool_stats(conn, where_sql, params)
    elif show_models:
        display_model_stats(conn, where_sql, params)
    elif show_time:
        display_time_stats(conn, where_sql, params)
    elif by_workspace:
        display_workspace_stats(conn, where_sql, params)
    elif by_day:
        display_daily_stats(conn, where_sql, params)
    else:
        display_summary_stats(conn, where_sql, params)

    conn.close()


def display_summary_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display summary statistics dashboard."""
    # Session stats
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_sessions,
            SUM(CASE WHEN is_agent = 0 THEN 1 ELSE 0 END) as main_sessions,
            SUM(CASE WHEN is_agent = 1 THEN 1 ELSE 0 END) as agent_sessions,
            SUM(message_count) as total_messages
        FROM sessions s
        WHERE {where_sql}
    """,
        params,
    )
    session_stats = cursor.fetchone()

    # Token stats
    cursor = conn.execute(
        f"""
        SELECT
            COALESCE(SUM(m.input_tokens), 0) as total_input,
            COALESCE(SUM(m.output_tokens), 0) as total_output,
            COALESCE(SUM(m.cache_creation_tokens), 0) as total_cache_creation,
            COALESCE(SUM(m.cache_read_tokens), 0) as total_cache_read
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    token_stats = cursor.fetchone()

    # Tool stats
    cursor = conn.execute(
        f"""
        SELECT
            COUNT(*) as total_tool_uses,
            SUM(CASE WHEN is_error = 1 THEN 1 ELSE 0 END) as tool_errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
    """,
        params,
    )
    tool_stats = cursor.fetchone()

    # Source breakdown
    cursor = conn.execute(
        f"""
        SELECT source, COUNT(*) as count
        FROM sessions s
        WHERE {where_sql}
        GROUP BY source
        ORDER BY count DESC
    """,
        params,
    )
    sources = cursor.fetchall()

    # Model breakdown (top 3)
    cursor = conn.execute(
        f"""
        SELECT m.model, COUNT(*) as count
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL
        GROUP BY m.model
        ORDER BY count DESC
        LIMIT 3
    """,
        params,
    )
    models = cursor.fetchall()

    # Top workspaces (top 5) - with alias aggregation
    # First get all workspace stats
    cursor = conn.execute(
        f"""
        SELECT workspace, source, COUNT(*) as sessions, SUM(message_count) as messages
        FROM sessions s
        WHERE {where_sql}
        GROUP BY workspace, source
        ORDER BY sessions DESC
    """,
        params,
    )
    all_workspace_stats = cursor.fetchall()

    # Build workspace-to-alias mapping from aliases config
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    # Create reverse mapping: (workspace, source) -> alias_name
    workspace_to_alias = {}
    for alias_name, alias_config in aliases.items():
        for source_key, workspaces in alias_config.items():
            if isinstance(workspaces, list):
                # Normalize source key for matching
                # Alias source keys: 'local', 'windows:user', 'wsl:distro', 'remote:host'
                # DB source values: 'local', 'windows', 'wsl:distro', 'remote:host'
                normalized_source = source_key
                if source_key.startswith("windows:"):
                    normalized_source = "windows"  # DB uses just 'windows'

                for ws in workspaces:
                    # Get clean workspace name for matching
                    ws_name = get_workspace_name_from_path(ws)
                    workspace_to_alias[(ws_name, normalized_source)] = alias_name

    # Aggregate by alias where applicable
    alias_stats = {}  # alias_name -> {sessions, messages, workspaces}
    unaliased_workspaces = []  # workspaces not in any alias

    for row in all_workspace_stats:
        ws_name = row["workspace"]
        source = row["source"]
        sessions = row["sessions"]
        messages = row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {"sessions": 0, "messages": 0, "workspaces": set()}
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["workspaces"].add(ws_name)
        else:
            unaliased_workspaces.append(
                {"name": ws_name, "sessions": sessions, "messages": messages, "is_alias": False}
            )

    # Combine aliases and unaliased workspaces, sort by sessions
    top_workspaces = []
    for alias_name, stats in alias_stats.items():
        top_workspaces.append(
            {
                "name": f"@{alias_name}",
                "sessions": stats["sessions"],
                "messages": stats["messages"],
                "is_alias": True,
                "workspace_count": len(stats["workspaces"]),
            }
        )
    top_workspaces.extend(unaliased_workspaces)
    top_workspaces.sort(key=lambda x: x["sessions"], reverse=True)
    top_workspaces = top_workspaces[:5]  # Top 5

    # Calculate derived stats
    total_input = token_stats["total_input"] or 0
    total_output = token_stats["total_output"] or 0
    cache_creation = token_stats["total_cache_creation"] or 0
    cache_read = token_stats["total_cache_read"] or 0

    cache_total = cache_creation + cache_read
    cache_hit_ratio = (cache_read / cache_total * 100) if cache_total > 0 else 0

    # Display
    print("=" * 60)
    print("CLAUDE CODE METRICS SUMMARY")
    print("=" * 60)

    total_sessions = session_stats["total_sessions"] or 0
    main_sessions = session_stats["main_sessions"] or 0
    agent_sessions = session_stats["agent_sessions"] or 0
    total_messages = session_stats["total_messages"] or 0

    print("\nðŸ“Š Sessions")
    print(f"   Total: {total_sessions:,}")
    print(f"   Main sessions: {main_sessions:,}")
    print(f"   Agent tasks: {agent_sessions:,}")
    print(f"   Total messages: {total_messages:,}")

    print("\nðŸ”¤ Tokens")
    print(f"   Input: {total_input:,}")
    print(f"   Output: {total_output:,}")
    print(f"   Cache created: {cache_creation:,}")
    print(f"   Cache read: {cache_read:,}")
    print(f"   Cache hit ratio: {cache_hit_ratio:.1f}%")

    total_tool_uses = tool_stats["total_tool_uses"] or 0
    tool_errors = tool_stats["tool_errors"] or 0

    print("\nðŸ”§ Tools")
    print(f"   Total uses: {total_tool_uses:,}")
    print(f"   Errors: {tool_errors:,}")
    if total_tool_uses > 0:
        error_rate = tool_errors / total_tool_uses * 100
        print(f"   Error rate: {error_rate:.1f}%")

    if sources:
        print("\nðŸ“ Sources")
        for src in sources:
            print(f"   {src['source']}: {src['count']:,} sessions")

    if models:
        print("\nðŸ¤– Models (top 3)")
        for model in models:
            model_name = model["model"] or "unknown"
            # Shorten model name for display
            short_name = model_name.replace("claude-", "").replace("-20250929", "").replace("-20251001", "")
            print(f"   {short_name}: {model['count']:,} messages")

    if top_workspaces:
        print("\nðŸ“ Top Workspaces")
        for ws in top_workspaces:
            if ws.get("is_alias"):
                # Show alias with workspace count
                ws_count = ws.get("workspace_count", 0)
                print(f"   {ws['name']} ({ws_count} workspaces): {ws['sessions']} sessions, {ws['messages']} msgs")
            else:
                print(f"   {ws['name']}: {ws['sessions']} sessions, {ws['messages']} msgs")

    print()


def display_tool_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display tool usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            t.tool_name,
            COUNT(*) as uses,
            SUM(CASE WHEN t.is_error = 1 THEN 1 ELSE 0 END) as errors
        FROM tool_uses t
        JOIN sessions s ON t.session_id = s.session_id
        WHERE {where_sql}
        GROUP BY t.tool_name
        ORDER BY uses DESC
    """,
        params,
    )
    tools = cursor.fetchall()

    print("=" * 60)
    print("TOOL USAGE STATISTICS")
    print("=" * 60)

    print(f"\n{'Tool':<25} {'Uses':>10} {'Errors':>10} {'Error %':>10}")
    print("-" * 55)

    for row in tools:
        uses = row["uses"]
        errors = row["errors"]
        error_pct = (errors / uses * 100) if uses > 0 else 0
        print(f"{row['tool_name']:<25} {uses:>10,} {errors:>10,} {error_pct:>9.1f}%")

    print()


def display_model_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display model usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            m.model,
            COUNT(*) as messages,
            COUNT(DISTINCT m.session_id) as sessions,
            SUM(m.input_tokens) as input_tokens,
            SUM(m.output_tokens) as output_tokens
        FROM messages m
        JOIN sessions s ON m.session_id = s.session_id
        WHERE {where_sql} AND m.model IS NOT NULL
        GROUP BY m.model
        ORDER BY messages DESC
    """,
        params,
    )
    models = cursor.fetchall()

    print("=" * 60)
    print("MODEL USAGE STATISTICS")
    print("=" * 60)

    print(f"\n{'Model':<35} {'Messages':>10} {'Sessions':>10} {'Avg Out':>10}")
    print("-" * 65)

    for row in models:
        model = row["model"] or "unknown"
        short_model = (
            model.replace("claude-", "").replace("-20250929", "").replace("-20251001", "").replace("-20251101", "")
        )
        messages = row["messages"]
        avg_output = (row["output_tokens"] / messages) if messages > 0 else 0
        print(f"{short_model:<35} {messages:>10,} {row['sessions']:>10,} {avg_output:>10,.0f}")

    print()


def display_workspace_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display per-workspace statistics with alias aggregation."""
    cursor = conn.execute(
        f"""
        SELECT
            s.workspace,
            s.source,
            COUNT(*) as sessions,
            SUM(s.message_count) as messages,
            SUM(m_stats.input_tokens) as input_tokens,
            SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (
            SELECT session_id,
                   SUM(input_tokens) as input_tokens,
                   SUM(output_tokens) as output_tokens
            FROM messages
            GROUP BY session_id
        ) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql}
        GROUP BY s.workspace, s.source
        ORDER BY sessions DESC
    """,
        params,
    )
    all_workspaces = cursor.fetchall()

    # Build workspace-to-alias mapping
    aliases_data = load_aliases()
    aliases = aliases_data.get("aliases", {})

    workspace_to_alias = {}
    for alias_name, alias_config in aliases.items():
        for source_key, workspaces in alias_config.items():
            if isinstance(workspaces, list):
                normalized_source = source_key
                if source_key.startswith("windows:"):
                    normalized_source = "windows"
                for ws in workspaces:
                    ws_name = get_workspace_name_from_path(ws)
                    workspace_to_alias[(ws_name, normalized_source)] = alias_name

    # Aggregate by alias
    alias_stats = {}
    unaliased = []

    for row in all_workspaces:
        ws_name = row["workspace"]
        source = row["source"]
        sessions = row["sessions"]
        messages = row["messages"] or 0

        alias_name = workspace_to_alias.get((ws_name, source))
        if alias_name:
            if alias_name not in alias_stats:
                alias_stats[alias_name] = {"sessions": 0, "messages": 0, "sources": set()}
            alias_stats[alias_name]["sessions"] += sessions
            alias_stats[alias_name]["messages"] += messages
            alias_stats[alias_name]["sources"].add(source)
        else:
            unaliased.append({"name": ws_name, "source": source, "sessions": sessions, "messages": messages})

    print("=" * 60)
    print("WORKSPACE STATISTICS")
    print("=" * 60)

    # Show aliases first
    if alias_stats:
        print("\nðŸ“¦ Aliases")
        print(f"{'Alias':<30} {'Sources':<15} {'Sessions':>8} {'Messages':>10}")
        print("-" * 65)

        sorted_aliases = sorted(alias_stats.items(), key=lambda x: x[1]["sessions"], reverse=True)
        for alias_name, stats in sorted_aliases:
            sources_str = ",".join(sorted(stats["sources"]))[:13]
            print(f"@{alias_name:<29} {sources_str:<15} {stats['sessions']:>8,} {stats['messages']:>10,}")

    # Show unaliased workspaces
    print("\nðŸ“ Individual Workspaces (Top 20)")
    print(f"{'Workspace':<30} {'Source':<10} {'Sessions':>8} {'Messages':>10}")
    print("-" * 60)

    for row in unaliased[:20]:
        ws_name = row["name"][:28] if len(row["name"]) > 28 else row["name"]
        print(f"{ws_name:<30} {row['source']:<10} {row['sessions']:>8,} {row['messages']:>10,}")

    print()


def display_daily_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display daily usage statistics."""
    cursor = conn.execute(
        f"""
        SELECT
            DATE(s.start_time) as date,
            COUNT(*) as sessions,
            SUM(s.message_count) as messages,
            SUM(m_stats.input_tokens) as input_tokens,
            SUM(m_stats.output_tokens) as output_tokens
        FROM sessions s
        LEFT JOIN (
            SELECT session_id,
                   SUM(input_tokens) as input_tokens,
                   SUM(output_tokens) as output_tokens
            FROM messages
            GROUP BY session_id
        ) m_stats ON s.session_id = m_stats.session_id
        WHERE {where_sql} AND s.start_time IS NOT NULL
        GROUP BY DATE(s.start_time)
        ORDER BY date DESC
        LIMIT 30
    """,
        params,
    )
    days = cursor.fetchall()

    print("=" * 60)
    print("DAILY STATISTICS (Last 30 days)")
    print("=" * 60)

    print(f"\n{'Date':<12} {'Sessions':>10} {'Messages':>10} {'Input Tokens':>15}")
    print("-" * 50)

    for row in days:
        print(
            f"{row['date'] or 'unknown':<12} {row['sessions']:>10,} {row['messages'] or 0:>10,} {row['input_tokens'] or 0:>15,}"
        )

    print()


def format_duration_hm(seconds: float) -> str:
    """Format seconds as Xh Ym."""
    if seconds < 60:
        return f"{seconds:.0f}s"
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    if hours > 0:
        return f"{hours}h {minutes}m"
    return f"{minutes}m"


def calculate_daily_work_time(conn: sqlite3.Connection, where_sql: str, params: list) -> dict:
    """Calculate work time per day from message timestamps.

    This properly distributes work time across days, even for sessions
    that span multiple days. Uses gap detection (30 min threshold) to
    identify work periods.

    IMPORTANT: Merges ALL timestamps across all files to avoid double-counting
    overlapping time from concurrent agents.

    Returns dict: {date_str: {'work_seconds': float, 'messages': int, 'work_periods': int}}
    """
    # Get ALL message timestamps from matching sessions, ordered globally by time
    cursor = conn.execute(
        f"""
        SELECT m.timestamp
        FROM messages m
        JOIN sessions s ON m.file_path = s.file_path
        WHERE {where_sql} AND m.timestamp IS NOT NULL
        ORDER BY m.timestamp
    """,
        params,
    )

    # Collect all timestamps
    all_timestamps = [row["timestamp"] for row in cursor]

    if not all_timestamps:
        return {}

    daily_stats = {}  # date -> {work_seconds, messages, work_periods}
    gap_threshold = WORK_PERIOD_GAP_THRESHOLD

    def parse_ts(ts):
        return datetime.fromisoformat(ts.replace("Z", "+00:00"))

    def get_date(dt):
        return dt.strftime("%Y-%m-%d")

    def add_period_time(stats, start_dt, end_dt, is_new_period=False):
        """Add work period time, splitting across day boundaries if needed."""
        if start_dt >= end_dt:
            return

        current = start_dt
        first_day = True
        while current < end_dt:
            date_str = current.strftime("%Y-%m-%d")
            if date_str not in stats:
                stats[date_str] = {"work_seconds": 0.0, "messages": 0, "work_periods": 0}

            # Count new work period only on its first day
            if is_new_period and first_day:
                stats[date_str]["work_periods"] += 1
                first_day = False

            # Calculate end of this day
            next_midnight = (current + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)

            if next_midnight >= end_dt:
                # Period ends before midnight
                stats[date_str]["work_seconds"] += (end_dt - current).total_seconds()
                break
            else:
                # Period continues past midnight
                stats[date_str]["work_seconds"] += (next_midnight - current).total_seconds()
                current = next_midnight

    # Process all timestamps globally (merged across all files)
    prev_ts = None
    period_start = None

    for ts_str in all_timestamps:
        ts = parse_ts(ts_str)
        date_str = get_date(ts)

        # Initialize date entry if needed
        if date_str not in daily_stats:
            daily_stats[date_str] = {"work_seconds": 0.0, "messages": 0, "work_periods": 0}

        # Count message
        daily_stats[date_str]["messages"] += 1

        if prev_ts is None:
            # First message - start first period
            period_start = ts
            daily_stats[date_str]["work_periods"] += 1
        else:
            gap = (ts - prev_ts).total_seconds()

            if gap > gap_threshold:
                # End previous period, start new one
                add_period_time(daily_stats, period_start, prev_ts)
                period_start = ts
                daily_stats[date_str]["work_periods"] += 1

        prev_ts = ts

    # Don't forget the last period
    if period_start and prev_ts:
        add_period_time(daily_stats, period_start, prev_ts)

    return daily_stats


def display_time_stats(conn: sqlite3.Connection, where_sql: str, params: list):
    """Display time tracking statistics with daily breakdown."""

    # Calculate daily work time from message timestamps
    daily_stats = calculate_daily_work_time(conn, where_sql, params)

    # Calculate totals
    total_work = sum(d["work_seconds"] for d in daily_stats.values())
    total_messages = sum(d["messages"] for d in daily_stats.values())
    total_periods = sum(d["work_periods"] for d in daily_stats.values())

    # Get session count
    cursor = conn.execute(
        f"""
        SELECT COUNT(*) as num_files
        FROM sessions s
        WHERE {where_sql}
    """,
        params,
    )
    num_files = cursor.fetchone()["num_files"] or 0

    # Get date range
    dates = sorted(daily_stats.keys()) if daily_stats else []
    first_date = dates[0] if dates else None
    last_date = dates[-1] if dates else None

    print("=" * 70)
    print("TIME TRACKING")
    print("=" * 70)

    print("\nðŸ“Š Summary")
    print(f"   Total work time: {format_duration_hm(total_work)}")
    print(f"   Work periods: {total_periods}")
    print(f"   Session files: {num_files}")
    if first_date and last_date:
        print(f"   Date range: {first_date} to {last_date}")

    if daily_stats:
        print("\nðŸ“… Daily Breakdown")
        print(f"\n{'Date':<12} {'Work Time':>12} {'Periods':>10} {'Messages':>10}")
        print("-" * 46)

        # Sort by date descending
        for date_str in sorted(daily_stats.keys(), reverse=True):
            stats = daily_stats[date_str]
            work_time = format_duration_hm(stats["work_seconds"])
            print(f"{date_str:<12} {work_time:>12} {stats['work_periods']:>10} {stats['messages']:>10}")

        print("-" * 46)
        print(f"{'TOTAL':<12} {format_duration_hm(total_work):>12} {total_periods:>10} {total_messages:>10}")

    print()


# ============================================================================
# Commands
# ============================================================================


def cmd_list(args):
    """List sessions for a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, "remote", None)

    # Parse date filters
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    # Handle WSL, Windows, or remote listing
    if remote_host:
        # Check if this is a Windows remote (windows or windows://username)
        if is_windows_remote(remote_host):
            # Extract username if specified (windows://username)
            username = None
            if remote_host.startswith("windows://"):
                username = remote_host[10:]  # Remove 'windows://' prefix

            # Get Windows projects directory (uses /mnt/c/Users/... path)
            projects_dir = get_windows_projects_dir(username)

            # Use local file operations on Windows filesystem
            # Temporarily override get_claude_projects_dir to return Windows path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_windows_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = get_windows_override

            try:
                # Get patterns list (multiple patterns support)
                patterns = getattr(args, "patterns", None)
                if patterns is None:
                    patterns = [getattr(args, "workspace", "")]

                # Collect sessions from all patterns with deduplication
                all_sessions = []
                seen_files = set()
                for pattern in patterns:
                    pattern_sessions = get_workspace_sessions(pattern, since_date=since_date, until_date=until_date)
                    for session in pattern_sessions:
                        file_key = str(session["file"])
                        if file_key not in seen_files:
                            seen_files.add(file_key)
                            all_sessions.append(session)

                # Filter out remote caches to prevent showing cached data
                # Only show native Windows workspaces, not cached remote/wsl data
                sessions = [
                    s
                    for s in all_sessions
                    if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                ]

                if not sessions:
                    sys.stderr.write("Error: No native sessions found in Windows\n")
                    sys.exit(1)

                workspaces_only = getattr(args, "workspaces_only", False)

                if workspaces_only:
                    # Just print workspace names
                    workspaces = set()
                    for session in sessions:
                        workspaces.add(session["workspace_readable"])

                    for ws in sorted(workspaces):
                        print(ws)
                else:
                    # Print header and sessions
                    print("WORKSPACE\tFILE\tMESSAGES\tDATE")
                    for session in sessions:
                        print(
                            f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                        )

            finally:
                # Restore original function
                current_module.get_claude_projects_dir = original_get_claude_projects_dir

            return  # Done with Windows handling

        # Check if this is a WSL remote (wsl://DistroName)
        elif is_wsl_remote(remote_host):
            # Extract distro name from wsl://DistroName
            distro_name = remote_host[6:]  # Remove 'wsl://' prefix

            # Get WSL projects directory (uses Windows path //wsl.localhost/...)
            projects_dir = get_wsl_projects_dir(distro_name)

            # Use local file operations on WSL filesystem
            # Temporarily override get_claude_projects_dir to return WSL path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_wsl_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = get_wsl_override

            try:
                # Get patterns list (multiple patterns support)
                patterns = getattr(args, "patterns", None)
                if patterns is None:
                    patterns = [getattr(args, "workspace", "")]

                # Collect sessions from all patterns with deduplication
                all_sessions = []
                seen_files = set()
                for pattern in patterns:
                    pattern_sessions = get_workspace_sessions(pattern, since_date=since_date, until_date=until_date)
                    for session in pattern_sessions:
                        file_key = str(session["file"])
                        if file_key not in seen_files:
                            seen_files.add(file_key)
                            all_sessions.append(session)

                # Filter out remote caches to prevent showing cached data
                # Only show native WSL workspaces, not cached remote/wsl data
                sessions = [
                    s
                    for s in all_sessions
                    if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                ]

                if not sessions:
                    sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
                    sys.exit(1)

                workspaces_only = getattr(args, "workspaces_only", False)

                if workspaces_only:
                    # Just print workspace names
                    workspaces = set()
                    for session in sessions:
                        workspaces.add(session["workspace_readable"])

                    for ws in sorted(workspaces):
                        print(ws)
                else:
                    # Print header and sessions
                    print("WORKSPACE\tFILE\tMESSAGES\tDATE")
                    for session in sessions:
                        print(
                            f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                        )
            finally:
                # Restore original function
                current_module.get_claude_projects_dir = original_get_claude_projects_dir

            return  # Done with WSL handling

        # SSH remote handling
        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Get patterns list (multiple patterns support)
        patterns = getattr(args, "patterns", None)
        if patterns is None:
            patterns = [getattr(args, "workspace", "")]
        workspaces_only = getattr(args, "workspaces_only", False)

        # Helper to check if workspace matches any pattern
        def matches_any_pattern(ws_name, patterns_list):
            if not patterns_list or patterns_list == [""] or patterns_list == ["*"] or patterns_list == ["all"]:
                return True
            return any(pattern in ws_name for pattern in patterns_list if pattern)

        # Use fast batch approach for workspaces-only mode
        if workspaces_only:
            batch_workspaces = get_remote_workspaces_batch(remote_host)

            if not batch_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by patterns (any pattern matches)
            batch_workspaces = [ws for ws in batch_workspaces if matches_any_pattern(ws["encoded"], patterns)]

            if not batch_workspaces:
                pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
                sys.stderr.write(f"Error: No workspaces matching pattern '{pattern_str}'\n")
                sys.exit(1)

            # Just print workspace names, one per line
            for ws_info in sorted(batch_workspaces, key=lambda x: x["decoded"]):
                print(ws_info["decoded"])

        else:
            # Detailed mode - get full session info
            remote_workspaces = list_remote_workspaces(remote_host)

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by patterns (any pattern matches)
            remote_workspaces = [ws for ws in remote_workspaces if matches_any_pattern(ws, patterns)]

            if not remote_workspaces:
                pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
                sys.stderr.write(f"Error: No workspaces matching pattern '{pattern_str}'\n")
                sys.exit(1)

            # Get session info for each workspace
            sessions = []

            for remote_workspace in remote_workspaces:
                readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)

                # Get session files from this workspace
                workspace_sessions = get_remote_session_info(remote_host, remote_workspace)

                for session_info in workspace_sessions:
                    # Apply date filtering
                    modified_date = session_info["modified"].replace(hour=0, minute=0, second=0, microsecond=0)
                    if since_date and modified_date < since_date:
                        continue
                    if until_date and modified_date > until_date:
                        continue

                    sessions.append(
                        {
                            "workspace": remote_workspace,
                            "workspace_readable": readable_name,
                            "filename": session_info["filename"],
                            "size_kb": session_info["size_kb"],
                            "modified": session_info["modified"],
                            "message_count": session_info["message_count"],
                        }
                    )

            if not sessions:
                sys.stderr.write("Error: No sessions found\n")
                sys.exit(1)

            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in sessions:
                print(
                    f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                )

    else:
        # Local listing - handle multiple patterns
        patterns = getattr(args, "patterns", None)
        if patterns is None:
            # Backward compatibility: use single workspace attribute
            patterns = [getattr(args, "workspace", "")]

        # Collect sessions from all patterns
        all_sessions = []
        seen_files = set()  # Deduplicate by file path
        for pattern in patterns:
            sessions = get_workspace_sessions(pattern, since_date=since_date, until_date=until_date)
            for session in sessions:
                file_key = str(session["file"])
                if file_key not in seen_files:
                    seen_files.add(file_key)
                    all_sessions.append(session)

        # Check if workspaces-only mode
        workspaces_only = getattr(args, "workspaces_only", False)

        if not all_sessions:
            # For workspaces listing (lsw), silently return empty (no error)
            # For sessions listing (lss), show error message
            if not workspaces_only:
                pattern_str = ", ".join(patterns) if len(patterns) > 1 else patterns[0]
                sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
                sys.exit(1)
            # For lsw with no matches, just exit successfully with no output
            return

        if workspaces_only:
            # Just print workspace names, one per line
            workspaces = set()
            for session in all_sessions:
                workspaces.add(session["workspace_readable"])

            for ws in sorted(workspaces):
                print(ws)

        else:
            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in all_sessions:
                print(
                    f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                )


def cmd_convert(args):
    """Convert a single .jsonl file to markdown."""
    remote_host = getattr(args, "remote", None)

    if remote_host:
        # Handle remote file conversion
        import subprocess
        import tempfile

        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Download file to temporary location
        with tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            # Use scp to download the file
            result = subprocess.run(
                ["scp", f"{remote_host}:{args.jsonl_file}", str(tmp_path)], capture_output=True, text=True, timeout=60
            )

            if result.returncode != 0:
                sys.stderr.write(f"Error downloading file: {result.stderr}\n")
                tmp_path.unlink()
                sys.exit(1)

            # Convert the downloaded file
            jsonl_file = tmp_path
            filename = Path(args.jsonl_file).name
            output_file = Path(args.output) if args.output else Path(filename).with_suffix(".md")

            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding="utf-8")

            print(output_file)

        finally:
            # Clean up temporary file
            if tmp_path.exists():
                tmp_path.unlink()

    else:
        # Handle local file conversion
        jsonl_file = Path(args.jsonl_file)

        if not jsonl_file.exists():
            sys.stderr.write(f"Error: {jsonl_file} not found\n")
            sys.exit(1)

        output_file = Path(args.output) if args.output else jsonl_file.with_suffix(".md")

        try:
            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding="utf-8")

            print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting file: {e}\n")
            sys.exit(1)


def cmd_batch(args):
    """Batch convert all sessions from a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, "remote", None)

    # Parse date filters
    since_str = getattr(args, "since", None)
    until_str = getattr(args, "until", None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    output_dir = Path(args.output_dir)

    # Handle Windows, WSL, or remote export
    if remote_host:
        # Check if this is a Windows remote (windows or windows://username)
        if is_windows_remote(remote_host):
            # Extract username if specified (windows://username)
            username = None
            if remote_host.startswith("windows://"):
                username = remote_host[10:]  # Remove 'windows://' prefix

            # Get Windows projects directory (uses /mnt/c/Users/... path)
            projects_dir = get_windows_projects_dir(username)

            # Use local file operations on Windows filesystem
            # Temporarily override get_claude_projects_dir to return Windows path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_windows_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = get_windows_override

            try:
                # Get patterns list (multiple patterns support)
                patterns = getattr(args, "patterns", None)
                if patterns is None:
                    patterns = [getattr(args, "workspace", "")]

                # Collect sessions from all patterns with deduplication
                all_sessions = []
                seen_files = set()
                for pattern in patterns if patterns else [""]:
                    pattern_sessions = get_workspace_sessions(
                        pattern, quiet=True, since_date=since_date, until_date=until_date
                    )
                    for session in pattern_sessions:
                        file_key = str(session["file"])
                        if file_key not in seen_files:
                            seen_files.add(file_key)
                            all_sessions.append(session)

                # Filter out remote caches to prevent circular fetching
                # Only export native Windows workspaces, not cached remote/wsl data
                sessions = [
                    s
                    for s in all_sessions
                    if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                ]
            finally:
                # Restore original function
                current_module.get_claude_projects_dir = original_get_claude_projects_dir

            if not sessions:
                sys.stderr.write("Error: No native sessions found in Windows\n")
                sys.exit(1)

            # Continue with export processing below (same as local)

        # Check if this is a WSL remote (wsl://DistroName)
        elif is_wsl_remote(remote_host):
            # Extract distro name from wsl://DistroName
            distro_name = remote_host[6:]  # Remove 'wsl://' prefix

            # Get WSL projects directory (uses Windows path //wsl.localhost/...)
            projects_dir = get_wsl_projects_dir(distro_name)

            # Use local file operations on WSL filesystem
            # Temporarily override get_claude_projects_dir to return WSL path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_wsl_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            current_module.get_claude_projects_dir = get_wsl_override

            try:
                # Get patterns list (multiple patterns support)
                patterns = getattr(args, "patterns", None)
                if patterns is None:
                    patterns = [getattr(args, "workspace", "")]

                # Collect sessions from all patterns with deduplication
                all_sessions = []
                seen_files = set()
                for pattern in patterns if patterns else [""]:
                    pattern_sessions = get_workspace_sessions(
                        pattern, quiet=True, since_date=since_date, until_date=until_date
                    )
                    for session in pattern_sessions:
                        file_key = str(session["file"])
                        if file_key not in seen_files:
                            seen_files.add(file_key)
                            all_sessions.append(session)

                # Filter out remote caches to prevent circular fetching
                # Only export native WSL workspaces, not cached remote/wsl data
                sessions = [
                    s
                    for s in all_sessions
                    if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                ]
            finally:
                # Restore original function
                current_module.get_claude_projects_dir = original_get_claude_projects_dir

            if not sessions:
                sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
                sys.exit(1)

            # Continue with export processing below (same as local)

        else:
            # SSH remote handling - Check SSH connectivity
            if not check_ssh_connection(remote_host):
                sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
                sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
                sys.exit(1)

            # Get patterns list (multiple patterns support)
            patterns = getattr(args, "patterns", None)
            if patterns is None:
                patterns = [getattr(args, "workspace", "")]

            # Get remote workspaces
            all_remote_workspaces = list_remote_workspaces(remote_host)

            if not all_remote_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by patterns (any pattern matches) - be lenient
            def matches_any_pattern(ws_name, patterns_list):
                if not patterns_list or patterns_list == [""] or patterns_list == ["*"] or patterns_list == ["all"]:
                    return True
                return any(pattern in ws_name for pattern in patterns_list if pattern)

            remote_workspaces = [ws for ws in all_remote_workspaces if matches_any_pattern(ws, patterns)]

            if not remote_workspaces:
                # Be lenient - just return empty sessions, don't exit
                sessions = []
            else:
                # Fetch each workspace to local cache (silent)
                local_projects_dir = get_claude_projects_dir()
                hostname = get_remote_hostname(remote_host)

                for remote_workspace in remote_workspaces:
                    result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)

                    if not result["success"]:
                        sys.stderr.write(f"Error fetching {remote_workspace}: {result.get('error', 'Unknown error')}\n")

                # Now export from cached location with deduplication
                cached_prefix = f"remote_{hostname}_"
                all_sessions = []
                seen_files = set()

                # Get all sessions from remote cache (prefix matches all cached workspaces)
                # Use include_cached=True to access remote_* directories
                all_cached_sessions = get_workspace_sessions(
                    cached_prefix, quiet=True, since_date=since_date, until_date=until_date, include_cached=True
                )

                # Filter by patterns if specified (pattern matches workspace name)
                for session in all_cached_sessions:
                    ws_name = session["workspace"]
                    if not patterns or patterns == [""] or patterns == ["*"] or patterns == ["all"]:
                        matches = True
                    else:
                        matches = any(pattern in ws_name for pattern in patterns if pattern)

                    if matches:
                        file_key = str(session["file"])
                        if file_key not in seen_files:
                            seen_files.add(file_key)
                            all_sessions.append(session)

                sessions = all_sessions

    else:
        # Local export - handle multiple patterns
        patterns = getattr(args, "patterns", None)
        if patterns is None:
            patterns = [getattr(args, "workspace", "")]

        # Collect sessions from all patterns with deduplication
        all_sessions = []
        seen_files = set()
        for pattern in patterns if patterns else [""]:
            pattern_sessions = get_workspace_sessions(pattern, quiet=True, since_date=since_date, until_date=until_date)
            for session in pattern_sessions:
                file_key = str(session["file"])
                if file_key not in seen_files:
                    seen_files.add(file_key)
                    all_sessions.append(session)

        sessions = all_sessions

    if not sessions:
        # Check if we should be lenient (called from export-all)
        lenient = getattr(args, "lenient", False)
        if not lenient:
            pattern_str = ", ".join(patterns) if patterns and len(patterns) > 1 else (patterns[0] if patterns else "")
            sys.stderr.write(f"Error: No sessions found matching '{pattern_str}'\n")
            sys.exit(1)
        # In lenient mode, just return without error
        return

    output_dir.mkdir(parents=True, exist_ok=True)

    # Convert each session
    force = getattr(args, "force", False)
    flat = getattr(args, "flat", False)
    remote_host = getattr(args, "remote", None)

    for session in sessions:
        jsonl_file = session["file"]

        # Extract first timestamp and create timestamped filename
        first_ts = get_first_timestamp(jsonl_file)
        if first_ts:
            try:
                dt = datetime.fromisoformat(first_ts.replace("Z", "+00:00"))
                ts_prefix = dt.strftime("%Y%m%d%H%M%S")
            except Exception:
                ts_prefix = None
        else:
            ts_prefix = None

        # Generate filename with optional source tag
        if not flat:
            # Organized structure (default) - workspace subdirectories with source tags
            # Get source tag
            source_tag = get_source_tag(remote_host)

            # Generate filename: [source_]timestamp_sessionid.md
            if ts_prefix:
                output_name = f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
            else:
                output_name = f"{source_tag}{jsonl_file.stem}.md"

            # Create workspace subdirectory
            workspace_name = get_workspace_name_from_path(session["workspace"])
            workspace_subdir = output_dir / workspace_name
            workspace_subdir.mkdir(parents=True, exist_ok=True)
            output_file = workspace_subdir / output_name
        else:
            # Flat structure - all files in one directory, no source tags
            if ts_prefix:
                output_name = f"{ts_prefix}_{jsonl_file.stem}.md"
            else:
                output_name = f"{jsonl_file.stem}.md"
            output_file = output_dir / output_name

        # Check if we should skip (incremental export)
        if not force and output_file.exists():
            source_mtime = jsonl_file.stat().st_mtime
            output_mtime = output_file.stat().st_mtime

            if output_mtime >= source_mtime:
                continue

        try:
            minimal = getattr(args, "minimal", False)
            split_lines = getattr(args, "split", None)

            # Read messages from JSONL
            messages = read_jsonl_messages(jsonl_file)

            # Check if we should split into multiple parts
            if split_lines and len(messages) > 0:
                parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)

                if parts:
                    # Save multiple part files
                    for part_num, total_parts, part_md, start_msg, end_msg in parts:
                        base_name = output_name.rsplit(".md", 1)[0]
                        part_filename = f"{base_name}_part{part_num}.md"
                        part_file = output_file.parent / part_filename

                        # Add navigation footer (remove emoji)
                        nav_lines = ["\n---\n"]
                        nav_parts = [f"**Part {part_num} of {total_parts}**"]

                        if part_num > 1:
                            prev_filename = f"{base_name}_part{part_num - 1}.md"
                            nav_parts.append(f"[â† Part {part_num - 1}]({prev_filename})")

                        if part_num < total_parts:
                            next_filename = f"{base_name}_part{part_num + 1}.md"
                            nav_parts.append(f"[Part {part_num + 1} â†’]({next_filename})")

                        nav_lines.append("> " + " | ".join(nav_parts))

                        # Write part file with navigation
                        part_file.write_text(part_md + "\n".join(nav_lines), encoding="utf-8")

                    # Print all part files
                    for part_num in range(1, total_parts + 1):
                        part_filename = f"{base_name}_part{part_num}.md"
                        print(output_file.parent / part_filename)
                else:
                    # Splitting not needed, use single file
                    markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                    output_file.write_text(markdown, encoding="utf-8")
                    print(output_file)
            else:
                # No splitting, use single file
                markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                output_file.write_text(markdown, encoding="utf-8")
                print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting {jsonl_file.name}: {e}\n")


def cmd_version(args):
    """Show version information."""
    print(f"claude-history {__version__}")


def cmd_list_wsl(args):
    """List available WSL distributions with Claude Code workspaces."""
    distributions = get_wsl_distributions()

    if not distributions:
        sys.stderr.write("No WSL distributions found or WSL is not available.\n")
        sys.stderr.write("Make sure WSL is installed on Windows.\n")
        sys.exit(1)

    # Filter to only distributions with Claude Code
    claude_distros = [d for d in distributions if d["has_claude"]]

    if not claude_distros:
        sys.stderr.write("No WSL distributions with Claude Code workspaces found.\n")
        sys.exit(1)

    # Print tab-separated output
    print("DISTRO\tUSERNAME\tPATH")
    for distro in claude_distros:
        print(f"{distro['name']}\t{distro['username']}\t{distro['path']}")


def cmd_list_windows(args):
    """List available Windows users with Claude Code workspaces (from WSL)."""
    if not is_running_in_wsl():
        sys.stderr.write("Error: --list-windows is only available from WSL.\n")
        sys.stderr.write("To access Windows sessions from Windows, use the tool directly.\n")
        sys.exit(1)

    users = get_windows_users_with_claude()

    if not users:
        sys.stderr.write("No Windows users with Claude Code workspaces found.\n")
        sys.stderr.write("Make sure Claude Code is installed on Windows.\n")
        sys.exit(1)

    # Print tab-separated output
    print("USERNAME\tDRIVE\tWORKSPACES\tPATH")
    for user in users:
        print(f"{user['username']}\t{user['drive']}\t{user['workspace_count']}\t{user['path']}")


def cmd_list_all_sources(args):
    """List workspaces or sessions from all sources: local, WSL/Windows, and remotes."""
    in_wsl = is_running_in_wsl()
    workspaces_only = getattr(args, "workspaces_only", False)

    # Handle multiple patterns
    patterns = getattr(args, "patterns", None)
    if patterns is None:
        # Backward compatibility
        patterns = [getattr(args, "workspace", "")]

    since_date = getattr(args, "since_date", None)
    until_date = getattr(args, "until_date", None)
    remotes = getattr(args, "remotes", []) or []

    # Include saved sources
    saved_sources = get_saved_sources()
    for src in saved_sources:
        if src not in remotes:
            remotes.append(src)

    all_results = []  # List of (source_label, sessions)

    # Helper to get sessions for multiple patterns with deduplication
    def get_sessions_for_patterns(patterns_list, since_date, until_date):
        all_sessions = []
        seen_files = set()
        for pattern in patterns_list:
            sessions = get_workspace_sessions(pattern, since_date=since_date, until_date=until_date, quiet=True)
            for session in sessions:
                file_key = str(session["file"])
                if file_key not in seen_files:
                    seen_files.add(file_key)
                    all_sessions.append(session)
        return all_sessions

    # 1. Get from local
    try:
        local_label = "Local (WSL)" if in_wsl else "Local"
        sessions = get_sessions_for_patterns(patterns, since_date, until_date)
        # Filter out cached remote/wsl workspaces
        native_sessions = [
            s
            for s in sessions
            if not s["workspace"].startswith("remote_")
            and not s["workspace"].startswith("wsl_")
            and not s["workspace"].startswith("windows_")
        ]
        if native_sessions:
            all_results.append((local_label, native_sessions))
    except Exception as e:
        sys.stderr.write(f"Error accessing local: {e}\n")

    # 2a. Get from Windows (when running in WSL)
    if in_wsl:
        try:
            windows_users = get_windows_users_with_claude()
            for user in windows_users:
                try:
                    projects_dir = get_windows_projects_dir(user["username"])
                    original_get_claude = get_claude_projects_dir

                    def temp_override():
                        return projects_dir

                    current_module = sys.modules[__name__]
                    current_module.get_claude_projects_dir = temp_override

                    try:
                        sessions = get_sessions_for_patterns(patterns, since_date, until_date)
                        native_sessions = [
                            s
                            for s in sessions
                            if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                        ]
                        if native_sessions:
                            all_results.append((f"Windows ({user['username']})", native_sessions))
                    finally:
                        current_module.get_claude_projects_dir = original_get_claude
                except Exception as e:
                    sys.stderr.write(f"Error accessing Windows ({user['username']}): {e}\n")
        except Exception as e:
            sys.stderr.write(f"Error listing Windows users: {e}\n")

    # 2b. Get from WSL (when running on Windows)
    if not in_wsl:
        try:
            wsl_distros = get_wsl_distributions()
            claude_distros = [d for d in wsl_distros if d["has_claude"]]

            for distro in claude_distros:
                try:
                    projects_dir = get_wsl_projects_dir(distro["name"])
                    original_get_claude = get_claude_projects_dir

                    def temp_override():
                        return projects_dir

                    current_module = sys.modules[__name__]
                    current_module.get_claude_projects_dir = temp_override

                    try:
                        sessions = get_sessions_for_patterns(patterns, since_date, until_date)
                        native_sessions = [
                            s
                            for s in sessions
                            if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                        ]
                        if native_sessions:
                            all_results.append((f"WSL ({distro['name']})", native_sessions))
                    finally:
                        current_module.get_claude_projects_dir = original_get_claude
                except Exception as e:
                    sys.stderr.write(f"Error accessing WSL ({distro['name']}): {e}\n")
        except Exception as e:
            sys.stderr.write(f"Error listing WSL distributions: {e}\n")

    # 3. Get from SSH remotes
    for remote in remotes:
        try:
            # Check SSH connection
            if not check_ssh_connection(remote):
                sys.stderr.write(f"Cannot connect to remote: {remote}\n")
                continue

            hostname = get_remote_hostname(remote)
            all_remote_workspaces = list_remote_workspaces(remote)

            # Filter by patterns if specified (any pattern matches)
            if patterns and patterns != [""]:
                remote_workspaces = []
                for ws in all_remote_workspaces:
                    for pattern in patterns:
                        if pattern in ws:
                            remote_workspaces.append(ws)
                            break  # Don't add same workspace twice
            else:
                remote_workspaces = all_remote_workspaces

            if remote_workspaces:
                remote_sessions = []
                for ws_name in remote_workspaces:
                    ws_info = get_remote_session_info(remote, ws_name)
                    for session in ws_info:
                        # Apply date filtering
                        if since_date and session.get("modified") and session["modified"].date() < since_date:
                            continue
                        if until_date and session.get("modified") and session["modified"].date() > until_date:
                            continue
                        session["workspace"] = ws_name
                        session["workspace_readable"] = normalize_workspace_name(ws_name)
                        remote_sessions.append(session)

                if remote_sessions:
                    all_results.append((f"Remote ({hostname})", remote_sessions))
        except Exception as e:
            sys.stderr.write(f"Error accessing remote {remote}: {e}\n")

    # Output results
    if not all_results:
        if workspaces_only:
            sys.stderr.write("No workspaces found across any sources.\n")
        else:
            sys.stderr.write("No sessions found across any sources.\n")
        return

    if workspaces_only:
        # Print workspaces grouped by source
        for source_label, sessions in all_results:
            print(f"# {source_label}")
            workspaces = sorted({s["workspace_readable"] for s in sessions})
            for ws in workspaces:
                print(f"  {ws}")
            print()
    else:
        # Print sessions grouped by source
        print("SOURCE\tWORKSPACE\tFILE\tMESSAGES\tDATE")
        for source_label, sessions in all_results:
            for session in sessions:
                print(
                    f"{source_label}\t{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}"
                )


def cmd_lsh(args):
    """List all home folders with Claude Code installations."""
    # Check for subcommands first
    lsh_action = getattr(args, "lsh_action", None)

    if lsh_action == "add":
        cmd_lsh_add(args)
        return
    elif lsh_action == "remove":
        cmd_lsh_remove(args)
        return
    elif lsh_action == "clear":
        cmd_lsh_clear(args)
        return

    # Default: list all hosts
    in_wsl = is_running_in_wsl()

    # Determine what to show based on flags
    # If no flags are specified, show all
    # If any flag is specified, show only that one
    show_local = args.local or (not args.wsl and not args.windows and not args.remotes)
    show_wsl = args.wsl
    show_windows = args.windows
    show_remotes = getattr(args, "remotes", False) or (not args.local and not args.wsl and not args.windows)

    # 1. Show local home folder
    if show_local:
        try:
            projects_dir = get_claude_projects_dir()
            if projects_dir.exists():
                # Count workspaces
                workspace_count = len([d for d in projects_dir.iterdir() if d.is_dir() and not d.name.startswith(".")])

                print("Local:")
                print(f"  {projects_dir.parent}\t{workspace_count} workspaces")
                print()
        except Exception as e:
            sys.stderr.write(f"Error accessing local: {e}\n")

    # 2. Show WSL distributions (when on Windows)
    if show_wsl and not in_wsl:
        print("WSL Distributions:")
        try:
            wsl_distros = get_wsl_distributions()
            claude_distros = [d for d in wsl_distros if d["has_claude"]]

            if claude_distros:
                for distro in claude_distros:
                    # Count workspaces
                    projects_dir = get_wsl_projects_dir(distro["name"])
                    workspace_count = len(
                        [d for d in projects_dir.iterdir() if d.is_dir() and not d.name.startswith(".")]
                    )

                    print(f"  {distro['name']}\t{distro['username']}\t{workspace_count} workspaces\t{projects_dir}")
            else:
                print("  No WSL distributions with Claude Code found")
            print()
        except Exception as e:
            sys.stderr.write(f"Error accessing WSL: {e}\n")

    # 3. Show Windows users (when on WSL)
    if show_windows and in_wsl:
        print("Windows Users:")
        try:
            windows_users = get_windows_users_with_claude()

            if windows_users:
                for user in windows_users:
                    print(f"  {user['username']}\t{user['path']}\t{user['workspace_count']} workspaces")
            else:
                print("  No Windows users with Claude Code found")
            print()
        except Exception as e:
            sys.stderr.write(f"Error accessing Windows: {e}\n")

    # 4. Show configured SSH remotes
    if show_remotes:
        sources = get_saved_sources()
        print("SSH Remotes:")
        if sources:
            for source in sources:
                print(f"  {source}")
        else:
            print("  (none configured)")
        print()


def cmd_lsh_add(args):
    """Add a remote source to configuration."""
    source = args.source

    # WSL and Windows are auto-detected, no need to save them
    if source.startswith("wsl://") or source == "windows":
        sys.stderr.write(f"Note: {source} is auto-detected by --as flag, no need to add it.\n")
        sys.stderr.write("This command is for SSH remotes (user@hostname).\n")
        return

    # Validate source format (SSH remote)
    if "@" not in source:
        sys.stderr.write(f"Error: Invalid source format: {source}\n")
        sys.stderr.write("Expected: user@hostname (e.g., alice@server.example.com)\n")
        sys.exit(1)

    config = load_config()
    sources = config.get("sources", [])

    if source in sources:
        print(f"Source '{source}' already configured.")
        return

    sources.append(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Added source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_remove(args):
    """Remove a remote source from configuration."""
    source = args.source

    config = load_config()
    sources = config.get("sources", [])

    if source not in sources:
        sys.stderr.write(f"Error: Source '{source}' not found.\n")
        sys.stderr.write(f"Configured sources: {', '.join(sources) if sources else '(none)'}\n")
        sys.exit(1)

    sources.remove(source)
    config["sources"] = sources

    if save_config(config):
        print(f"Removed source: {source}")
    else:
        sys.exit(1)


def cmd_lsh_clear(args):
    """Clear all saved remote sources."""
    config = load_config()
    config["sources"] = []

    if save_config(config):
        print("Cleared all SSH remotes.")
    else:
        sys.exit(1)


def cmd_reset(args):
    """Reset claude-history data (database, settings, aliases)."""
    config_dir = get_config_dir()

    what = getattr(args, "what", None)
    yes = getattr(args, "yes", False)

    # Determine what to reset
    reset_db = what in (None, "db", "all")
    reset_settings = what in (None, "settings", "all")
    reset_aliases = what in (None, "aliases", "all")

    # Build list of files to delete
    items = []
    if reset_db:
        db_path = get_metrics_db_path()
        if db_path.exists():
            items.append(("Metrics database", db_path))
    if reset_settings:
        config_file = config_dir / "config.json"
        if config_file.exists():
            items.append(("Settings (SSH remotes)", config_file))
    if reset_aliases:
        aliases_file = config_dir / "aliases.json"
        if aliases_file.exists():
            items.append(("Aliases", aliases_file))

    if not items:
        print("Nothing to reset.")
        return

    # Show what will be deleted
    print("This will delete:")
    for desc, path in items:
        print(f"  - {desc}: {path}")

    # Confirm unless -y flag
    if not yes:
        try:
            response = input("\nProceed? [y/N] ").strip().lower()
            if response not in ("y", "yes"):
                print("Cancelled.")
                return
        except (EOFError, KeyboardInterrupt):
            print("\nCancelled.")
            return

    # Perform reset
    print()
    for desc, path in items:
        path.unlink()
        print(f"Deleted {desc.lower()}")


def cmd_fetch(args):
    """Fetch remote conversation sessions via SSH."""
    remote_host = args.remote_host
    workspace_pattern = getattr(args, "pattern", None) or ""

    # Check SSH connectivity
    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    # Get hostname for directory prefix
    hostname = get_remote_hostname(remote_host)

    # List remote workspaces
    remote_workspaces = list_remote_workspaces(remote_host)

    if not remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    # Filter by pattern if specified
    if workspace_pattern and workspace_pattern not in ("", "*", "all"):
        filtered = [ws for ws in remote_workspaces if workspace_pattern in ws]
        if not filtered:
            sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
            sys.exit(1)
        remote_workspaces = filtered

    # Get local projects directory
    local_projects_dir = get_claude_projects_dir()

    # Fetch each workspace (silent on success)
    for remote_workspace in remote_workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        # New naming: remote_hostname_path
        workspace_path = remote_workspace.lstrip("-")
        local_name = f"remote_{hostname}_{workspace_path}"

        result = fetch_workspace_files(remote_host, remote_workspace, local_projects_dir, hostname)

        if result["success"]:
            # Print the local path where it was cached
            print(normalize_workspace_name(local_name, verify_local=False))
        else:
            error = result.get("error", "Unknown error")
            sys.stderr.write(f"Error fetching {readable_name}: {error}\n")


def validate_export_all_sources(args, in_wsl):
    """
    Validate all sources before starting export.
    Returns (success, errors) where errors is a list of error messages.
    """
    errors = []

    # 1. Check local access
    try:
        projects_dir = get_claude_projects_dir()
        if not projects_dir.exists():
            errors.append(f"âŒ Local: Claude projects directory not found: {projects_dir}")
    except Exception as e:
        errors.append(f"âŒ Local: {e}")

    # 2. Check Windows users (when in WSL)
    if in_wsl:
        try:
            windows_users = get_windows_users_with_claude()
            if windows_users:
                for user in windows_users:
                    try:
                        projects_dir = get_windows_projects_dir(user["username"])
                        if not projects_dir or not projects_dir.exists():
                            errors.append(f"âŒ Windows ({user['username']}): Projects directory not accessible")
                    except Exception as e:
                        errors.append(f"âŒ Windows ({user['username']}): {e}")
        except Exception as e:
            errors.append(f"âŒ Windows: {e}")

    # 3. Check WSL distributions (when on Windows)
    if not in_wsl:
        try:
            wsl_distros = get_wsl_distributions()
            claude_distros = [d for d in wsl_distros if d["has_claude"]]
            if claude_distros:
                for distro in claude_distros:
                    try:
                        projects_dir = get_wsl_projects_dir(distro["name"])
                        if not projects_dir or not projects_dir.exists():
                            errors.append(f"âŒ WSL ({distro['name']}): Projects directory not accessible")
                    except Exception as e:
                        errors.append(f"âŒ WSL ({distro['name']}): {e}")
        except Exception as e:
            errors.append(f"âŒ WSL: {e}")

    # 4. Check SSH remotes (if specified)
    if hasattr(args, "remotes") and args.remotes:
        for remote_host in args.remotes:
            # Skip non-SSH remotes (windows)
            if is_windows_remote(remote_host):
                continue

            # Check SSH connectivity
            if not check_ssh_connection(remote_host):
                errors.append(f"âŒ Remote ({remote_host}): Cannot connect via passwordless SSH")

    return (len(errors) == 0, errors)


def cmd_export_all(args):
    """Export from all sources: local, WSL distributions (Windows), Windows (WSL), and optionally remotes."""
    # Detect environment
    in_wsl = is_running_in_wsl()
    local_label = "Local WSL" if in_wsl else "Local Windows"

    # Get patterns list (multiple patterns support)
    patterns = getattr(args, "patterns", None)
    if patterns is None:
        # Backward compat: use single workspace
        ws = getattr(args, "workspace", "")
        patterns = [ws] if ws else []

    # Include saved sources in remotes
    remotes = getattr(args, "remotes", []) or []
    saved_sources = get_saved_sources()
    for src in saved_sources:
        if src not in remotes:
            remotes.append(src)
    args.remotes = remotes

    # Pre-flight validation: Check all sources before starting export
    print("[Validation] Checking access to all sources...")
    valid, errors = validate_export_all_sources(args, in_wsl)

    if not valid:
        print("\nâŒ Export aborted: Cannot access all sources\n")
        for error in errors:
            print(error)
        print("\nPlease fix the issues above and try again.")
        sys.exit(1)

    print("âœ… All sources accessible\n")

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Track export statistics
    sources_processed = []
    total_sessions = 0
    total_errors = 0

    # Helper to get sessions for multiple patterns with deduplication
    def get_sessions_for_patterns(patterns_list, quiet=True, since_date=None, until_date=None):
        all_sessions = []
        seen_files = set()
        for pattern in patterns_list if patterns_list else [""]:
            sessions = get_workspace_sessions(pattern, quiet=quiet, since_date=since_date, until_date=until_date)
            for session in sessions:
                file_key = str(session["file"])
                if file_key not in seen_files:
                    seen_files.add(file_key)
                    all_sessions.append(session)
        return all_sessions

    # 1. Export from local (WSL or Windows depending on environment)
    print(f"[Local] Exporting from {local_label}...")
    try:
        output_dir_str = str(output_dir)

        class LocalExportArgs:
            output_dir = output_dir_str
            since = getattr(args, "since", None)
            until = getattr(args, "until", None)
            force = getattr(args, "force", False)
            minimal = getattr(args, "minimal", False)
            split = getattr(args, "split", None)
            flat = False  # Always organized for export-all
            remote = None  # Local
            lenient = True  # Don't fail on empty - export-all handles this

        LocalExportArgs.patterns = patterns
        LocalExportArgs.workspace = patterns[0] if patterns else ""  # Backward compat

        # Get local sessions count before export
        local_sessions = get_sessions_for_patterns(patterns, quiet=True)
        if local_sessions:
            cmd_batch(LocalExportArgs())
            sources_processed.append({"source": local_label, "sessions": len(local_sessions)})
            total_sessions += len(local_sessions)
            print(f"[OK] Local: {len(local_sessions)} sessions exported\n")
        else:
            print("[OK] Local: No matching sessions\n")
    except Exception as e:
        print(f"[ERROR] Local export failed: {e}\n")
        total_errors += 1

    # 2a. Export from Windows (when running in WSL)
    if in_wsl:
        print("[Windows] Checking Windows users with Claude...")
        try:
            windows_users = get_windows_users_with_claude()

            if windows_users:
                for user in windows_users:
                    print(f"  Exporting from Windows: {user['username']}...")
                    try:
                        output_dir_str = str(output_dir)
                        username = user["username"]

                        class WindowsExportArgs:
                            output_dir = output_dir_str
                            since = getattr(args, "since", None)
                            until = getattr(args, "until", None)
                            force = getattr(args, "force", False)
                            minimal = getattr(args, "minimal", False)
                            split = getattr(args, "split", None)
                            flat = False  # Always organized
                            remote = f"windows://{username}"
                            lenient = True  # Don't fail on empty

                        WindowsExportArgs.patterns = patterns
                        WindowsExportArgs.workspace = patterns[0] if patterns else ""

                        # Count sessions first
                        projects_dir = get_windows_projects_dir(username)
                        original_get_claude = get_claude_projects_dir

                        def temp_override():
                            return projects_dir

                        current_module = sys.modules[__name__]
                        current_module.get_claude_projects_dir = temp_override
                        try:
                            win_sessions = get_sessions_for_patterns(patterns, quiet=True)
                            # Filter out cached data
                            native_sessions = [
                                s
                                for s in win_sessions
                                if not s["workspace"].startswith("remote_") and not s["workspace"].startswith("wsl_")
                            ]
                            session_count = len(native_sessions)
                        finally:
                            current_module.get_claude_projects_dir = original_get_claude

                        if session_count > 0:
                            cmd_batch(WindowsExportArgs())
                            sources_processed.append({"source": f"Windows: {username}", "sessions": session_count})
                            total_sessions += session_count
                            print(f"  [OK] {username}: {session_count} sessions exported")
                        else:
                            print(f"  [OK] {username}: No matching sessions")
                    except Exception as e:
                        print(f"  [ERROR] {username} failed: {e}")
                        total_errors += 1
                print()
            else:
                print("  No Windows users with Claude Code found\n")
        except Exception as e:
            print(f"[ERROR] Windows export failed: {e}\n")

    # 2b. Export from all WSL distributions (when running on Windows)
    if not in_wsl:
        print("[WSL] Checking WSL distributions...")
        try:
            wsl_distros = get_wsl_distributions()
            claude_distros = [d for d in wsl_distros if d["has_claude"]]

            if claude_distros:
                for distro in claude_distros:
                    print(f"  Exporting from WSL: {distro['name']}...")
                    try:
                        output_dir_str = str(output_dir)
                        distro_name = distro["name"]

                        class WslExportArgs:
                            output_dir = output_dir_str
                            since = getattr(args, "since", None)
                            until = getattr(args, "until", None)
                            force = getattr(args, "force", False)
                            minimal = getattr(args, "minimal", False)
                            split = getattr(args, "split", None)
                            flat = False  # Always organized
                            remote = f"wsl://{distro_name}"
                            lenient = True  # Don't fail on empty

                        WslExportArgs.patterns = patterns
                        WslExportArgs.workspace = patterns[0] if patterns else ""

                        # Count sessions first
                        projects_dir = get_wsl_projects_dir(distro["name"])
                        original_get_claude = get_claude_projects_dir

                        def temp_override():
                            return projects_dir

                        current_module = sys.modules[__name__]
                        current_module.get_claude_projects_dir = temp_override
                        try:
                            wsl_sessions = get_sessions_for_patterns(patterns, quiet=True)
                            session_count = len(wsl_sessions)
                        finally:
                            current_module.get_claude_projects_dir = original_get_claude

                        if session_count > 0:
                            cmd_batch(WslExportArgs())
                            sources_processed.append({"source": f"WSL: {distro['name']}", "sessions": session_count})
                            total_sessions += session_count
                            print(f"  [OK] {distro['name']}: {session_count} sessions exported")
                        else:
                            print(f"  [OK] {distro['name']}: No matching sessions")
                    except Exception as e:
                        print(f"  [ERROR] {distro['name']} failed: {e}")
                        total_errors += 1
                print()
            else:
                print("  No WSL distributions with Claude Code found\n")
        except Exception as e:
            print(f"[ERROR] WSL export failed: {e}\n")

    # 3. Export from remote hosts (if specified)
    if hasattr(args, "remotes") and args.remotes:
        print("[Remote] Exporting from remote hosts...")
        for remote_host in args.remotes:
            print(f"  Exporting from {remote_host}...")
            try:
                output_dir_str = str(output_dir)

                class RemoteExportArgs:
                    output_dir = output_dir_str
                    since = getattr(args, "since", None)
                    until = getattr(args, "until", None)
                    force = getattr(args, "force", False)
                    minimal = getattr(args, "minimal", False)
                    split = getattr(args, "split", None)
                    flat = False
                    remote = remote_host
                    lenient = True  # Don't fail on empty

                RemoteExportArgs.patterns = patterns
                RemoteExportArgs.workspace = patterns[0] if patterns else ""

                # Try to export - cmd_batch will handle "no sessions" gracefully
                try:
                    cmd_batch(RemoteExportArgs())
                except SystemExit:
                    # cmd_batch might exit on "no sessions" - catch and report
                    pass

                # Count sessions (cached)
                hostname = get_remote_hostname(remote_host)
                cached_pattern = f"remote_{hostname}_"
                # Filter by workspace patterns if specified
                cached_sessions = get_workspace_sessions(cached_pattern, quiet=True)
                if patterns:
                    # Further filter by patterns
                    filtered = []
                    for s in cached_sessions:
                        for p in patterns:
                            if p in s["workspace"] or p in s.get("workspace_readable", ""):
                                filtered.append(s)
                                break
                    session_count = len(filtered)
                else:
                    session_count = len(cached_sessions)

                if session_count > 0:
                    sources_processed.append({"source": f"Remote: {remote_host}", "sessions": session_count})
                    total_sessions += session_count
                    print(f"  [OK] {remote_host}: {session_count} sessions exported")
                else:
                    print(f"  [OK] {remote_host}: No matching sessions")
            except Exception as e:
                print(f"  [ERROR] {remote_host} failed: {e}")
                total_errors += 1
        print()

    # Print summary
    print("=" * 80)
    print("Export-All Summary")
    print("=" * 80)
    for source_info in sources_processed:
        print(f"{source_info['source']:.<40} {source_info['sessions']:>5} sessions")
    print("-" * 80)
    print(f"Total sessions exported: {total_sessions}")
    if total_errors > 0:
        print(f"Errors encountered: {total_errors}")
    print(f"Output directory: {output_dir.absolute()}")
    print("=" * 80)

    # Generate index.md if requested
    if getattr(args, "index", True):  # Default to true
        generate_index_manifest(output_dir, sources_processed)


def generate_index_manifest(output_dir: Path, sources_info: list):
    """Generate index.md manifest file summarizing all exported sessions."""
    from datetime import datetime

    index_path = output_dir / "index.md"

    # Scan all workspace directories
    workspaces = {}
    for item in output_dir.iterdir():
        if item.is_dir():
            workspace_name = item.name
            sessions = list(item.glob("*.md"))

            # Group by source
            sources = {"local": 0, "wsl": 0, "windows": 0, "remote": 0}
            for session_file in sessions:
                if session_file.name.startswith("wsl_"):
                    sources["wsl"] += 1
                elif session_file.name.startswith("windows_"):
                    sources["windows"] += 1
                elif session_file.name.startswith("remote_"):
                    sources["remote"] += 1
                else:
                    sources["local"] += 1

            workspaces[workspace_name] = {"total": len(sessions), "sources": sources, "sessions": sessions}

    # Generate markdown
    lines = [
        "# Claude Conversation Export Index",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Total Workspaces:** {len(workspaces)}",
        f"**Total Sessions:** {sum(ws['total'] for ws in workspaces.values())}",
        "",
        "## Sources",
        "",
    ]

    # Add sources summary
    for source_info in sources_info:
        lines.append(f"- **{source_info['source']}**: {source_info['sessions']} sessions")

    lines.extend(["", "## Workspaces", ""])

    # Add workspace details
    for workspace_name in sorted(workspaces.keys()):
        ws_info = workspaces[workspace_name]
        lines.append(f"### {workspace_name} ({ws_info['total']} sessions)")
        lines.append("")
        if ws_info["sources"]["local"] > 0:
            lines.append(f"- **Local:** {ws_info['sources']['local']} sessions")
        if ws_info["sources"]["wsl"] > 0:
            lines.append(f"- **WSL:** {ws_info['sources']['wsl']} sessions")
        if ws_info["sources"]["windows"] > 0:
            lines.append(f"- **Windows:** {ws_info['sources']['windows']} sessions")
        if ws_info["sources"]["remote"] > 0:
            lines.append(f"- **Remote:** {ws_info['sources']['remote']} sessions")
        lines.append("")

    # Write index
    index_path.write_text("\n".join(lines), encoding="utf-8")
    print(f"\n[Index] Generated: {index_path}")


# ============================================================================
# Main
# ============================================================================


def main():
    # Fix Unicode encoding for Windows console (emojis in output)
    import sys

    if sys.platform == "win32":
        try:
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        except AttributeError:
            # Python < 3.7 doesn't have reconfigure
            import codecs

            sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
            sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")

    parser = argparse.ArgumentParser(
        prog="claude-history",
        description="Browse and export Claude Code conversation history",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
EXAMPLES:

  List workspaces:
    claude-history lsw                        # all local workspaces
    claude-history lsw myproject              # filter by pattern
    claude-history lsw -r user@server         # remote workspaces

  List sessions:
    claude-history lss                        # current workspace
    claude-history lss myproject              # specific workspace
    claude-history lss myproject -r user@server    # remote sessions

  Export (unified interface with orthogonal flags):
    claude-history export                     # current workspace, local source
    claude-history export --as                # current workspace, all sources
    claude-history export --aw                # all workspaces, local source
    claude-history export --as --aw           # all workspaces, all sources

    claude-history export myproject           # specific workspace, local
    claude-history export myproject --as      # specific workspace, all sources
    claude-history export file.jsonl         # export single file

    claude-history export -o /tmp/backup      # current workspace, custom output
    claude-history export myproject -o ./out  # specific workspace, custom output

    claude-history export -r user@server      # current workspace, specific remote
    claude-history export --as -r user@vm01   # current workspace, all sources + SSH

  Date filtering:
    claude-history lss myproject --since 2025-11-01
    claude-history export myproject --since 2025-11-01 --until 2025-11-30

  Export options:
    claude-history export myproject --minimal       # minimal mode
    claude-history export myproject --split 500     # split long conversations
    claude-history export myproject --flat          # flat structure (no subdirs)

  WSL access (Windows):
    claude-history lsh --wsl                        # list WSL distributions
    claude-history lsw --wsl                        # list WSL workspaces
    claude-history lsw --wsl Ubuntu                 # list from specific distro
    claude-history lss myproject --wsl              # list WSL sessions
    claude-history export myproject --wsl           # export from WSL

  Windows access (from WSL):
    claude-history lsh --windows                    # list Windows users with Claude
    claude-history lsw --windows                    # list Windows workspaces
    claude-history lss myproject --windows          # list Windows sessions
    claude-history export myproject --windows       # export from Windows
        """,
    )

    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    # Create subparsers for commands
    subparsers = parser.add_subparsers(dest="command", help="Command to execute", required=False)

    # ========== lsw - list workspaces ==========
    parser_lsw = subparsers.add_parser(
        "lsw", help="List workspaces", description="List all workspaces (local or remote)"
    )

    parser_lsw.add_argument(
        "pattern", nargs="*", help="Optional pattern(s) to filter workspaces (multiple patterns supported)"
    )

    parser_lsw.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List workspaces from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )

    parser_lsw.add_argument("--wsl", action="store_true", help="List workspaces from WSL (auto-detects distribution)")

    parser_lsw.add_argument("--windows", action="store_true", help="List workspaces from Windows (auto-detects user)")

    parser_lsw.add_argument(
        "--as",
        "--all-sources",
        dest="all_sources",
        action="store_true",
        help="List workspaces from all sources (local + WSL/Windows + remotes)",
    )

    # ========== lss - list sessions ==========
    parser_lss = subparsers.add_parser(
        "lss", help="List sessions", description="List sessions in a workspace (defaults to current workspace)"
    )

    parser_lss.add_argument(
        "workspace", nargs="*", help="Workspace name(s) (default: current workspace, multiple patterns supported)"
    )

    parser_lss.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="List sessions from remote host via SSH (user@hostname) - can be used multiple times with --as",
    )

    parser_lss.add_argument("--wsl", action="store_true", help="List sessions from WSL (auto-detects distribution)")

    parser_lss.add_argument("--windows", action="store_true", help="List sessions from Windows (auto-detects user)")

    parser_lss.add_argument(
        "--since", metavar="DATE", help="Only include sessions modified on or after this date (YYYY-MM-DD)"
    )

    parser_lss.add_argument(
        "--until", metavar="DATE", help="Only include sessions modified on or before this date (YYYY-MM-DD)"
    )

    parser_lss.add_argument(
        "--as",
        "--all-sources",
        dest="all_sources",
        action="store_true",
        help="List sessions from all sources (local + WSL/Windows + remotes)",
    )

    parser_lss.add_argument(
        "--alias", metavar="NAME", help="Use alias instead of workspace pattern (alternative to @name syntax)"
    )

    parser_lss.add_argument(
        "--this", action="store_true", dest="this_only", help="Use current workspace only, not its alias (if aliased)"
    )

    # ========== lsh - list hosts ==========
    parser_lsh = subparsers.add_parser(
        "lsh",
        help="List hosts and manage SSH remotes",
        description="List all Claude Code installations (local, WSL, Windows, SSH remotes)",
    )

    lsh_subparsers = parser_lsh.add_subparsers(dest="lsh_action")

    # lsh add
    parser_lsh_add = lsh_subparsers.add_parser(
        "add", help="Add an SSH remote", description="Add an SSH remote (user@hostname)"
    )
    parser_lsh_add.add_argument("source", help="SSH remote (user@hostname)")

    # lsh remove
    parser_lsh_remove = lsh_subparsers.add_parser(
        "remove", help="Remove an SSH remote", description="Remove an SSH remote from configuration"
    )
    parser_lsh_remove.add_argument("source", help="SSH remote to remove")

    # lsh clear
    lsh_subparsers.add_parser("clear", help="Clear all SSH remotes", description="Remove all saved SSH remotes")

    # Filter flags for listing (default action)
    parser_lsh.add_argument("--wsl", action="store_true", help="Show WSL distributions only")

    parser_lsh.add_argument("--windows", action="store_true", help="Show Windows users only")

    parser_lsh.add_argument("--local", action="store_true", help="Show local home folder only")

    parser_lsh.add_argument("--remotes", action="store_true", help="Show SSH remotes only")

    # ========== export command ==========
    parser_export = subparsers.add_parser(
        "export", help="Export to markdown", description="Export workspace or single file to markdown"
    )

    parser_export.add_argument(
        "target",
        nargs="*",
        default=[],
        help="Workspace pattern(s) or file.jsonl (optional, multiple patterns supported)",
    )

    parser_export.add_argument(
        "output_dir",
        nargs="?",
        default="./claude-conversations",
        help="Output directory (default: ./claude-conversations)",
    )

    parser_export.add_argument(
        "-o",
        "--output",
        metavar="DIR",
        dest="output_override",
        help="Output directory (overrides positional output_dir)",
    )

    parser_export.add_argument(
        "-a",
        "--all",
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Export all workspaces (default: current workspace)",
    )

    parser_export.add_argument(
        "--as",
        "--all-sources",
        action="store_true",
        dest="all_sources",
        help="Export from all sources: local + WSL + Windows + remotes (default: local only)",
    )

    parser_export.add_argument(
        "-r",
        "--remote",
        metavar="HOST",
        action="append",
        dest="remotes",
        help="Export from remote host via SSH (user@hostname) - can be used multiple times",
    )

    parser_export.add_argument("--wsl", action="store_true", help="Export from WSL (auto-detects distribution)")

    parser_export.add_argument("--windows", action="store_true", help="Export from Windows (auto-detects user)")

    parser_export.add_argument("--force", action="store_true", help="Force re-export (default: incremental)")

    parser_export.add_argument(
        "--minimal", action="store_true", help="Minimal export: omit metadata, keep only conversation content"
    )

    parser_export.add_argument(
        "--split",
        metavar="LINES",
        type=validate_split_lines,
        help="Split long conversations into parts (e.g., --split 500)",
    )

    parser_export.add_argument(
        "--since", metavar="DATE", help="Only include sessions modified on or after this date (YYYY-MM-DD)"
    )

    parser_export.add_argument(
        "--until", metavar="DATE", help="Only include sessions modified on or before this date (YYYY-MM-DD)"
    )

    parser_export.add_argument(
        "--flat",
        action="store_true",
        help="Use flat directory structure (default: organized by workspace with source tags)",
    )

    parser_export.add_argument("--alias", metavar="NAME", help="Export using alias (alternative to @name syntax)")

    parser_export.add_argument(
        "--this", action="store_true", dest="this_only", help="Use current workspace only, not its alias (if aliased)"
    )

    # ========== alias command ==========
    parser_alias = subparsers.add_parser(
        "alias",
        help="Manage workspace aliases",
        description="Create and manage workspace aliases for consolidated exports",
    )

    alias_subparsers = parser_alias.add_subparsers(dest="alias_command", help="Alias subcommand")

    # alias list
    alias_subparsers.add_parser(
        "list", help="List all aliases", description="List all aliases with their workspaces and session counts"
    )

    # alias show
    parser_alias_show = alias_subparsers.add_parser(
        "show", help="Show alias details", description="Show details of a single alias"
    )
    parser_alias_show.add_argument("name", help="Alias name")

    # alias create
    parser_alias_create = alias_subparsers.add_parser(
        "create", help="Create a new alias", description="Create a new empty alias"
    )
    parser_alias_create.add_argument("name", help="Alias name")

    # alias delete
    parser_alias_delete = alias_subparsers.add_parser(
        "delete", help="Delete an alias", description="Delete an alias and all its workspace associations"
    )
    parser_alias_delete.add_argument("name", help="Alias name")

    # alias add
    parser_alias_add = alias_subparsers.add_parser(
        "add", help="Add workspace(s) to alias", description="Add one or more workspaces to an alias"
    )
    parser_alias_add.add_argument(
        "name", nargs="?", default="", help="Alias name (default: auto-detect from current directory)"
    )
    parser_alias_add.add_argument("workspaces", nargs="*", help="Workspace names to add (encoded directory names)")
    parser_alias_add.add_argument("--pick", action="store_true", help="Interactive workspace picker")
    parser_alias_add.add_argument(
        "-r", "--remote", metavar="HOST", help="Add workspace from remote host (user@hostname)"
    )
    parser_alias_add.add_argument(
        "--wsl", action="store_true", help="Add workspace from WSL (auto-detects distribution)"
    )
    parser_alias_add.add_argument(
        "--windows", action="store_true", help="Add workspace from Windows (auto-detects user)"
    )
    parser_alias_add.add_argument(
        "--as",
        "--all-sources",
        action="store_true",
        dest="all_sources",
        help="Add from all sources (local + WSL/Windows + remotes)",
    )

    # alias remove
    parser_alias_remove = alias_subparsers.add_parser(
        "remove", help="Remove workspace from alias", description="Remove a workspace from an alias"
    )
    parser_alias_remove.add_argument("name", help="Alias name")
    parser_alias_remove.add_argument("workspace", help="Workspace name to remove (encoded directory name)")
    parser_alias_remove.add_argument("-r", "--remote", metavar="HOST", help="Remove workspace from remote source")
    parser_alias_remove.add_argument("--wsl", action="store_true", help="Remove workspace from WSL source")
    parser_alias_remove.add_argument("--windows", action="store_true", help="Remove workspace from Windows source")

    # alias export
    alias_subparsers.add_parser(
        "export", help="Export aliases to JSON", description="Export aliases configuration to JSON (stdout)"
    )

    # alias import
    parser_alias_import = alias_subparsers.add_parser(
        "import", help="Import aliases from JSON", description="Import aliases from JSON file or stdin"
    )
    parser_alias_import.add_argument("file", nargs="?", help="JSON file to import (default: stdin)")
    parser_alias_import.add_argument("--replace", action="store_true", help="Replace all aliases instead of merging")

    # ========== stats command ==========
    parser_stats = subparsers.add_parser(
        "stats",
        help="Show usage statistics and metrics",
        description="Display Claude Code usage statistics from synced metrics database",
    )

    parser_stats.add_argument("workspace", nargs="*", default=None, help="Filter by workspace pattern(s) or @alias")

    parser_stats.add_argument("--sync", action="store_true", help="Sync JSONL files to metrics database")

    parser_stats.add_argument("--force", action="store_true", help="Force re-sync all files (ignore mtime)")

    parser_stats.add_argument("--tools", action="store_true", help="Show tool usage statistics")

    parser_stats.add_argument("--models", action="store_true", help="Show model usage statistics")

    parser_stats.add_argument("--time", action="store_true", help="Show time tracking with daily breakdown")

    parser_stats.add_argument("--by-workspace", action="store_true", help="Show statistics grouped by workspace")

    parser_stats.add_argument("--by-day", action="store_true", help="Show daily statistics")

    parser_stats.add_argument(
        "--source", metavar="SOURCE", help="Filter by source (local, wsl:distro, windows, remote:host)"
    )

    parser_stats.add_argument("--since", metavar="DATE", help="Filter sessions starting from this date (YYYY-MM-DD)")

    parser_stats.add_argument("--until", metavar="DATE", help="Filter sessions until this date (YYYY-MM-DD)")

    parser_stats.add_argument(
        "--aw",
        "--all-workspaces",
        action="store_true",
        dest="all_workspaces",
        help="Show stats for all workspaces (default: current workspace)",
    )

    parser_stats.add_argument(
        "--this", action="store_true", dest="this_only", help="Use current workspace only, not its alias (if aliased)"
    )

    parser_stats.add_argument(
        "--as",
        "--all-sources",
        action="store_true",
        dest="all_sources",
        help="Sync from all sources (local + WSL/Windows)",
    )

    parser_stats.add_argument("--wsl", action="store_true", help="Include WSL source")

    parser_stats.add_argument("--windows", action="store_true", help="Include Windows source")

    parser_stats.add_argument(
        "-r",
        "--remote",
        action="append",
        dest="remotes",
        metavar="HOST",
        help="Include remote source (can be repeated)",
    )

    # ========== reset command ==========
    parser_reset = subparsers.add_parser(
        "reset",
        help="Reset stored data (database, settings, aliases)",
        description="Delete metrics database, settings, and/or aliases.",
    )

    parser_reset.add_argument(
        "what",
        nargs="?",
        choices=["db", "settings", "aliases", "all"],
        default="all",
        help="What to reset: db (metrics), settings (SSH remotes), aliases, or all (default: all)",
    )

    parser_reset.add_argument("-y", "--yes", action="store_true", help="Skip confirmation prompt")

    # Parse arguments
    args = parser.parse_args()

    # Helper function to resolve remote flags
    def resolve_remote_flag(args):
        """Convert --wsl and --windows flags to remote format."""
        remotes = getattr(args, "remotes", None)
        remote = remotes[0] if remotes else None  # Use first remote for single-remote commands
        wsl_flag = getattr(args, "wsl", False)
        windows_flag = getattr(args, "windows", False)

        # Convert boolean flags to remote format (auto-detect)
        if wsl_flag:
            wsl_distros = get_wsl_distributions()
            if wsl_distros:
                claude_distros = [d for d in wsl_distros if d["has_claude"]]
                if claude_distros:
                    return f"wsl://{claude_distros[0]['name']}"
            sys.stderr.write("âŒ Error: No WSL distributions with Claude Code found\n")
            sys.exit(1)

        if windows_flag:
            return "windows"

        return remote

    def resolve_remote_list(args):
        """Convert --wsl and --windows flags to remote list for export command."""
        remotes = getattr(args, "remotes", None) or []
        wsl_flag = getattr(args, "wsl", False)
        windows_flag = getattr(args, "windows", False)

        # Add --wsl to remotes list (auto-detect first available distro)
        if wsl_flag:
            wsl_distros = get_wsl_distributions()
            if wsl_distros:
                claude_distros = [d for d in wsl_distros if d["has_claude"]]
                if claude_distros:
                    remotes.append(f"wsl://{claude_distros[0]['name']}")

        # Add --windows to remotes list (auto-detect)
        if windows_flag:
            remotes.append("windows")

        return remotes if remotes else None

    # Dispatch to command handlers
    if args.command == "lsw":
        # List workspaces - handle multiple patterns
        patterns = args.pattern if args.pattern else [""]

        # Check for --all-sources flag
        if getattr(args, "all_sources", False):
            # Get remotes list
            lsw_remotes = getattr(args, "remotes", None) or []

            class LswAllArgs:
                workspaces_only = True
                since_date = None
                until_date = None

            LswAllArgs.remotes = lsw_remotes
            LswAllArgs.patterns = patterns

            cmd_list_all_sources(LswAllArgs())
        else:

            class LswArgs:
                remote = resolve_remote_flag(args)
                workspaces_only = True  # lsw always lists workspaces

            LswArgs.patterns = patterns
            # Add workspace for backward compatibility with remote handlers
            LswArgs.workspace = patterns[0] if patterns and patterns[0] else ""
            cmd_list(LswArgs())

    elif args.command == "lss":
        # Handle multiple patterns - args.workspace is now a list
        workspace_list = args.workspace if args.workspace else []
        alias_flag = getattr(args, "alias", None)

        # Detect @ prefix in first workspace (alias syntax)
        alias_name = None
        if alias_flag:
            alias_name = alias_flag
        elif workspace_list and workspace_list[0].startswith("@"):
            alias_name = workspace_list[0][1:]

        if alias_name:
            # Use alias-based listing
            since_date = parse_date_string(args.since) if args.since else None
            until_date = parse_date_string(args.until) if args.until else None
            cmd_alias_lss(alias_name, since_date=since_date, until_date=until_date)
        elif getattr(args, "all_sources", False):
            # List from all sources
            this_only = getattr(args, "this_only", False)
            since_date = parse_date_string(args.since) if args.since else None
            until_date = parse_date_string(args.until) if args.until else None
            lss_remotes = getattr(args, "remotes", None) or []

            # Determine patterns - check for alias if no workspace specified
            if workspace_list:
                patterns = workspace_list
            else:
                # Check if current directory is a valid workspace
                pattern, exists = check_current_workspace_exists()
                if not exists:
                    # Not in a workspace - list all workspaces from all sources
                    patterns = [""]
                elif not this_only:
                    alias_for_ws = get_alias_for_workspace(pattern, "local")
                    if alias_for_ws:
                        # Use alias-based listing instead
                        sys.stderr.write(f"ðŸ“Ž Using alias @{alias_for_ws} (use --this for current workspace only)\n")
                        cmd_alias_lss(alias_for_ws, since_date=since_date, until_date=until_date)
                        return  # Early return after alias listing
                    patterns = [pattern]
                else:
                    patterns = [pattern]

            class LssAllArgs:
                workspaces_only = False

            LssAllArgs.patterns = patterns
            LssAllArgs.since_date = since_date
            LssAllArgs.until_date = until_date
            LssAllArgs.remotes = lss_remotes

            cmd_list_all_sources(LssAllArgs())
        else:
            # Regular workspace listing
            this_only = getattr(args, "this_only", False)
            if workspace_list:
                patterns = workspace_list
            else:
                # No workspace specified - check if current directory is a known workspace
                pattern, exists = check_current_workspace_exists()
                if not exists:
                    print("Error: Not in a Claude Code workspace.", file=sys.stderr)
                    print("\nTo list sessions, either:", file=sys.stderr)
                    print("  â€¢ Run from within a workspace directory", file=sys.stderr)
                    print("  â€¢ Specify a workspace pattern: lss <pattern>", file=sys.stderr)
                    print("  â€¢ Use --as to list from all sources: lss --as", file=sys.stderr)
                    sys.exit(1)

                # Check if current workspace belongs to an alias (unless --this is used)
                if not this_only:
                    alias_for_ws = get_alias_for_workspace(pattern, "local")
                    if alias_for_ws:
                        # Use alias-based listing instead
                        sys.stderr.write(f"ðŸ“Ž Using alias @{alias_for_ws} (use --this for current workspace only)\n")
                        since_date = parse_date_string(args.since) if args.since else None
                        until_date = parse_date_string(args.until) if args.until else None
                        cmd_alias_lss(alias_for_ws, since_date=since_date, until_date=until_date)
                        return  # Early return after alias listing

                patterns = [pattern]

            class LssArgs:
                since = args.since
                until = args.until
                remote = resolve_remote_flag(args)
                workspaces_only = False  # lss always lists sessions

            LssArgs.patterns = patterns
            cmd_list(LssArgs())

    elif args.command == "lsh":
        # List hosts
        cmd_lsh(args)

    elif args.command == "export":
        # Determine output directory (-o overrides positional)
        final_output_dir = (
            args.output_override if hasattr(args, "output_override") and args.output_override else args.output_dir
        )

        # Handle target(s) - now a list
        targets = args.target if args.target else []

        # Check for alias (@ prefix or --alias flag)
        alias_flag = getattr(args, "alias", None)
        alias_name = None
        if alias_flag:
            alias_name = alias_flag
        elif targets and targets[0].startswith("@"):
            alias_name = targets[0][1:]

        if alias_name:
            # Use alias-based export
            cmd_alias_export(alias_name, final_output_dir, args)
            return

        # Get patterns (default to current workspace if empty)
        # Skip workspace check if --aw is set (all workspaces)
        # Also skip if --as is set and not in a workspace (implies --aw)
        this_only = getattr(args, "this_only", False)
        if not targets and not args.all_workspaces:
            # No workspace specified - check if current directory is a known workspace
            pattern, exists = check_current_workspace_exists()
            if not exists:
                # If --as is used from outside a workspace, treat as --aw
                if args.all_sources:
                    args.all_workspaces = True
                else:
                    print("Error: Not in a Claude Code workspace.", file=sys.stderr)
                    print("\nTo export sessions, either:", file=sys.stderr)
                    print("  â€¢ Run from within a workspace directory", file=sys.stderr)
                    print("  â€¢ Specify a workspace pattern: export <pattern>", file=sys.stderr)
                    print("  â€¢ Use --aw to export all workspaces: export --aw", file=sys.stderr)
                    print("  â€¢ Use --as to export from all sources: export --as", file=sys.stderr)
                    sys.exit(1)
            else:
                # Check if current workspace belongs to an alias (unless --this is used)
                if not this_only:
                    alias_for_ws = get_alias_for_workspace(pattern, "local")
                    if alias_for_ws:
                        # Use alias-based export instead
                        sys.stderr.write(f"ðŸ“Ž Using alias @{alias_for_ws} (use --this for current workspace only)\n")
                        cmd_alias_export(alias_for_ws, final_output_dir, args)
                        return  # Early return after alias export

                targets = [pattern]

        # Check for single file export (one target ending with .jsonl)
        if len(targets) == 1 and targets[0].endswith(".jsonl"):
            final_remotes = resolve_remote_list(args)
            final_remote = final_remotes[0] if final_remotes else None

            class ConvertArgs:
                jsonl_file = targets[0]
                output = None  # Use default
                remote = final_remote

            cmd_convert(ConvertArgs())
            return

        # Resolve remotes (including --wsl and --windows flags)
        final_remotes = resolve_remote_list(args)

        # Handle --all-sources flag (export from all sources)
        if args.all_sources:
            # Export from all sources (local + WSL + Windows + remotes)
            # Pass patterns list for multiple pattern support
            final_patterns = [] if args.all_workspaces else targets

            class ExportAllArgs:
                patterns = final_patterns
                workspace = ""  # Backward compat: empty means all or use patterns
                output_dir = final_output_dir
                remotes = final_remotes
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split

            cmd_export_all(ExportAllArgs())
        else:
            # Regular export - single source
            final_patterns = [] if args.all_workspaces else targets
            final_remote = final_remotes[0] if final_remotes else None

            class ExportArgs:
                patterns = final_patterns
                workspace = final_patterns[0] if final_patterns else ""  # Backward compat
                output_dir = final_output_dir
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split
                flat = args.flat
                remote = final_remote

            cmd_batch(ExportArgs())

    elif args.command == "alias":
        # Alias management commands
        alias_cmd = getattr(args, "alias_command", None)

        if alias_cmd == "list" or alias_cmd is None:
            cmd_alias_list(args)
        elif alias_cmd == "show":
            cmd_alias_show(args)
        elif alias_cmd == "create":
            cmd_alias_create(args)
        elif alias_cmd == "delete":
            cmd_alias_delete(args)
        elif alias_cmd == "add":
            cmd_alias_add(args)
        elif alias_cmd == "remove":
            cmd_alias_remove(args)
        elif alias_cmd == "export":
            cmd_alias_config_export(args)
        elif alias_cmd == "import":
            cmd_alias_config_import(args)
        else:
            # Show alias help if unknown subcommand
            parser_alias.print_help()

    elif args.command == "stats":
        # Stats/metrics commands
        if args.sync:
            # Build remotes list from flags
            remotes = getattr(args, "remotes", None) or []
            if args.wsl:
                wsl_distros = get_wsl_distributions()
                claude_distros = [d for d in wsl_distros if d.get("has_claude")]
                if claude_distros:
                    remotes.append(f"wsl://{claude_distros[0]['name']}")
            if args.windows:
                remotes.append("windows")

            class SyncArgs:
                force = args.force
                all_sources = args.all_sources
                patterns = [args.workspace] if args.workspace else [""]

            SyncArgs.remotes = remotes
            cmd_stats_sync(SyncArgs())
        else:
            # Display stats
            # If --as is given, sync first then display
            if args.all_sources:
                remotes = getattr(args, "remotes", None) or []
                if args.wsl:
                    wsl_distros = get_wsl_distributions()
                    claude_distros = [d for d in wsl_distros if d.get("has_claude")]
                    if claude_distros:
                        remotes.append(f"wsl://{claude_distros[0]['name']}")
                if args.windows:
                    remotes.append("windows")

                class SyncArgs:
                    force = args.force
                    all_sources = True
                    patterns = [args.workspace] if args.workspace else [""]

                SyncArgs.remotes = remotes
                cmd_stats_sync(SyncArgs())
                print()  # Blank line between sync and display

            # --as and --aw are orthogonal:
            # --as = sync from all sources first
            # --aw = query all workspaces
            class StatsArgs:
                workspace = args.workspace
                source = args.source
                since = args.since
                until = args.until
                tools = args.tools
                models = args.models
                time = getattr(args, "time", False)
                by_workspace = getattr(args, "by_workspace", False)
                by_day = getattr(args, "by_day", False)
                all_workspaces = getattr(args, "all_workspaces", False)

            cmd_stats(StatsArgs())

    elif args.command == "reset":
        cmd_reset(args)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.stderr.write("\nInterrupted\n")
        sys.exit(130)
    except Exception as e:
        sys.stderr.write(f"\nError: {e}\n")
        sys.exit(1)
