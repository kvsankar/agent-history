#!/usr/bin/env python3
"""
claude-sessions - Manage and export Claude Code conversation sessions by workspace

A tool to list, filter, and export Claude Code conversation sessions from specific
workspaces, bypassing the brittle session-ID-based approach of other tools.

Author: Built with Claude Code
Version: 1.0.0
License: MIT
"""

import argparse
import base64
import json
import sys
from pathlib import Path
from datetime import datetime

__version__ = "1.2.0"


# ============================================================================
# Date Parsing
# ============================================================================

def parse_date_string(date_str):
    """
    Parse ISO date string (YYYY-MM-DD) into datetime object.
    Returns datetime object or None if parsing fails.
    """
    if not date_str:
        return None

    try:
        return datetime.strptime(date_str.strip(), '%Y-%m-%d')
    except ValueError:
        return None


# ============================================================================
# JSONL Parsing and Markdown Conversion
# ============================================================================

def decode_content(encoded_str):
    """Decode base64 content."""
    try:
        return base64.b64decode(encoded_str).decode('utf-8')
    except Exception as e:
        return f"[Error decoding content: {e}]"


def extract_content(message_obj):
    """Extract text content from message object, preserving all information."""
    content_parts = []

    if 'content' in message_obj:
        content = message_obj['content']

        # User messages have simple string content
        if isinstance(content, str):
            return content

        # Assistant messages have array of content blocks
        if isinstance(content, list):
            for block in content:
                if block.get('type') == 'text':
                    content_parts.append(block.get('text', ''))

                elif block.get('type') == 'tool_use':
                    tool_name = block.get('name', 'unknown')
                    tool_id = block.get('id', '')
                    tool_input = block.get('input', {})

                    content_parts.append(f"\n**[Tool Use: {tool_name}]**")
                    if tool_id:
                        content_parts.append(f"Tool ID: `{tool_id}`")
                    content_parts.append("\nInput:")
                    content_parts.append("```json")
                    content_parts.append(json.dumps(tool_input, indent=2))
                    content_parts.append("```\n")

                elif block.get('type') == 'tool_result':
                    tool_use_id = block.get('tool_use_id', '')
                    is_error = block.get('is_error', False)
                    result_content = block.get('content', '')

                    # Handle both string and list content in tool results
                    if isinstance(result_content, list):
                        result_text = '\n'.join(
                            item.get('text', '') if isinstance(item, dict) else str(item)
                            for item in result_content
                        )
                    else:
                        result_text = result_content

                    status = "ERROR" if is_error else "Success"
                    content_parts.append(f"\n**[Tool Result: {status}]**")
                    if tool_use_id:
                        content_parts.append(f"Tool Use ID: `{tool_use_id}`")
                    content_parts.append("\n```")
                    content_parts.append(result_text)
                    content_parts.append("```\n")

    return '\n'.join(content_parts) if content_parts else '[No content]'


def get_first_timestamp(jsonl_file: Path) -> str:
    """Extract the first message timestamp from a .jsonl file."""
    try:
        with open(jsonl_file, encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    entry_type = entry.get('type')
                    if entry_type in ('user', 'assistant'):
                        timestamp = entry.get('timestamp', '')
                        if timestamp:
                            return timestamp
                except json.JSONDecodeError:
                    continue
    except Exception:
        pass
    return None


def estimate_message_lines(msg_content: str, has_metadata: bool) -> int:
    """Estimate number of lines a message will take in markdown."""
    lines = 0
    lines += 1  # Message header (## Message N)
    lines += 1  # Empty line
    lines += 1  # Timestamp
    lines += 1  # Empty line
    lines += len(msg_content.split('\n'))  # Content
    lines += 1  # Empty line
    if has_metadata:
        lines += 20  # Approximate metadata section
        lines += 1  # Empty line
    lines += 1  # Separator (---)
    lines += 1  # Empty line
    return lines


def is_tool_result_message(msg_content: str) -> bool:
    """Check if message content contains a tool result."""
    return '**[Tool Result:' in msg_content


def calculate_time_gap(msg1, msg2) -> float:
    """Calculate time gap in seconds between two messages."""
    try:
        from datetime import datetime
        ts1 = datetime.fromisoformat(msg1['timestamp'].replace('Z', '+00:00'))
        ts2 = datetime.fromisoformat(msg2['timestamp'].replace('Z', '+00:00'))
        return abs((ts2 - ts1).total_seconds())
    except:
        return 0


def find_best_split_point(messages, target_lines: int, minimal: bool) -> int:
    """Find the best message index to split at, near target_lines.

    Returns message index to start part 2 (exclusive end of part 1).
    Returns None if no split needed.
    """
    min_lines = int(target_lines * 0.8)  # 80% of target
    max_lines = int(target_lines * 1.3)  # 130% of target (more flexible)

    current_lines = 30  # Approximate header lines
    best_split = None
    best_score = -1

    for i in range(len(messages)):
        msg = messages[i]
        msg_lines = estimate_message_lines(msg['content'], not minimal)
        current_lines += msg_lines

        # If we're past max_lines, must break before this message
        if current_lines > max_lines:
            return i if i > 0 else 1

        # If we're in the sweet spot, evaluate this as a break point
        if current_lines >= min_lines:
            score = 0

            # Priority 1: Next message is User (best - starting new topic)
            if i + 1 < len(messages) and messages[i + 1]['role'] == 'user':
                score += 100

            # Priority 2: Current message is tool result (action complete)
            if is_tool_result_message(msg['content']):
                score += 50

            # Priority 3: Time gap > 5 minutes before next message
            if i + 1 < len(messages):
                time_gap = calculate_time_gap(msg, messages[i + 1])
                if time_gap > 300:  # 5 minutes
                    score += 30
                elif time_gap > 60:  # 1 minute
                    score += 10

            # Slight preference for being closer to target
            distance_from_target = abs(current_lines - target_lines)
            score -= distance_from_target * 0.05

            if score > best_score:
                best_score = score
                best_split = i + 1  # Split after this message

    return best_split


def read_jsonl_messages(jsonl_file: Path):
    """Read and parse messages from a JSONL file.

    Returns:
        List of message dictionaries with role, content, timestamp, and metadata.
    """
    messages = []

    with open(jsonl_file, encoding='utf-8') as f:
        for line in f:
            try:
                entry = json.loads(line)

                # Look for user or assistant message types
                entry_type = entry.get('type')
                if entry_type in ('user', 'assistant'):
                    message_obj = entry.get('message', {})
                    role = message_obj.get('role', entry_type)
                    timestamp = entry.get('timestamp', '')
                    content = extract_content(message_obj)

                    # Preserve all metadata from the entry
                    messages.append({
                        'role': role,
                        'content': content,
                        'timestamp': timestamp,
                        'uuid': entry.get('uuid', ''),
                        'parentUuid': entry.get('parentUuid'),
                        'sessionId': entry.get('sessionId', ''),
                        'agentId': entry.get('agentId'),
                        'requestId': entry.get('requestId'),
                        'cwd': entry.get('cwd', ''),
                        'version': entry.get('version', ''),
                        'gitBranch': entry.get('gitBranch'),
                        'isSidechain': entry.get('isSidechain'),
                        'userType': entry.get('userType'),
                        'model': message_obj.get('model'),
                        'usage': message_obj.get('usage'),
                        'stop_reason': message_obj.get('stop_reason'),
                        'stop_sequence': message_obj.get('stop_sequence'),
                    })

            except json.JSONDecodeError as e:
                print(f"Warning: Couldn't parse line: {e}", file=sys.stderr)
                continue

    return messages


def parse_jsonl_to_markdown(jsonl_file: Path, minimal: bool = False) -> str:
    """Parse a .jsonl file and convert to markdown.

    Args:
        jsonl_file: Path to the JSONL file to convert
        minimal: If True, omit metadata and only include conversation content
    """
    messages = read_jsonl_messages(jsonl_file)

    # Check if this is an agent conversation
    is_agent = any(msg.get('isSidechain') for msg in messages)
    parent_session_id = None
    agent_id = None
    if is_agent and messages:
        parent_session_id = messages[0].get('sessionId')
        agent_id = messages[0].get('agentId')

    # Generate markdown
    md_lines = []

    # Add title with agent indicator
    if is_agent:
        md_lines.append(f"# Claude Conversation (Agent)")
    else:
        md_lines.append(f"# Claude Conversation")

    md_lines.append(f"")
    md_lines.append(f"**File:** {jsonl_file.name}")
    md_lines.append(f"**Messages:** {len(messages)}")

    if messages:
        first_ts = messages[0]['timestamp']
        last_ts = messages[-1]['timestamp']
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append(f"")
        md_lines.append("> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.")
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append(f"")
    md_lines.append("---")
    md_lines.append(f"")

    # Build UUID to message index map for internal linking
    uuid_to_index = {}
    for i, msg in enumerate(messages, 1):
        if msg.get('uuid'):
            uuid_to_index[msg['uuid']] = i

    for i, msg in enumerate(messages, 1):
        # Determine message label based on context
        if is_agent and i == 1 and msg['role'] == 'user':
            # First message in agent conversation is the task prompt from parent Claude
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg['role'] == 'user' else "ðŸ¤–"
            role_label = msg['role'].title()

        # Add HTML anchor for this message's UUID (skip in minimal mode)
        if not minimal and msg.get('uuid'):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append(f"")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append(f"")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append(f"")
        md_lines.append(msg['content'])
        md_lines.append(f"")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            # Core identifiers with navigation links
            if msg.get('uuid'):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get('parentUuid'):
                parent_uuid = msg['parentUuid']
                # Check if parent is in the same file
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*")
                else:
                    # Parent is in a different file (e.g., main session when this is an agent)
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get('sessionId'):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get('agentId'):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get('requestId'):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            # Environment info
            if msg.get('cwd'):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get('gitBranch'):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get('version'):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            # User/session info
            if msg.get('userType'):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get('isSidechain') is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            # Model info (for assistant messages)
            if msg.get('model'):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get('stop_reason'):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get('stop_sequence'):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            # Usage stats (for assistant messages)
            if msg.get('usage'):
                usage = msg['usage']
                md_lines.append(f"- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get('cache_creation_input_tokens'):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get('cache_read_input_tokens'):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append(f"")
        md_lines.append("---")
        md_lines.append(f"")

    return "\n".join(md_lines)


def generate_markdown_parts(messages, jsonl_file: Path, minimal: bool, split_lines: int):
    """Generate multiple markdown parts from messages, split at smart break points.

    Returns list of (part_num, total_parts, markdown_content, start_msg, end_msg) tuples.
    """
    if not split_lines or len(messages) == 0:
        return None

    # Find all split points
    split_points = [0]  # Start with message 0
    remaining_messages = messages

    while True:
        # Find next split point in remaining messages
        split_idx = find_best_split_point(remaining_messages, split_lines, minimal)

        if split_idx is None or split_idx >= len(remaining_messages):
            # No more splits needed
            break

        # Add this split point (adjusted for global message index)
        global_idx = split_points[-1] + split_idx
        split_points.append(global_idx)
        remaining_messages = messages[global_idx:]

    # Add end point
    split_points.append(len(messages))

    # If only one part, no splitting needed
    if len(split_points) <= 2:
        return None

    total_parts = len(split_points) - 1
    parts = []

    for part_num in range(total_parts):
        start_idx = split_points[part_num]
        end_idx = split_points[part_num + 1]
        part_messages = messages[start_idx:end_idx]

        # Generate markdown for this part
        part_md = generate_markdown_for_messages(
            part_messages, jsonl_file, minimal,
            part_num=part_num + 1, total_parts=total_parts,
            start_msg_num=start_idx + 1, end_msg_num=end_idx
        )

        parts.append((part_num + 1, total_parts, part_md, start_idx + 1, end_idx))

    return parts


def generate_markdown_for_messages(part_messages, jsonl_file, minimal, part_num=None, total_parts=None, start_msg_num=1, end_msg_num=None):
    """Generate markdown for a subset of messages (used for splitting)."""
    # Similar logic to parse_jsonl_to_markdown but for a message subset
    # This is a simplified version - we'll call the full function and then extract messages

    # Check if this is an agent conversation
    is_agent = any(msg.get('isSidechain') for msg in part_messages)
    parent_session_id = None
    agent_id = None
    if is_agent and part_messages:
        parent_session_id = part_messages[0].get('sessionId')
        agent_id = part_messages[0].get('agentId')

    md_lines = []

    # Add title with part indicator
    if part_num and total_parts:
        if is_agent:
            md_lines.append(f"# Claude Conversation (Agent) - Part {part_num} of {total_parts}")
        else:
            md_lines.append(f"# Claude Conversation - Part {part_num} of {total_parts}")
    else:
        if is_agent:
            md_lines.append(f"# Claude Conversation (Agent)")
        else:
            md_lines.append(f"# Claude Conversation")

    md_lines.append(f"")
    md_lines.append(f"**File:** {jsonl_file.name}")

    if part_num and total_parts:
        md_lines.append(f"**Part:** {part_num} of {total_parts}")
        md_lines.append(f"**Messages in this part:** {len(part_messages)} (#{start_msg_num}-#{end_msg_num})")
    else:
        md_lines.append(f"**Messages:** {len(part_messages)}")

    if part_messages:
        first_ts = part_messages[0]['timestamp']
        last_ts = part_messages[-1]['timestamp']
        md_lines.append(f"**First message:** {first_ts}")
        md_lines.append(f"**Last message:** {last_ts}")

    # Add agent conversation notice
    if is_agent:
        md_lines.append(f"")
        md_lines.append("> âš ï¸ **Agent Conversation:** This is a sub-task executed by an agent spawned from the main conversation.")
        md_lines.append(">")
        md_lines.append("> - Messages labeled 'User' represent task instructions from the parent Claude session")
        md_lines.append("> - Messages labeled 'Assistant' are responses from this agent")
        if parent_session_id:
            md_lines.append(f"> - **Parent Session ID:** `{parent_session_id}`")
        if agent_id:
            md_lines.append(f"> - **Agent ID:** `{agent_id}`")

    md_lines.append(f"")
    md_lines.append("---")
    md_lines.append(f"")

    # Build UUID to message index map
    uuid_to_index = {}
    for i, msg in enumerate(part_messages, start_msg_num):
        if msg.get('uuid'):
            uuid_to_index[msg['uuid']] = i

    # Generate messages
    for i, msg in enumerate(part_messages, start_msg_num):
        # Determine message label
        if is_agent and i == start_msg_num and msg['role'] == 'user':
            role_emoji = "ðŸ”§"
            role_label = "Task Prompt (from Parent Claude)"
        else:
            role_emoji = "ðŸ‘¤" if msg['role'] == 'user' else "ðŸ¤–"
            role_label = msg['role'].title()

        # Add HTML anchor (skip in minimal mode)
        if not minimal and msg.get('uuid'):
            md_lines.append(f'<a name="msg-{msg["uuid"]}"></a>')
            md_lines.append(f"")

        md_lines.append(f"## Message {i} - {role_emoji} {role_label}")
        md_lines.append(f"")
        md_lines.append(f"*{msg['timestamp']}*")
        md_lines.append(f"")
        md_lines.append(msg['content'])
        md_lines.append(f"")

        # Add metadata section (skip in minimal mode)
        if not minimal:
            md_lines.append("### Metadata")
            md_lines.append("")

            if msg.get('uuid'):
                md_lines.append(f"- **UUID:** `{msg['uuid']}`")

            if msg.get('parentUuid'):
                parent_uuid = msg['parentUuid']
                if parent_uuid in uuid_to_index:
                    parent_msg_num = uuid_to_index[parent_uuid]
                    md_lines.append(f"- **Parent UUID:** [`{parent_uuid}`](#msg-{parent_uuid}) *(â†’ Message {parent_msg_num})*")
                else:
                    md_lines.append(f"- **Parent UUID:** `{parent_uuid}` *(in different session)*")
            if msg.get('sessionId'):
                md_lines.append(f"- **Session ID:** `{msg['sessionId']}`")
            if msg.get('agentId'):
                md_lines.append(f"- **Agent ID:** `{msg['agentId']}`")
            if msg.get('requestId'):
                md_lines.append(f"- **Request ID:** `{msg['requestId']}`")

            if msg.get('cwd'):
                md_lines.append(f"- **Working Directory:** `{msg['cwd']}`")
            if msg.get('gitBranch'):
                md_lines.append(f"- **Git Branch:** `{msg['gitBranch']}`")
            if msg.get('version'):
                md_lines.append(f"- **Version:** `{msg['version']}`")

            if msg.get('userType'):
                md_lines.append(f"- **User Type:** `{msg['userType']}`")
            if msg.get('isSidechain') is not None:
                md_lines.append(f"- **Is Sidechain:** `{msg['isSidechain']}`")

            if msg.get('model'):
                md_lines.append(f"- **Model:** `{msg['model']}`")
            if msg.get('stop_reason'):
                md_lines.append(f"- **Stop Reason:** `{msg['stop_reason']}`")
            if msg.get('stop_sequence'):
                md_lines.append(f"- **Stop Sequence:** `{msg['stop_sequence']}`")

            if msg.get('usage'):
                usage = msg['usage']
                md_lines.append(f"- **Usage:**")
                md_lines.append(f"  - Input tokens: {usage.get('input_tokens', 0)}")
                md_lines.append(f"  - Output tokens: {usage.get('output_tokens', 0)}")
                if usage.get('cache_creation_input_tokens'):
                    md_lines.append(f"  - Cache creation tokens: {usage['cache_creation_input_tokens']}")
                if usage.get('cache_read_input_tokens'):
                    md_lines.append(f"  - Cache read tokens: {usage['cache_read_input_tokens']}")

            md_lines.append(f"")
        md_lines.append("---")
        md_lines.append(f"")

    return "\n".join(md_lines)


# ============================================================================
# Workspace Scanning
# ============================================================================

def get_claude_projects_dir():
    """Get the Claude projects directory, with error handling."""
    projects_dir = Path.home() / ".claude" / "projects"

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found at {projects_dir}\n")
        sys.exit(1)

    return projects_dir


def normalize_workspace_name(workspace_dir_name: str, verify_local: bool = True, base_path: Path = None) -> str:
    """Convert workspace directory name to readable path.

    Args:
        workspace_dir_name: Encoded workspace name (e.g., '-home-user-my-project')
        verify_local: If True, verify against local filesystem to handle dashes correctly
        base_path: Base path to prepend when verifying (for WSL: //wsl.localhost/Ubuntu)

    Returns:
        Decoded path (e.g., 'home/user/my-project')
    """
    # Remove leading dash
    if workspace_dir_name.startswith('-'):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # If not verifying, just replace all dashes (legacy behavior)
    if not verify_local:
        return '/' + encoded.replace('-', '/')

    # Try to reconstruct the original path by checking filesystem
    # Split by dashes and try to build the path, preferring longer matches
    parts = encoded.split('-')
    path_segments = []
    i = 0

    while i < len(parts):
        # Build current path to check children against
        if path_segments:
            current_path_str = '/' + '/'.join(path_segments)
            if base_path:
                current_path = base_path / current_path_str.lstrip('/')
            else:
                current_path = Path(current_path_str)
        else:
            # Starting from root
            if base_path:
                current_path = base_path
            else:
                current_path = Path('/')

        # Try progressively longer combinations (longest first for better matching)
        best_match = None
        best_match_len = 0

        # Try from longest to shortest to prefer longer directory names with dashes
        for j in range(len(parts), i, -1):
            # Try combining parts[i:j] with dashes
            candidate_segment = '-'.join(parts[i:j])

            # Check if this directory name exists as a child of current_path
            candidate_path = current_path / candidate_segment

            if candidate_path.exists() and candidate_path.is_dir():
                # This exact directory exists - use it
                best_match = candidate_segment
                best_match_len = j - i
                break  # Found the longest match, stop searching

        if best_match:
            # Use the match we found
            path_segments.append(best_match)
            i += best_match_len
        else:
            # No match found, use single part and move on
            path_segments.append(parts[i])
            i += 1

    # If we couldn't build a valid path, fall back to simple replacement
    if len(path_segments) == 0:
        return '/' + encoded.replace('-', '/')

    return '/' + '/'.join(path_segments)


def get_current_workspace_pattern():
    """
    Detect the current workspace based on the current working directory.
    Returns a pattern that can be used to match the workspace directory.
    """
    cwd = Path.cwd()
    cwd_str = str(cwd)

    # Handle Windows paths (C:\path\to\project -> C--path-to-project)
    if sys.platform == 'win32' and len(cwd_str) >= 2 and cwd_str[1] == ':':
        # Extract drive letter and path
        drive = cwd_str[0]
        path_part = cwd_str[2:].lstrip('\\').lstrip('/')
        # Convert backslashes and forward slashes to dashes
        path_part = path_part.replace('\\', '-').replace('/', '-')
        workspace_pattern = f"{drive}--{path_part}"
    else:
        # Unix/Linux paths (/home/user/projects/myapp -> home-user-projects-myapp)
        workspace_pattern = cwd_str.lstrip('/').replace('/', '-')

    return workspace_pattern


def get_workspace_sessions(workspace_pattern: str, quiet: bool = False, since_date=None, until_date=None):
    """Find all sessions in workspaces matching the pattern.

    Args:
        workspace_pattern: Pattern to match workspace names
        quiet: Suppress output if True
        since_date: Only include sessions modified on or after this date (datetime object)
        until_date: Only include sessions modified on or before this date (datetime object)
    """
    projects_dir = get_claude_projects_dir()
    sessions = []

    # If pattern is empty, "*", or "all", match everything
    match_all = workspace_pattern in ("", "*", "all")

    # Detect if using WSL path (for proper dash handling in workspace names)
    projects_dir_str = str(projects_dir)
    is_wsl = projects_dir_str.startswith('\\\\wsl.localhost\\') or projects_dir_str.startswith('//wsl.localhost/')

    # For WSL, extract the base path for filesystem verification
    if is_wsl:
        # Extract distro name and construct base path for verification
        # Path format: //wsl.localhost/Ubuntu/home/user/.claude/projects
        # For UNC paths, parts[0] contains '\\wsl.localhost\Ubuntu\' (includes distro)
        parts = projects_dir.parts
        if len(parts) >= 1:
            # parts[0] is '\\wsl.localhost\Ubuntu\' - this is our base
            wsl_base = Path(parts[0].rstrip('\\').rstrip('/'))
        else:
            wsl_base = None
    else:
        wsl_base = None

    # Scan each workspace directory
    for workspace_dir in projects_dir.iterdir():
        if not workspace_dir.is_dir():
            continue

        # Check if workspace path matches our pattern
        if match_all or workspace_pattern in workspace_dir.name:
            readable_name = normalize_workspace_name(workspace_dir.name, base_path=wsl_base)

            # Get all .jsonl files in this workspace
            for jsonl_file in workspace_dir.glob("*.jsonl"):
                # Get file stats
                stat = jsonl_file.stat()
                size_kb = stat.st_size / 1024
                modified = datetime.fromtimestamp(stat.st_mtime)
                modified_date = modified.replace(hour=0, minute=0, second=0, microsecond=0)

                # Check date range filter
                if since_date and modified_date < since_date:
                    continue
                if until_date and modified_date > until_date:
                    continue

                # Count messages (each line is a message)
                try:
                    with open(jsonl_file, encoding='utf-8') as f:
                        message_count = sum(1 for _ in f)
                except Exception as e:
                    message_count = 0

                sessions.append({
                    'workspace': workspace_dir.name,
                    'workspace_readable': readable_name,
                    'file': jsonl_file,
                    'filename': jsonl_file.name,
                    'size_kb': size_kb,
                    'modified': modified,
                    'message_count': message_count
                })

    # Sort by modification time
    sessions.sort(key=lambda s: s['modified'])

    return sessions


# ============================================================================
# WSL Access
# ============================================================================

def is_running_in_wsl() -> bool:
    """Detect if we're running inside WSL."""
    try:
        with open('/proc/version', 'r') as f:
            return 'microsoft' in f.read().lower() or 'wsl' in f.read().lower()
    except:
        return False


def get_windows_home_from_wsl(username: str = None):
    """
    Get Windows user home directory from WSL.

    Args:
        username: Optional Windows username. If None, uses USERPROFILE.

    Returns:
        Path to Windows home directory, or None if not found.
    """
    import subprocess

    if username:
        # Try specific username across all drives
        mnt = Path('/mnt')
        if mnt.exists():
            for drive in sorted(mnt.iterdir()):
                if drive.is_dir() and drive.name not in ['wsl', 'wslg']:
                    user_path = drive / 'Users' / username
                    if user_path.exists() and (user_path / '.claude' / 'projects').exists():
                        return user_path
        return None

    # Primary approach: use Windows USERPROFILE
    try:
        result = subprocess.run(
            ['cmd.exe', '/c', 'echo %USERPROFILE%'],
            capture_output=True,
            text=True,
            check=True,
            timeout=5
        )
        win_path = result.stdout.strip()

        if win_path and win_path != '%USERPROFILE%':
            # Convert Windows path to WSL path
            result = subprocess.run(
                ['wslpath', win_path],
                capture_output=True,
                text=True,
                check=True,
                timeout=5
            )
            wsl_path = Path(result.stdout.strip())

            if wsl_path.exists() and (wsl_path / '.claude' / 'projects').exists():
                return wsl_path
    except:
        pass

    # Fallback: scan all drives
    mnt = Path('/mnt')
    if mnt.exists():
        for drive in sorted(mnt.iterdir()):
            if drive.is_dir() and drive.name not in ['wsl', 'wslg']:
                users_dir = drive / 'Users'
                if users_dir.exists():
                    for user_dir in users_dir.iterdir():
                        if user_dir.is_dir() and not user_dir.is_symlink():
                            claude_dir = user_dir / '.claude' / 'projects'
                            if claude_dir.exists():
                                return user_dir

    return None


def get_windows_users_with_claude():
    """
    Get list of all Windows users with Claude Code installed.

    Returns:
        List of dicts with 'username', 'path', and 'workspace_count'.
    """
    results = []
    mnt = Path('/mnt')

    if not mnt.exists():
        return results

    for drive in sorted(mnt.iterdir()):
        if drive.is_dir() and drive.name not in ['wsl', 'wslg']:
            users_dir = drive / 'Users'
            if users_dir.exists():
                for user_dir in users_dir.iterdir():
                    if user_dir.is_dir() and not user_dir.is_symlink():
                        claude_dir = user_dir / '.claude' / 'projects'
                        if claude_dir.exists():
                            workspace_count = len([d for d in claude_dir.iterdir() if d.is_dir()])
                            results.append({
                                'username': user_dir.name,
                                'drive': drive.name,
                                'path': user_dir,
                                'claude_dir': claude_dir,
                                'workspace_count': workspace_count
                            })

    return results


def is_wsl_remote(remote_spec: str) -> bool:
    """Check if remote spec is a WSL distribution (wsl://DistroName)."""
    return remote_spec.startswith('wsl://')


def is_windows_remote(remote_spec: str) -> bool:
    """Check if remote spec is Windows from WSL (windows or windows://username)."""
    return remote_spec == 'windows' or remote_spec.startswith('windows://')


def get_source_tag(remote_spec: str = None) -> str:
    """
    Generate source tag for filename/directory prefixes.

    Args:
        remote_spec: Remote specification (None for local, 'wsl://Ubuntu', 'windows', 'user@host')

    Returns:
        Source tag string:
        - '' for local (no tag)
        - 'wsl_{distro}_' for WSL
        - 'windows_' for Windows from WSL
        - 'remote_{hostname}_' for SSH remotes
    """
    if not remote_spec:
        # Local - no tag
        return ''

    if is_wsl_remote(remote_spec):
        # WSL: wsl_ubuntu_
        distro = remote_spec[6:]  # Remove 'wsl://' prefix
        return f"wsl_{distro.lower()}_"

    if is_windows_remote(remote_spec):
        # Windows from WSL: windows_ or windows_username_
        if remote_spec.startswith('windows://'):
            username = remote_spec[10:]  # Remove 'windows://' prefix
            return f"windows_{username.lower()}_"
        else:
            return "windows_"

    # SSH Remote: remote_hostname_
    # Extract hostname from user@hostname or hostname
    if '@' in remote_spec:
        hostname = remote_spec.split('@')[1]
    else:
        hostname = remote_spec

    # Take first part if FQDN (e.g., dev.example.com -> dev)
    hostname = hostname.split('.')[0].lower()

    return f"remote_{hostname}_"


def get_workspace_name_from_path(workspace_dir_name: str) -> str:
    """
    Extract clean workspace name from directory name.
    Removes source tags and normalizes path.

    Examples:
        'C--sankar-projects-claude-sessions' -> 'claude-sessions'
        'remote_ubuntuvm01_home-sankar-projects-claude-skills' -> 'claude-skills'
        'wsl_ubuntu_home-sankar-projects-auth' -> 'auth'
    """
    # Remove source tags if present
    if workspace_dir_name.startswith('remote_'):
        # remote_hostname_path -> path
        parts = workspace_dir_name.split('_', 2)
        if len(parts) >= 3:
            workspace_dir_name = parts[2]
    elif workspace_dir_name.startswith('wsl_'):
        # wsl_distro_path -> path
        parts = workspace_dir_name.split('_', 2)
        if len(parts) >= 3:
            workspace_dir_name = parts[2]

    # Get the last path component (workspace name)
    # home-sankar-projects-claude-sessions -> claude-sessions
    # C--sankar-projects-claude-sessions -> claude-sessions
    path_parts = workspace_dir_name.split('-')

    # Find the workspace name (usually last component or last few)
    # Simple heuristic: take last part, or last 2 if hyphenated (e.g., claude-sessions)
    if len(path_parts) >= 2:
        # Check if last 2 parts form a common pattern
        last_two = '-'.join(path_parts[-2:])
        # If the second-to-last part is short (likely part of name like "claude-sessions")
        if len(path_parts[-2]) <= 10:
            return last_two

    return path_parts[-1] if path_parts else workspace_dir_name


def get_wsl_distributions() -> list:
    """Get list of available WSL distributions.

    Returns:
        List of dicts with distro info: {'name': str, 'username': str, 'has_claude': bool}
    """
    import subprocess
    import platform

    # WSL only available on Windows
    if platform.system() != 'Windows':
        return []

    try:
        # Get list of WSL distributions
        result = subprocess.run(
            ['wsl', '--list', '--quiet'],
            capture_output=True,
            timeout=5
        )

        if result.returncode != 0:
            return []

        # Parse distribution names (output is UTF-16 LE with BOM and null terminators)
        output = result.stdout.decode('utf-16-le', errors='ignore')
        distros_raw = [line.strip().rstrip('\x00') for line in output.split('\n') if line.strip()]

        distributions = []
        for distro_name in distros_raw:
            if not distro_name:
                continue

            # Get username for this distro
            try:
                user_result = subprocess.run(
                    ['wsl', '-d', distro_name, 'whoami'],
                    capture_output=True,
                    text=True,
                    timeout=5
                )

                if user_result.returncode != 0:
                    continue

                username = user_result.stdout.strip()

                # Check if .claude/projects exists
                claude_path = Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects")
                has_claude = claude_path.exists()

                distributions.append({
                    'name': distro_name,
                    'username': username,
                    'has_claude': has_claude,
                    'path': str(claude_path) if has_claude else None
                })
            except (subprocess.TimeoutExpired, FileNotFoundError):
                continue

        return distributions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_wsl_projects_dir(distro_name: str) -> Path:
    """Get Claude projects directory for a WSL distribution.

    Args:
        distro_name: WSL distribution name (e.g., 'Ubuntu', 'Debian')

    Returns:
        Path to .claude/projects in WSL, accessible from Windows
    """
    import subprocess

    # Get username from WSL
    try:
        result = subprocess.run(
            ['wsl', '-d', distro_name, 'whoami'],
            capture_output=True,
            text=True,
            timeout=5
        )

        if result.returncode != 0:
            sys.stderr.write(f"Error: Could not get username for WSL distribution '{distro_name}'\n")
            sys.exit(1)

        username = result.stdout.strip()

        # Construct Windows path to WSL filesystem
        projects_dir = Path(f"//wsl.localhost/{distro_name}/home/{username}/.claude/projects")

        if not projects_dir.exists():
            sys.stderr.write(f"Error: Claude projects directory not found in WSL '{distro_name}'\n")
            sys.stderr.write(f"Expected path: {projects_dir}\n")
            sys.exit(1)

        return projects_dir

    except subprocess.TimeoutExpired:
        sys.stderr.write(f"Error: Timeout accessing WSL distribution '{distro_name}'\n")
        sys.exit(1)
    except FileNotFoundError:
        sys.stderr.write(f"Error: WSL not found. Is it installed?\n")
        sys.exit(1)


def get_windows_projects_dir(username: str = None) -> Path:
    """Get Claude projects directory for Windows from WSL.

    Args:
        username: Optional Windows username. If None, auto-detects from USERPROFILE.

    Returns:
        Path to .claude/projects in Windows, accessible from WSL
    """
    if not is_running_in_wsl():
        sys.stderr.write(f"Error: Not running in WSL. Windows access is only available from WSL.\n")
        sys.exit(1)

    windows_home = get_windows_home_from_wsl(username)

    if not windows_home:
        if username:
            sys.stderr.write(f"Error: Could not find Windows user '{username}' with Claude Code\n")
        else:
            sys.stderr.write(f"Error: Could not find Windows home directory with Claude Code\n")
        sys.stderr.write(f"\nTips:\n")
        sys.stderr.write(f"  â€¢ Make sure Claude Code is installed on Windows\n")
        sys.stderr.write(f"  â€¢ Try: ./claude-history --list-windows\n")
        if not username:
            sys.stderr.write(f"  â€¢ Or specify username: -r windows://username\n")
        sys.exit(1)

    projects_dir = windows_home / '.claude' / 'projects'

    if not projects_dir.exists():
        sys.stderr.write(f"Error: Claude projects directory not found in Windows\n")
        sys.stderr.write(f"Expected path: {projects_dir}\n")
        sys.exit(1)

    return projects_dir


# ============================================================================
# Remote Fetching
# ============================================================================

def parse_remote_host(remote_spec: str) -> tuple:
    """Parse remote host specification.

    Args:
        remote_spec: Remote host in format 'user@hostname' or 'hostname'

    Returns:
        Tuple of (user, hostname, full_spec) or (None, hostname, hostname)
    """
    if '@' in remote_spec:
        user, hostname = remote_spec.split('@', 1)
        return (user, hostname, remote_spec)
    else:
        # Just hostname, SSH will use current user
        return (None, remote_spec, remote_spec)


def check_ssh_connection(remote_host: str) -> bool:
    """Check if passwordless SSH connection is possible.

    Args:
        remote_host: Full remote spec (user@hostname or hostname)

    Returns:
        True if connection successful, False otherwise
    """
    import subprocess

    try:
        # Try SSH with BatchMode (no password prompts) and short timeout
        result = subprocess.run(
            ['ssh', '-o', 'BatchMode=yes', '-o', 'ConnectTimeout=5',
             remote_host, 'echo ok'],
            capture_output=True,
            text=True,
            timeout=10
        )
        return result.returncode == 0 and result.stdout.strip() == 'ok'
    except (subprocess.TimeoutExpired, FileNotFoundError):
        return False


def get_remote_hostname(remote_host: str) -> str:
    """Extract hostname from remote spec for use in directory prefix.

    Args:
        remote_host: Remote spec (user@hostname or hostname)

    Returns:
        Hostname portion only
    """
    _, hostname, _ = parse_remote_host(remote_host)
    # Clean hostname for use in directory names (remove dots, etc.)
    return hostname.replace('.', '-')


def get_remote_workspaces_batch(remote_host: str) -> list:
    """Get all workspace info from remote in one batch operation.

    Creates and runs a script on remote to gather all info efficiently.

    Returns:
        List of dicts with workspace info including decoded paths
    """
    import subprocess
    import json

    # Create a shell script that will run on remote
    script = '''#!/bin/bash
cd ~/.claude/projects/ 2>/dev/null || exit 1

for dir in -*/ ; do
    dir=${dir%/}  # Remove trailing slash
    [ -d "$dir" ] || continue

    # Decode the workspace name by testing paths
    encoded="${dir#-}"  # Remove leading dash

    # Try to find the actual path
    IFS='-' read -ra PARTS <<< "$encoded"
    best_path=""

    # Build path by testing combinations
    current=""
    for part in "${PARTS[@]}"; do
        if [ -z "$current" ]; then
            current="$part"
        else
            # Try with dash
            test_with_dash="$current-$part"
            # Try as separate segment
            test_separate="$current/$part"

            if [ -d "/$test_with_dash" ]; then
                current="$test_with_dash"
            elif [ -d "/$current/$part" ]; then
                current="$current/$part"
            else
                current="$test_separate"
            fi
        fi
    done

    # Get session count
    session_count=$(find "$dir" -maxdepth 1 -name "*.jsonl" 2>/dev/null | wc -l)

    # Output: encoded_name|decoded_path|session_count
    echo "$dir|/$current|$session_count"
done
'''

    try:
        # Run the script on remote via SSH
        # Ensure Unix line endings (important on Windows)
        # Replace both \r\n and standalone \r with \n
        script_unix = script.replace('\r\n', '\n').replace('\r', '\n')
        script_bytes = script_unix.encode('utf-8')

        result = subprocess.run(
            ['ssh', remote_host, 'bash -s'],
            input=script_bytes,
            capture_output=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        stdout_text = result.stdout.decode('utf-8', errors='replace')
        # Parse results
        workspaces = []
        for line in stdout_text.strip().split('\n'):
            if not line:
                continue
            parts = line.split('|')
            if len(parts) >= 3:
                workspaces.append({
                    'encoded': parts[0],
                    'decoded': parts[1],
                    'session_count': int(parts[2]) if parts[2].isdigit() else 0
                })

        return workspaces

    except Exception as e:
        return []


def normalize_remote_workspace_name(remote_host: str, workspace_dir_name: str) -> str:
    """Convert remote workspace directory name to readable path by verifying via SSH.

    Args:
        remote_host: Remote host specification
        workspace_dir_name: Encoded workspace name

    Returns:
        Decoded path
    """
    import subprocess

    # Remove leading dash
    if workspace_dir_name.startswith('-'):
        encoded = workspace_dir_name[1:]
    else:
        encoded = workspace_dir_name

    # Generate possible decodings by trying different dash positions
    parts = encoded.split('-')

    # Generate all reasonable path combinations (limit to avoid exponential explosion)
    # We'll try keeping dashes together in common patterns
    candidates = []

    # Strategy: Build path greedily, trying longer segments first
    def generate_paths(parts, start_idx, current_path):
        if start_idx >= len(parts):
            candidates.append('/'.join(current_path))
            return

        # Try combining 1 to 3 parts (limit to keep it reasonable)
        for length in range(1, min(4, len(parts) - start_idx + 1)):
            segment = '-'.join(parts[start_idx:start_idx + length])
            generate_paths(parts, start_idx + length, current_path + [segment])

    generate_paths(parts, 0, [])

    # Limit candidates to reasonable number
    candidates = candidates[:20]

    # Test all candidates in ONE SSH call
    test_commands = ' || '.join([f'test -e "/{path}" && echo "/{path}"' for path in candidates])

    try:
        result = subprocess.run(
            ['ssh', remote_host, test_commands],
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.stdout.strip():
            # Return the first (longest) match
            return result.stdout.strip().split('\n')[0]
    except:
        pass

    # Fallback to simple replacement
    return '/' + encoded.replace('-', '/')


def list_remote_workspaces(remote_host: str) -> list:
    """List workspace directories on remote host.

    Args:
        remote_host: Remote host specification

    Returns:
        List of workspace directory names (e.g., ['-home-user-project', ...])
        Excludes remote caches (remote_* and wsl_*) to prevent circular fetching
    """
    import subprocess

    try:
        # List directories in remote ~/.claude/projects/ (simple and fast)
        result = subprocess.run(
            ['ssh', remote_host,
             'ls -1 ~/.claude/projects/ | grep "^-"'],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        # Parse output - one directory name per line
        all_workspaces = [line.strip() for line in result.stdout.strip().split('\n') if line.strip()]

        # Filter out remote caches to prevent circular fetching:
        # - remote_* = SSH remote caches
        # - wsl_* = WSL caches
        # These are already fetched data and shouldn't be re-fetched
        workspaces = [
            ws for ws in all_workspaces
            if not ws.startswith('remote_') and not ws.startswith('wsl_')
        ]

        return workspaces

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def get_remote_session_info(remote_host: str, remote_workspace: str) -> list:
    """Get session file information from remote workspace without downloading.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name

    Returns:
        List of session info dicts with filename, size_kb, modified, message_count
    """
    import subprocess
    import json

    try:
        # Get file stats from remote using find and stat
        # Output format: filename|size_bytes|mtime_epoch|line_count
        cmd = f'''cd ~/.claude/projects/{remote_workspace} && \
                  for f in *.jsonl; do \
                      [ -f "$f" ] || continue; \
                      size=$(stat -c %s "$f" 2>/dev/null || stat -f %z "$f" 2>/dev/null); \
                      mtime=$(stat -c %Y "$f" 2>/dev/null || stat -f %m "$f" 2>/dev/null); \
                      lines=$(wc -l < "$f"); \
                      echo "$f|$size|$mtime|$lines"; \
                  done'''

        result = subprocess.run(
            ['ssh', remote_host, cmd],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            return []

        sessions = []
        for line in result.stdout.strip().split('\n'):
            if not line or '|' not in line:
                continue

            parts = line.split('|')
            if len(parts) != 4:
                continue

            filename, size_bytes, mtime_epoch, line_count = parts

            try:
                size_kb = int(size_bytes) / 1024
                modified = datetime.fromtimestamp(int(mtime_epoch))
                message_count = int(line_count)

                sessions.append({
                    'filename': filename,
                    'size_kb': size_kb,
                    'modified': modified,
                    'message_count': message_count
                })
            except (ValueError, OSError):
                continue

        return sessions

    except (subprocess.TimeoutExpired, FileNotFoundError):
        return []


def fetch_workspace_files(remote_host: str, remote_workspace: str,
                          local_projects_dir: Path, hostname: str) -> dict:
    """Fetch all files from a remote workspace using rsync.

    Args:
        remote_host: Remote host specification
        remote_workspace: Remote workspace directory name (e.g., '-home-user-project')
        local_projects_dir: Local ~/.claude/projects/ directory
        hostname: Clean hostname for prefix

    Returns:
        Dict with stats: {'success': bool, 'files_copied': int, 'bytes': int}
    """
    import subprocess

    # Generate local directory name with remote prefix
    # New naming: remote_hostname_path (consistent with export tags)
    # Strip leading dash from remote_workspace if present
    workspace_path = remote_workspace.lstrip('-')
    local_workspace = f"remote_{hostname}_{workspace_path}"
    local_dir = local_projects_dir / local_workspace

    # Create local directory
    local_dir.mkdir(parents=True, exist_ok=True)

    # Build rsync command
    # -a: archive mode (preserves timestamps, permissions, etc.)
    # -v: verbose
    # -h: human-readable
    # --include='*.jsonl': only sync .jsonl files
    # --exclude='*': exclude everything else
    remote_path = f"{remote_host}:~/.claude/projects/{remote_workspace}/"

    try:
        # Convert Windows path to Unix-style for rsync (even on Windows)
        # rsync expects forward slashes
        local_path_posix = str(local_dir).replace('\\', '/')

        # On Windows, rsync interprets "C:" as a remote host
        # Convert to /cygdrive/c/ notation which rsync understands
        import re
        if ':' in local_path_posix and not local_path_posix.startswith('/'):
            # Windows absolute path with drive letter (e.g., C:/Users/...)
            # Convert to Cygwin-style: /cygdrive/c/Users/...
            drive_match = re.match(r'([A-Za-z]):(.*)', local_path_posix)
            if drive_match:
                drive_letter = drive_match.group(1).lower()
                path_part = drive_match.group(2)
                local_path_str = f'/cygdrive/{drive_letter}{path_part}/'
            else:
                local_path_str = local_path_posix + '/'
        else:
            local_path_str = local_path_posix + '/'

        rsync_cmd = ['rsync', '-avh',
                     '--include=*.jsonl', '--exclude=*',
                     remote_path, local_path_str]

        result = subprocess.run(
            rsync_cmd,
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )

        if result.returncode != 0:
            return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': result.stderr}

        # Parse rsync output to count files
        # Look for lines that don't start with special characters
        lines = result.stdout.split('\n')
        files_copied = sum(1 for line in lines if line.strip() and
                          not line.startswith(('sending', 'sent', 'total', 'building')))

        return {
            'success': True,
            'files_copied': files_copied,
            'local_dir': local_workspace,
            'output': result.stdout
        }

    except subprocess.TimeoutExpired:
        return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': 'Timeout'}
    except FileNotFoundError:
        return {'success': False, 'files_copied': 0, 'bytes': 0, 'error': 'rsync not found'}


# ============================================================================
# Commands
# ============================================================================

def cmd_list(args):
    """List sessions for a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, 'remote', None)

    # Parse date filters
    since_str = getattr(args, 'since', None)
    until_str = getattr(args, 'until', None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    # Handle WSL, Windows, or remote listing
    if remote_host:
        # Check if this is a Windows remote (windows or windows://username)
        if is_windows_remote(remote_host):
            # Extract username if specified (windows://username)
            username = None
            if remote_host.startswith('windows://'):
                username = remote_host[10:]  # Remove 'windows://' prefix

            # Get Windows projects directory (uses /mnt/c/Users/... path)
            projects_dir = get_windows_projects_dir(username)

            # Use local file operations on Windows filesystem
            # Temporarily override get_claude_projects_dir to return Windows path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_windows_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            setattr(current_module, 'get_claude_projects_dir', get_windows_override)

            try:
                # Use local session listing with Windows path
                all_sessions = get_workspace_sessions(args.workspace, since_date=since_date, until_date=until_date)

                # Filter out remote caches to prevent showing cached data
                # Only show native Windows workspaces, not cached remote/wsl data
                sessions = [
                    s for s in all_sessions
                    if not s['workspace'].startswith('remote_') and not s['workspace'].startswith('wsl_')
                ]

                if not sessions:
                    sys.stderr.write(f"Error: No native sessions found in Windows\n")
                    sys.exit(1)

                workspaces_only = getattr(args, 'workspaces_only', False)

                if workspaces_only:
                    # Just print workspace names
                    workspaces = set()
                    for session in sessions:
                        workspaces.add(session['workspace_readable'])

                    for ws in sorted(workspaces):
                        print(ws)
                else:
                    # Print header and sessions
                    print("WORKSPACE\tFILE\tMESSAGES\tDATE")
                    for session in sessions:
                        print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")

            finally:
                # Restore original function
                setattr(current_module, 'get_claude_projects_dir', original_get_claude_projects_dir)

            return  # Done with Windows handling

        # Check if this is a WSL remote (wsl://DistroName)
        elif is_wsl_remote(remote_host):
            # Extract distro name from wsl://DistroName
            distro_name = remote_host[6:]  # Remove 'wsl://' prefix

            # Get WSL projects directory (uses Windows path //wsl.localhost/...)
            projects_dir = get_wsl_projects_dir(distro_name)

            # Use local file operations on WSL filesystem
            # Temporarily override get_claude_projects_dir to return WSL path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_wsl_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            setattr(current_module, 'get_claude_projects_dir', get_wsl_override)

            try:
                # Use local session listing with WSL path
                all_sessions = get_workspace_sessions(args.workspace, since_date=since_date, until_date=until_date)

                # Filter out remote caches to prevent showing cached data
                # Only show native WSL workspaces, not cached remote/wsl data
                sessions = [
                    s for s in all_sessions
                    if not s['workspace'].startswith('remote_') and not s['workspace'].startswith('wsl_')
                ]

                if not sessions:
                    sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
                    sys.exit(1)

                workspaces_only = getattr(args, 'workspaces_only', False)

                if workspaces_only:
                    # Just print workspace names
                    workspaces = set()
                    for session in sessions:
                        workspaces.add(session['workspace_readable'])

                    for ws in sorted(workspaces):
                        print(ws)
                else:
                    # Print header and sessions
                    print("WORKSPACE\tFILE\tMESSAGES\tDATE")
                    for session in sessions:
                        print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")
            finally:
                # Restore original function
                setattr(current_module, 'get_claude_projects_dir', original_get_claude_projects_dir)

            return  # Done with WSL handling

        # SSH remote handling
        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        workspace_pattern = args.workspace
        workspaces_only = getattr(args, 'workspaces_only', False)

        # Use fast batch approach for workspaces-only mode
        if workspaces_only:
            batch_workspaces = get_remote_workspaces_batch(remote_host)

            if not batch_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by pattern
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                batch_workspaces = [ws for ws in batch_workspaces if workspace_pattern in ws['encoded']]

            if not batch_workspaces:
                sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
                sys.exit(1)

            # Just print workspace names, one per line
            for ws_info in sorted(batch_workspaces, key=lambda x: x['decoded']):
                print(ws_info['decoded'])

        else:
            # Detailed mode - get full session info
            remote_workspaces = list_remote_workspaces(remote_host)

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by pattern
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                remote_workspaces = [ws for ws in remote_workspaces if workspace_pattern in ws]

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
                sys.exit(1)

            # Get session info for each workspace
            sessions = []

            for remote_workspace in remote_workspaces:
                readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)

                # Get session files from this workspace
                workspace_sessions = get_remote_session_info(remote_host, remote_workspace)

                for session_info in workspace_sessions:
                    # Apply date filtering
                    modified_date = session_info['modified'].replace(hour=0, minute=0, second=0, microsecond=0)
                    if since_date and modified_date < since_date:
                        continue
                    if until_date and modified_date > until_date:
                        continue

                    sessions.append({
                        'workspace': remote_workspace,
                        'workspace_readable': readable_name,
                        'filename': session_info['filename'],
                        'size_kb': session_info['size_kb'],
                        'modified': session_info['modified'],
                        'message_count': session_info['message_count']
                    })

            if not sessions:
                sys.stderr.write(f"Error: No sessions found\n")
                sys.exit(1)

            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in sessions:
                print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")

    else:
        # Local listing
        sessions = get_workspace_sessions(args.workspace, since_date=since_date, until_date=until_date)

        if not sessions:
            sys.stderr.write(f"Error: No sessions found matching '{args.workspace}'\n")
            sys.exit(1)

        # Check if workspaces-only mode
        workspaces_only = getattr(args, 'workspaces_only', False)

        if workspaces_only:
            # Just print workspace names, one per line
            workspaces = set()
            for session in sessions:
                workspaces.add(session['workspace_readable'])

            for ws in sorted(workspaces):
                print(ws)

        else:
            # Print header and sessions in simple format
            print("WORKSPACE\tFILE\tMESSAGES\tDATE")
            for session in sessions:
                print(f"{session['workspace_readable']}\t{session['filename']}\t{session['message_count']}\t{session['modified'].strftime('%Y-%m-%d')}")


def cmd_convert(args):
    """Convert a single .jsonl file to markdown."""
    remote_host = getattr(args, 'remote', None)

    if remote_host:
        # Handle remote file conversion
        import subprocess
        import tempfile

        # Check SSH connectivity
        if not check_ssh_connection(remote_host):
            sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
            sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
            sys.exit(1)

        # Download file to temporary location
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            # Use scp to download the file
            result = subprocess.run(
                ['scp', f"{remote_host}:{args.jsonl_file}", str(tmp_path)],
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode != 0:
                sys.stderr.write(f"Error downloading file: {result.stderr}\n")
                tmp_path.unlink()
                sys.exit(1)

            # Convert the downloaded file
            jsonl_file = tmp_path
            filename = Path(args.jsonl_file).name
            output_file = Path(args.output) if args.output else Path(filename).with_suffix('.md')

            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding='utf-8')

            print(output_file)

        finally:
            # Clean up temporary file
            if tmp_path.exists():
                tmp_path.unlink()

    else:
        # Handle local file conversion
        jsonl_file = Path(args.jsonl_file)

        if not jsonl_file.exists():
            sys.stderr.write(f"Error: {jsonl_file} not found\n")
            sys.exit(1)

        output_file = Path(args.output) if args.output else jsonl_file.with_suffix('.md')

        try:
            markdown = parse_jsonl_to_markdown(jsonl_file)
            output_file.write_text(markdown, encoding='utf-8')

            print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting file: {e}\n")
            sys.exit(1)


def cmd_batch(args):
    """Batch convert all sessions from a workspace."""
    # Check if remote flag is set
    remote_host = getattr(args, 'remote', None)

    # Parse date filters
    since_str = getattr(args, 'since', None)
    until_str = getattr(args, 'until', None)

    since_date = parse_date_string(since_str) if since_str else None
    until_date = parse_date_string(until_str) if until_str else None

    # Validate date formats
    if since_str and since_date is None:
        sys.stderr.write(f"Error: Invalid date format for --since: '{since_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-01)\n")
        sys.exit(1)

    if until_str and until_date is None:
        sys.stderr.write(f"Error: Invalid date format for --until: '{until_str}'\n")
        sys.stderr.write("Use YYYY-MM-DD format (e.g., 2025-11-30)\n")
        sys.exit(1)

    if since_date and until_date and since_date > until_date:
        sys.stderr.write("Error: --since date must be before --until date\n")
        sys.exit(1)

    output_dir = Path(args.output_dir)

    # Handle Windows, WSL, or remote export
    if remote_host:
        # Check if this is a Windows remote (windows or windows://username)
        if is_windows_remote(remote_host):
            # Extract username if specified (windows://username)
            username = None
            if remote_host.startswith('windows://'):
                username = remote_host[10:]  # Remove 'windows://' prefix

            # Get Windows projects directory (uses /mnt/c/Users/... path)
            projects_dir = get_windows_projects_dir(username)

            # Use local file operations on Windows filesystem
            # Temporarily override get_claude_projects_dir to return Windows path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_windows_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            setattr(current_module, 'get_claude_projects_dir', get_windows_override)

            try:
                # Use local session listing with Windows path
                all_sessions = get_workspace_sessions(args.workspace, quiet=True, since_date=since_date, until_date=until_date)

                # Filter out remote caches to prevent circular fetching
                # Only export native Windows workspaces, not cached remote/wsl data
                sessions = [
                    s for s in all_sessions
                    if not s['workspace'].startswith('remote_') and not s['workspace'].startswith('wsl_')
                ]
            finally:
                # Restore original function
                setattr(current_module, 'get_claude_projects_dir', original_get_claude_projects_dir)

            if not sessions:
                sys.stderr.write(f"Error: No native sessions found in Windows\n")
                sys.exit(1)

            # Continue with export processing below (same as local)

        # Check if this is a WSL remote (wsl://DistroName)
        elif is_wsl_remote(remote_host):
            # Extract distro name from wsl://DistroName
            distro_name = remote_host[6:]  # Remove 'wsl://' prefix

            # Get WSL projects directory (uses Windows path //wsl.localhost/...)
            projects_dir = get_wsl_projects_dir(distro_name)

            # Use local file operations on WSL filesystem
            # Temporarily override get_claude_projects_dir to return WSL path
            original_get_claude_projects_dir = get_claude_projects_dir

            def get_wsl_override():
                return projects_dir

            # Replace function temporarily
            current_module = sys.modules[__name__]
            setattr(current_module, 'get_claude_projects_dir', get_wsl_override)

            try:
                # Use local session listing with WSL path
                all_sessions = get_workspace_sessions(args.workspace, quiet=True, since_date=since_date, until_date=until_date)

                # Filter out remote caches to prevent circular fetching
                # Only export native WSL workspaces, not cached remote/wsl data
                sessions = [
                    s for s in all_sessions
                    if not s['workspace'].startswith('remote_') and not s['workspace'].startswith('wsl_')
                ]
            finally:
                # Restore original function
                setattr(current_module, 'get_claude_projects_dir', original_get_claude_projects_dir)

            if not sessions:
                sys.stderr.write(f"Error: No native sessions found in WSL '{distro_name}'\n")
                sys.exit(1)

            # Continue with export processing below (same as local)

        else:
            # SSH remote handling - Check SSH connectivity
            if not check_ssh_connection(remote_host):
                sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
                sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
                sys.exit(1)

            # Get remote workspaces
            remote_workspaces = list_remote_workspaces(remote_host)

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
                sys.exit(1)

            # Filter by pattern
            workspace_pattern = args.workspace
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                remote_workspaces = [ws for ws in remote_workspaces if workspace_pattern in ws]

            if not remote_workspaces:
                sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
                sys.exit(1)

            # Fetch each workspace to local cache (silent)
            local_projects_dir = get_claude_projects_dir()
            hostname = get_remote_hostname(remote_host)

            for remote_workspace in remote_workspaces:
                result = fetch_workspace_files(remote_host, remote_workspace,
                                              local_projects_dir, hostname)

                if not result['success']:
                    sys.stderr.write(f"Error fetching {remote_workspace}: {result.get('error', 'Unknown error')}\n")

            # Now export from cached location
            # New naming: remote_hostname_
            cached_pattern = f"remote_{hostname}_"
            if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
                cached_pattern += workspace_pattern

            sessions = get_workspace_sessions(cached_pattern, quiet=True, since_date=since_date, until_date=until_date)

    else:
        # Local export
        sessions = get_workspace_sessions(args.workspace, quiet=True, since_date=since_date, until_date=until_date)

    if not sessions:
        sys.stderr.write(f"Error: No sessions found matching '{args.workspace}'\n")
        sys.exit(1)

    output_dir.mkdir(parents=True, exist_ok=True)

    # Convert each session
    force = getattr(args, 'force', False)
    flat = getattr(args, 'flat', False)
    remote_host = getattr(args, 'remote', None)

    for session in sessions:
        jsonl_file = session['file']

        # Extract first timestamp and create timestamped filename
        first_ts = get_first_timestamp(jsonl_file)
        if first_ts:
            try:
                dt = datetime.fromisoformat(first_ts.replace('Z', '+00:00'))
                ts_prefix = dt.strftime('%Y%m%d%H%M%S')
            except Exception:
                ts_prefix = None
        else:
            ts_prefix = None

        # Generate filename with optional source tag
        if not flat:
            # Organized structure (default) - workspace subdirectories with source tags
            # Get source tag
            source_tag = get_source_tag(remote_host)

            # Generate filename: [source_]timestamp_sessionid.md
            if ts_prefix:
                output_name = f"{source_tag}{ts_prefix}_{jsonl_file.stem}.md"
            else:
                output_name = f"{source_tag}{jsonl_file.stem}.md"

            # Create workspace subdirectory
            workspace_name = get_workspace_name_from_path(session['workspace'])
            workspace_subdir = output_dir / workspace_name
            workspace_subdir.mkdir(parents=True, exist_ok=True)
            output_file = workspace_subdir / output_name
        else:
            # Flat structure - all files in one directory, no source tags
            if ts_prefix:
                output_name = f"{ts_prefix}_{jsonl_file.stem}.md"
            else:
                output_name = f"{jsonl_file.stem}.md"
            output_file = output_dir / output_name

        # Check if we should skip (incremental export)
        if not force and output_file.exists():
            source_mtime = jsonl_file.stat().st_mtime
            output_mtime = output_file.stat().st_mtime

            if output_mtime >= source_mtime:
                continue

        try:
            minimal = getattr(args, 'minimal', False)
            split_lines = getattr(args, 'split', None)

            # Read messages from JSONL
            messages = read_jsonl_messages(jsonl_file)

            # Check if we should split into multiple parts
            if split_lines and len(messages) > 0:
                parts = generate_markdown_parts(messages, jsonl_file, minimal, split_lines)

                if parts:
                    # Save multiple part files
                    for part_num, total_parts, part_md, start_msg, end_msg in parts:
                        base_name = output_name.rsplit('.md', 1)[0]
                        part_filename = f"{base_name}_part{part_num}.md"
                        part_file = output_file.parent / part_filename

                        # Add navigation footer (remove emoji)
                        nav_lines = ["\n---\n"]
                        nav_parts = [f"**Part {part_num} of {total_parts}**"]

                        if part_num > 1:
                            prev_filename = f"{base_name}_part{part_num - 1}.md"
                            nav_parts.append(f"[â† Part {part_num - 1}]({prev_filename})")

                        if part_num < total_parts:
                            next_filename = f"{base_name}_part{part_num + 1}.md"
                            nav_parts.append(f"[Part {part_num + 1} â†’]({next_filename})")

                        nav_lines.append("> " + " | ".join(nav_parts))

                        # Write part file with navigation
                        part_file.write_text(part_md + "\n".join(nav_lines), encoding='utf-8')

                    # Print all part files
                    for part_num in range(1, total_parts + 1):
                        part_filename = f"{base_name}_part{part_num}.md"
                        print(output_file.parent / part_filename)
                else:
                    # Splitting not needed, use single file
                    markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                    output_file.write_text(markdown, encoding='utf-8')
                    print(output_file)
            else:
                # No splitting, use single file
                markdown = parse_jsonl_to_markdown(jsonl_file, minimal=minimal)
                output_file.write_text(markdown, encoding='utf-8')
                print(output_file)
        except Exception as e:
            sys.stderr.write(f"Error converting {jsonl_file.name}: {e}\n")


def cmd_version(args):
    """Show version information."""
    print(f"claude-history {__version__}")


def cmd_list_wsl(args):
    """List available WSL distributions with Claude Code workspaces."""
    distributions = get_wsl_distributions()

    if not distributions:
        sys.stderr.write("No WSL distributions found or WSL is not available.\n")
        sys.stderr.write("Make sure WSL is installed on Windows.\n")
        sys.exit(1)

    # Filter to only distributions with Claude Code
    claude_distros = [d for d in distributions if d['has_claude']]

    if not claude_distros:
        sys.stderr.write("No WSL distributions with Claude Code workspaces found.\n")
        sys.exit(1)

    # Print tab-separated output
    print("DISTRO\tUSERNAME\tPATH")
    for distro in claude_distros:
        print(f"{distro['name']}\t{distro['username']}\t{distro['path']}")


def cmd_list_windows(args):
    """List available Windows users with Claude Code workspaces (from WSL)."""
    if not is_running_in_wsl():
        sys.stderr.write("Error: --list-windows is only available from WSL.\n")
        sys.stderr.write("To access Windows sessions from Windows, use the tool directly.\n")
        sys.exit(1)

    users = get_windows_users_with_claude()

    if not users:
        sys.stderr.write("No Windows users with Claude Code workspaces found.\n")
        sys.stderr.write("Make sure Claude Code is installed on Windows.\n")
        sys.exit(1)

    # Print tab-separated output
    print("USERNAME\tDRIVE\tWORKSPACES\tPATH")
    for user in users:
        print(f"{user['username']}\t{user['drive']}\t{user['workspace_count']}\t{user['path']}")


def cmd_fetch(args):
    """Fetch remote conversation sessions via SSH."""
    remote_host = args.remote_host
    workspace_pattern = getattr(args, 'pattern', None) or ''

    # Check SSH connectivity
    if not check_ssh_connection(remote_host):
        sys.stderr.write(f"Error: Cannot connect to {remote_host} via passwordless SSH\n")
        sys.stderr.write(f"Setup: ssh-copy-id {remote_host}\n")
        sys.exit(1)

    # Get hostname for directory prefix
    hostname = get_remote_hostname(remote_host)

    # List remote workspaces
    remote_workspaces = list_remote_workspaces(remote_host)

    if not remote_workspaces:
        sys.stderr.write(f"Error: No workspaces found on {remote_host}\n")
        sys.exit(1)

    # Filter by pattern if specified
    if workspace_pattern and workspace_pattern not in ('', '*', 'all'):
        filtered = [ws for ws in remote_workspaces if workspace_pattern in ws]
        if not filtered:
            sys.stderr.write(f"Error: No workspaces matching pattern '{workspace_pattern}'\n")
            sys.exit(1)
        remote_workspaces = filtered

    # Get local projects directory
    local_projects_dir = get_claude_projects_dir()

    # Fetch each workspace (silent on success)
    for remote_workspace in remote_workspaces:
        readable_name = normalize_remote_workspace_name(remote_host, remote_workspace)
        # New naming: remote_hostname_path
        workspace_path = remote_workspace.lstrip('-')
        local_name = f"remote_{hostname}_{workspace_path}"

        result = fetch_workspace_files(remote_host, remote_workspace,
                                       local_projects_dir, hostname)

        if result['success']:
            # Print the local path where it was cached
            print(normalize_workspace_name(local_name, verify_local=False))
        else:
            error = result.get('error', 'Unknown error')
            sys.stderr.write(f"Error fetching {readable_name}: {error}\n")


def cmd_export_all(args):
    """Export from all sources: local, WSL distributions (Windows), Windows (WSL), and optionally remotes."""
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Track export statistics
    sources_processed = []
    total_sessions = 0
    total_errors = 0

    # Detect environment
    in_wsl = is_running_in_wsl()
    local_label = "Local WSL" if in_wsl else "Local Windows"

    # 1. Export from local (WSL or Windows depending on environment)
    print(f"[Local] Exporting from {local_label}...")
    try:
        output_dir_str = str(output_dir)
        workspace_val = args.workspace if hasattr(args, 'workspace') and args.workspace else ''
        class LocalExportArgs:
            workspace = workspace_val
            output_dir = output_dir_str
            since = getattr(args, 'since', None)
            until = getattr(args, 'until', None)
            force = getattr(args, 'force', False)
            minimal = getattr(args, 'minimal', False)
            split = getattr(args, 'split', None)
            flat = False  # Always organized for export-all
            remote = None  # Local

        # Get local sessions count before export
        local_sessions = get_workspace_sessions(LocalExportArgs.workspace, quiet=True)
        cmd_batch(LocalExportArgs())

        sources_processed.append({
            'source': local_label,
            'sessions': len(local_sessions)
        })
        total_sessions += len(local_sessions)
        print(f"[OK] Local: {len(local_sessions)} sessions exported\n")
    except Exception as e:
        print(f"[ERROR] Local export failed: {e}\n")
        total_errors += 1

    # 2a. Export from Windows (when running in WSL)
    if in_wsl:
        print("[Windows] Checking Windows users with Claude...")
        try:
            windows_users = get_windows_users_with_claude()

            if windows_users:
                for user in windows_users:
                    print(f"  Exporting from Windows: {user['username']}...")
                    try:
                        output_dir_str = str(output_dir)
                        workspace_val = args.workspace if hasattr(args, 'workspace') and args.workspace else ''
                        username = user['username']
                        class WindowsExportArgs:
                            workspace = workspace_val
                            output_dir = output_dir_str
                            since = getattr(args, 'since', None)
                            until = getattr(args, 'until', None)
                            force = getattr(args, 'force', False)
                            minimal = getattr(args, 'minimal', False)
                            split = getattr(args, 'split', None)
                            flat = False  # Always organized
                            remote = f"windows://{username}"

                        cmd_batch(WindowsExportArgs())

                        # Count sessions
                        projects_dir = get_windows_projects_dir(username)
                        original_get_claude = get_claude_projects_dir
                        def temp_override():
                            return projects_dir
                        current_module = sys.modules[__name__]
                        setattr(current_module, 'get_claude_projects_dir', temp_override)
                        try:
                            win_sessions = get_workspace_sessions(WindowsExportArgs.workspace, quiet=True)
                            # Filter out cached data
                            native_sessions = [s for s in win_sessions
                                             if not s['workspace'].startswith('remote_')
                                             and not s['workspace'].startswith('wsl_')]
                            session_count = len(native_sessions)
                        finally:
                            setattr(current_module, 'get_claude_projects_dir', original_get_claude)

                        sources_processed.append({
                            'source': f"Windows: {username}",
                            'sessions': session_count
                        })
                        total_sessions += session_count
                        print(f"  [OK] {username}: {session_count} sessions exported")
                    except Exception as e:
                        print(f"  [ERROR] {username} failed: {e}")
                        total_errors += 1
                print()
            else:
                print("  No Windows users with Claude Code found\n")
        except Exception as e:
            print(f"[ERROR] Windows export failed: {e}\n")

    # 2b. Export from all WSL distributions (when running on Windows)
    if not in_wsl:
        print("[WSL] Checking WSL distributions...")
        try:
            wsl_distros = get_wsl_distributions()
            claude_distros = [d for d in wsl_distros if d['has_claude']]

            if claude_distros:
                for distro in claude_distros:
                    print(f"  Exporting from WSL: {distro['name']}...")
                    try:
                        output_dir_str = str(output_dir)
                        workspace_val = args.workspace if hasattr(args, 'workspace') and args.workspace else ''
                        distro_name = distro['name']
                        class WslExportArgs:
                            workspace = workspace_val
                            output_dir = output_dir_str
                            since = getattr(args, 'since', None)
                            until = getattr(args, 'until', None)
                            force = getattr(args, 'force', False)
                            minimal = getattr(args, 'minimal', False)
                            split = getattr(args, 'split', None)
                            flat = False  # Always organized
                            remote = f"wsl://{distro_name}"

                        cmd_batch(WslExportArgs())

                        # Count sessions (approximation)
                        projects_dir = get_wsl_projects_dir(distro['name'])
                        original_get_claude = get_claude_projects_dir
                        def temp_override():
                            return projects_dir
                        current_module = sys.modules[__name__]
                        setattr(current_module, 'get_claude_projects_dir', temp_override)
                        try:
                            wsl_sessions = get_workspace_sessions(WslExportArgs.workspace, quiet=True)
                            session_count = len(wsl_sessions)
                        finally:
                            setattr(current_module, 'get_claude_projects_dir', original_get_claude)

                        sources_processed.append({
                            'source': f"WSL: {distro['name']}",
                            'sessions': session_count
                        })
                        total_sessions += session_count
                        print(f"  [OK] {distro['name']}: {session_count} sessions exported")
                    except Exception as e:
                        print(f"  [ERROR] {distro['name']} failed: {e}")
                        total_errors += 1
                print()
            else:
                print("  No WSL distributions with Claude Code found\n")
        except Exception as e:
            print(f"[ERROR] WSL export failed: {e}\n")

    # 3. Export from remote hosts (if specified)
    if hasattr(args, 'remotes') and args.remotes:
        print("[Remote] Exporting from remote hosts...")
        for remote_host in args.remotes:
            print(f"  Exporting from {remote_host}...")
            try:
                output_dir_str = str(output_dir)
                workspace_val = args.workspace if hasattr(args, 'workspace') and args.workspace else ''
                class RemoteExportArgs:
                    workspace = workspace_val
                    output_dir = output_dir_str
                    since = getattr(args, 'since', None)
                    until = getattr(args, 'until', None)
                    force = getattr(args, 'force', False)
                    minimal = getattr(args, 'minimal', False)
                    split = getattr(args, 'split', None)
                    flat = False
                    remote = remote_host

                cmd_batch(RemoteExportArgs())

                # Count sessions (cached)
                hostname = get_remote_hostname(remote_host)
                cached_pattern = f"remote_{hostname}_"
                cached_sessions = get_workspace_sessions(cached_pattern, quiet=True)
                session_count = len(cached_sessions)

                sources_processed.append({
                    'source': f"Remote: {remote_host}",
                    'sessions': session_count
                })
                total_sessions += session_count
                print(f"  [OK] {remote_host}: {session_count} sessions exported")
            except Exception as e:
                print(f"  [ERROR] {remote_host} failed: {e}")
                total_errors += 1
        print()

    # Print summary
    print("=" * 80)
    print("Export-All Summary")
    print("=" * 80)
    for source_info in sources_processed:
        print(f"{source_info['source']:.<40} {source_info['sessions']:>5} sessions")
    print("-" * 80)
    print(f"Total sessions exported: {total_sessions}")
    if total_errors > 0:
        print(f"Errors encountered: {total_errors}")
    print(f"Output directory: {output_dir.absolute()}")
    print("=" * 80)

    # Generate index.md if requested
    if getattr(args, 'index', True):  # Default to true
        generate_index_manifest(output_dir, sources_processed)


def generate_index_manifest(output_dir: Path, sources_info: list):
    """Generate index.md manifest file summarizing all exported sessions."""
    from datetime import datetime

    index_path = output_dir / "index.md"

    # Scan all workspace directories
    workspaces = {}
    for item in output_dir.iterdir():
        if item.is_dir():
            workspace_name = item.name
            sessions = list(item.glob("*.md"))

            # Group by source
            sources = {'local': 0, 'wsl': 0, 'windows': 0, 'remote': 0}
            for session_file in sessions:
                if session_file.name.startswith('wsl_'):
                    sources['wsl'] += 1
                elif session_file.name.startswith('windows_'):
                    sources['windows'] += 1
                elif session_file.name.startswith('remote_'):
                    sources['remote'] += 1
                else:
                    sources['local'] += 1

            workspaces[workspace_name] = {
                'total': len(sessions),
                'sources': sources,
                'sessions': sessions
            }

    # Generate markdown
    lines = [
        "# Claude Conversation Export Index",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**Total Workspaces:** {len(workspaces)}",
        f"**Total Sessions:** {sum(ws['total'] for ws in workspaces.values())}",
        "",
        "## Sources",
        ""
    ]

    # Add sources summary
    for source_info in sources_info:
        lines.append(f"- **{source_info['source']}**: {source_info['sessions']} sessions")

    lines.extend(["", "## Workspaces", ""])

    # Add workspace details
    for workspace_name in sorted(workspaces.keys()):
        ws_info = workspaces[workspace_name]
        lines.append(f"### {workspace_name} ({ws_info['total']} sessions)")
        lines.append("")
        if ws_info['sources']['local'] > 0:
            lines.append(f"- **Local:** {ws_info['sources']['local']} sessions")
        if ws_info['sources']['wsl'] > 0:
            lines.append(f"- **WSL:** {ws_info['sources']['wsl']} sessions")
        if ws_info['sources']['windows'] > 0:
            lines.append(f"- **Windows:** {ws_info['sources']['windows']} sessions")
        if ws_info['sources']['remote'] > 0:
            lines.append(f"- **Remote:** {ws_info['sources']['remote']} sessions")
        lines.append("")

    # Write index
    index_path.write_text('\n'.join(lines), encoding='utf-8')
    print(f"\n[Index] Generated: {index_path}")


# ============================================================================
# Main
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        prog='claude-history',
        description='Browse and export Claude Code conversation history',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
EXAMPLES:

  List workspaces:
    claude-history lsw                        # all local workspaces
    claude-history lsw myproject              # filter by pattern
    claude-history lsw -r user@server         # remote workspaces

  List sessions:
    claude-history lss                        # current workspace
    claude-history lss myproject              # specific workspace
    claude-history lss myproject -r user@server    # remote sessions

  Export:
    claude-history export myproject           # export workspace
    claude-history export -a                  # export all workspaces
    claude-history export file.jsonl         # export single file
    claude-history export myproject -r user@server  # remote export

  Date filtering:
    claude-history lss myproject --since 2025-11-01
    claude-history export myproject --since 2025-11-01 --until 2025-11-30

  Export options:
    claude-history export myproject --minimal       # minimal mode
    claude-history export myproject --split 500     # split long conversations
    claude-history export myproject ./output        # custom output directory
    claude-history export myproject --flat          # flat structure (no subdirs)

  Export from all sources:
    claude-history export-all                       # local + all WSL distros
    claude-history export-all myproject             # filter by workspace pattern
    claude-history export-all -r vm01 vm02          # include SSH remotes
    claude-history export-all ./backups --split 500 # custom dir + splitting

  WSL access (Windows):
    claude-history --list-wsl                       # list WSL distributions
    claude-history lsw -r wsl://Ubuntu              # list WSL workspaces
    claude-history lss myproject -r wsl://Ubuntu    # list WSL sessions
    claude-history export myproject -r wsl://Ubuntu # export from WSL

  Windows access (from WSL):
    claude-history --list-windows                   # list Windows users with Claude
    claude-history lsw -r windows                   # list Windows workspaces
    claude-history lss myproject -r windows         # list Windows sessions
    claude-history export myproject -r windows      # export from Windows
    claude-history lss -r windows://username        # specify Windows user
        '''
    )

    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    parser.add_argument('--list-wsl', action='store_true',
        help='List available WSL distributions with Claude Code workspaces')
    parser.add_argument('--list-windows', action='store_true',
        help='List available Windows users with Claude Code workspaces (from WSL)')

    # Create subparsers for commands
    subparsers = parser.add_subparsers(dest='command', help='Command to execute', required=False)

    # ========== lsw - list workspaces ==========
    parser_lsw = subparsers.add_parser('lsw',
        help='List workspaces',
        description='List all workspaces (local or remote)')

    parser_lsw.add_argument('pattern',
        nargs='?',
        help='Optional pattern to filter workspaces')

    parser_lsw.add_argument('-r', '--remote',
        metavar='HOST',
        help='List workspaces from remote host via SSH (user@hostname) or WSL (wsl://DistroName)')

    # ========== lss - list sessions ==========
    parser_lss = subparsers.add_parser('lss',
        help='List sessions',
        description='List sessions in a workspace (defaults to current workspace)')

    parser_lss.add_argument('workspace',
        nargs='?',
        help='Workspace name (default: current workspace)')

    parser_lss.add_argument('-r', '--remote',
        metavar='HOST',
        help='List sessions from remote host via SSH (user@hostname) or WSL (wsl://DistroName)')

    parser_lss.add_argument('--since',
        metavar='DATE',
        help='Only include sessions modified on or after this date (YYYY-MM-DD)')

    parser_lss.add_argument('--until',
        metavar='DATE',
        help='Only include sessions modified on or before this date (YYYY-MM-DD)')

    # ========== export command ==========
    parser_export = subparsers.add_parser('export',
        help='Export to markdown',
        description='Export workspace or single file to markdown')

    parser_export.add_argument('target',
        help='Workspace name or file.jsonl (detects file by .jsonl extension)')

    parser_export.add_argument('output_dir',
        nargs='?',
        default='./claude-conversations',
        help='Output directory (default: ./claude-conversations)')

    parser_export.add_argument('-a', '--all',
        action='store_true',
        help='Export all workspaces (target is ignored)')

    parser_export.add_argument('-r', '--remote',
        metavar='HOST',
        help='Export from remote host via SSH (user@hostname) or WSL (wsl://DistroName)')

    parser_export.add_argument('--force',
        action='store_true',
        help='Force re-export (default: incremental)')

    parser_export.add_argument('--minimal',
        action='store_true',
        help='Minimal export: omit metadata, keep only conversation content')

    parser_export.add_argument('--split',
        metavar='LINES',
        type=int,
        help='Split long conversations into parts (e.g., --split 500)')

    parser_export.add_argument('--since',
        metavar='DATE',
        help='Only include sessions modified on or after this date (YYYY-MM-DD)')

    parser_export.add_argument('--until',
        metavar='DATE',
        help='Only include sessions modified on or before this date (YYYY-MM-DD)')

    parser_export.add_argument('--flat',
        action='store_true',
        help='Use flat directory structure (default: organized by workspace with source tags)')

    # ========== export-all command ==========
    parser_export_all = subparsers.add_parser('export-all',
        help='Export from all sources',
        description='Export from all sources: local, WSL distributions, and optionally SSH remotes')

    parser_export_all.add_argument('workspace',
        nargs='?',
        default='',
        help='Optional workspace pattern to filter (default: all workspaces)')

    parser_export_all.add_argument('output_dir',
        nargs='?',
        default='./claude-conversations',
        help='Output directory (default: ./claude-conversations)')

    parser_export_all.add_argument('-r', '--remotes',
        metavar='HOST',
        nargs='+',
        help='Additional remote hosts to include via SSH (user@hostname)')

    parser_export_all.add_argument('--force',
        action='store_true',
        help='Force re-export (default: incremental)')

    parser_export_all.add_argument('--minimal',
        action='store_true',
        help='Minimal export: omit metadata, keep only conversation content')

    parser_export_all.add_argument('--split',
        metavar='LINES',
        type=int,
        help='Split long conversations into parts (e.g., --split 500)')

    parser_export_all.add_argument('--since',
        metavar='DATE',
        help='Only include sessions modified on or after this date (YYYY-MM-DD)')

    parser_export_all.add_argument('--until',
        metavar='DATE',
        help='Only include sessions modified on or before this date (YYYY-MM-DD)')

    parser_export_all.add_argument('--no-index',
        action='store_true',
        help='Skip index.md manifest generation')

    # Parse arguments
    args = parser.parse_args()

    # Handle --list-wsl flag (runs before subcommand check)
    if hasattr(args, 'list_wsl') and args.list_wsl:
        class ListWslArgs:
            pass
        cmd_list_wsl(ListWslArgs())
        return

    # Handle --list-windows flag (runs before subcommand check)
    if hasattr(args, 'list_windows') and args.list_windows:
        class ListWindowsArgs:
            pass
        cmd_list_windows(ListWindowsArgs())
        return

    # Dispatch to command handlers
    if args.command == 'lsw':
        # List workspaces
        pattern = args.pattern if args.pattern else ''
        class LswArgs:
            workspace = pattern
            remote = getattr(args, 'remote', None)
            workspaces_only = True  # lsw always lists workspaces
        cmd_list(LswArgs())

    elif args.command == 'lss':
        # List sessions
        workspace_val = args.workspace if args.workspace else get_current_workspace_pattern()
        class LssArgs:
            workspace = workspace_val
            since = args.since
            until = args.until
            remote = getattr(args, 'remote', None)
            workspaces_only = False  # lss always lists sessions
        cmd_list(LssArgs())

    elif args.command == 'export':
        # Export - handle file vs workspace
        if args.all:
            # Export all workspaces
            class ExportArgs:
                workspace = ''  # empty means all
                output_dir = args.output_dir
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split
                flat = args.flat
                remote = getattr(args, 'remote', None)
            cmd_batch(ExportArgs())
        elif args.target.endswith('.jsonl'):
            # Single file export
            class ConvertArgs:
                jsonl_file = args.target
                output = None  # Use default
                remote = getattr(args, 'remote', None)
            cmd_convert(ConvertArgs())
        else:
            # Workspace export
            class ExportArgs:
                workspace = args.target
                output_dir = args.output_dir
                since = args.since
                until = args.until
                force = args.force
                minimal = args.minimal
                split = args.split
                flat = args.flat
                remote = getattr(args, 'remote', None)
            cmd_batch(ExportArgs())

    elif args.command == 'export-all':
        # Export from all sources
        class ExportAllArgs:
            workspace = args.workspace
            output_dir = args.output_dir
            remotes = getattr(args, 'remotes', None)
            since = args.since
            until = args.until
            force = args.force
            minimal = args.minimal
            split = args.split
            index = not args.no_index
        cmd_export_all(ExportAllArgs())


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.stderr.write("\nInterrupted\n")
        sys.exit(130)
    except Exception as e:
        sys.stderr.write(f"\nError: {e}\n")
        sys.exit(1)
